{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wasimmadha/dubbing-project-research/blob/main/auxiliary_task_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "tXe1PPPtUsFn"
      },
      "outputs": [],
      "source": [
        "!tar -xzf '/content/drive/MyDrive/Dubbing Project/libspeech/dev-clean.tar.gz' -C /content/\n",
        "!tar -xzf '/content/drive/MyDrive/Dubbing Project/libspeech/train-clean-100.tar.gz' -C /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbH8Ke1TWO0V",
        "outputId": "a9f9e6ba-39c9-4fb1-d0d5-e6edaf668884"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.16.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.42)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.40.5)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pydub wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "IQ21N94Z05f4"
      },
      "outputs": [],
      "source": [
        "## Imports\n",
        "\n",
        "import torch\n",
        "from pydub import AudioSegment\n",
        "import numpy as np\n",
        "from scipy.signal import get_window\n",
        "import librosa.util as librosa_util\n",
        "import random\n",
        "import torch.utils.data\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from math import sqrt\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import read\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "import random\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "import torch.distributed as dist\n",
        "from torch.nn.modules import Module\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from scipy.signal import get_window\n",
        "from librosa.util import pad_center, tiny\n",
        "\n",
        "from librosa.filters import mel as librosa_mel_fn\n",
        "\n",
        "from huggingface_hub import PyTorchModelHubMixin\n",
        "import gc\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PS7T9YT0UV4O"
      },
      "source": [
        "#### hparams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "B7pkLxXlUYnh"
      },
      "outputs": [],
      "source": [
        "### hparams.py\n",
        "\n",
        "class mapDict(dict):\n",
        "  __getattr__ = dict.get\n",
        "  __setattr__ = dict.__setitem__\n",
        "  __delattr__ = dict.__delitem__\n",
        "\n",
        "\n",
        "\n",
        "def create_hparams(hparams_string=None,verbose=False):\n",
        "  hparams = {\n",
        "    ################################\n",
        "    # Experiment Parameters        #\n",
        "    ################################\n",
        "    \"epochs\":7,\n",
        "    \"iters_per_checkpoint\":10,\n",
        "    \"seed\":1234,\n",
        "    \"dynamic_loss_scaling\":True,\n",
        "    \"fp16_run\":False,\n",
        "    \"distributed_run\":False,\n",
        "    \"dist_backend\":\"nccl\",\n",
        "    \"dist_url\":\"tcp://localhost:54321\",\n",
        "    \"cudnn_enabled\":True,\n",
        "    \"cudnn_benchmark\":False,\n",
        "    \"ignore_layers\":['embedding.weight'],\n",
        "\n",
        "    ################################\n",
        "    # Data Parameters             #\n",
        "    ################################\n",
        "    \"load_mel_from_disk\":False,\n",
        "    \"training_files\":'data/train',\n",
        "    \"validation_files\":'data/val',\n",
        "    \"text_cleaners\":['english_cleaners'],\n",
        "\n",
        "    ################################\n",
        "    # Audio Parameters             #\n",
        "    ################################\n",
        "    \"max_wav_value\":32768.0,\n",
        "    \"sampling_rate\":22050,\n",
        "    \"filter_length\":1024,\n",
        "    \"hop_length\":256,\n",
        "    \"win_length\":1024,\n",
        "    \"n_mel_channels\":80,\n",
        "    \"mel_fmin\":0.0,\n",
        "    \"mel_fmax\":8000.0,\n",
        "\n",
        "    #Data parameters\n",
        "    \"input_data_root\": r'C:\\Users\\Wasim\\DubbingProject\\Speech2Speech\\google_research\\translatotron\\data\\prepared_data\\source\\train',\n",
        "    \"output_data_root\": r'C:\\Users\\Wasim\\DubbingProject\\Speech2Speech\\google_research\\translatotron\\data\\prepared_data\\target\\train',\n",
        "    \"train_size\": 0.75,\n",
        "    #Output Audio Parameters\n",
        "    \"out_channels\":1025,\n",
        "    ################################\n",
        "    # Model Parameters             #\n",
        "    ################################\n",
        "    \"symbols_embedding_dim\":512,\n",
        "\n",
        "    # Encoder parameters\n",
        "    \"encoder_kernel_size\":5,\n",
        "    \"encoder_n_convolutions\":3,\n",
        "    \"encoder_embedding_dim\":128,\n",
        "\n",
        "    # Decoder parameters\n",
        "    \"n_frames_per_step\":1,  # currently only 1 is supported\n",
        "    \"decoder_rnn_dim\":256,\n",
        "    \"prenet_dim\":32,\n",
        "    \"max_decoder_steps\":1000,\n",
        "    \"gate_threshold\":0.5,\n",
        "    \"p_attention_dropout\":0.1,\n",
        "    \"p_decoder_dropout\":0.1,\n",
        "\n",
        "    # Attention parameters\n",
        "    \"attention_rnn_dim\":256,\n",
        "    \"attention_dim\":128,\n",
        "    \"attention_heads\": 4,\n",
        "\n",
        "    # Location Layer parameters\n",
        "    \"attention_location_n_filters\":32,\n",
        "    \"attention_location_kernel_size\":31,\n",
        "\n",
        "    # Mel-post processing network parameters\n",
        "    \"postnet_embedding_dim\":128,\n",
        "    \"postnet_kernel_size\":5,\n",
        "    \"postnet_n_convolutions\":2,\n",
        "\n",
        "    ################################\n",
        "    # Optimization Hyperparameters #\n",
        "    ################################\n",
        "    \"use_saved_learning_rate\":False,\n",
        "    \"learning_rate\":1e-3,\n",
        "    \"weight_decay\":1e-6,\n",
        "    \"grad_clip_thresh\":1.0,\n",
        "    \"batch_size\":32,\n",
        "    \"mask_padding\":True\n",
        "    # set model's padded outputs to padded values\n",
        "  }\n",
        "\n",
        "  hparams = mapDict(hparams)\n",
        "\n",
        "  return hparams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "st3RpVqf7JzX"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGZjv9RsEc4t"
      },
      "source": [
        "#### Reading Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "2jAkFk6g8HVQ"
      },
      "outputs": [],
      "source": [
        "def read_json(file_path):\n",
        "  with open(file_path, 'r') as file:\n",
        "    file_content = json.load(file)\n",
        "  return file_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "szfPsiJR6_BI"
      },
      "outputs": [],
      "source": [
        "train_data = read_json('/content/drive/MyDrive/Dubbing Project/libspeech/train_output_dict.json')\n",
        "valid_data = read_json('/content/drive/MyDrive/Dubbing Project/libspeech/valid_output_dict.json')\n",
        "reverse_phoneme = read_json('/content/drive/MyDrive/Dubbing Project/libspeech/reverse_phoneme_dict.json')\n",
        "phoneme_list = read_json('/content/drive/MyDrive/Dubbing Project/libspeech/phoneme_dict.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEOTSNoW76ti",
        "outputId": "2f7fd64c-8d83-4716-8e78-04d24ef0c08e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28539, 2703)"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ],
      "source": [
        "len(train_data.keys()), len(valid_data.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2F-56Nvm9tEO",
        "outputId": "6b76de9a-d8a2-43e6-bb53-49f4bc964d97"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('5678-43302-0000', dict_keys(['file_path', 'text', 'phonemes']))"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "source": [
        "list(train_data.keys())[0], train_data[list(train_data.keys())[0]].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-YPS22X9wD9",
        "outputId": "f774c25b-2235-4c04-dbdd-55041b5e025f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('6313-66125-0000', dict_keys(['file_path', 'text', 'phonemes']))"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ],
      "source": [
        "list(valid_data.keys())[0], valid_data[list(valid_data.keys())[0]].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyVREL0b8Srg",
        "outputId": "e2c5c8a5-28cc-46ba-c346-46af3865910d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Phonemes in reverse:  72\n"
          ]
        }
      ],
      "source": [
        "print(\"Total Phonemes in reverse: \", len(reverse_phoneme))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZumhJHiB8lWg",
        "outputId": "c9267e67-422c-4ef6-a7c8-97de36e90fc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Phonemes:  72\n"
          ]
        }
      ],
      "source": [
        "print(\"Total Phonemes: \", len(phoneme_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRf1F-ppat46",
        "outputId": "a47c74ef-c4af-41d5-d8d8-d3a46254e677"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'AA2': 0,\n",
              " 'CH': 1,\n",
              " 'ZH': 2,\n",
              " 'DH': 3,\n",
              " 'UW0': 4,\n",
              " 'W': 5,\n",
              " 'IY0': 6,\n",
              " 'OY0': 7,\n",
              " 'UH2': 8,\n",
              " 'D': 9,\n",
              " 'Y': 10,\n",
              " 'Z': 11,\n",
              " 'OY2': 12,\n",
              " 'HH': 13,\n",
              " 'AY2': 14,\n",
              " 'IH0': 15,\n",
              " 'EH1': 16,\n",
              " 'UW2': 17,\n",
              " 'UH0': 18,\n",
              " 'JH': 19,\n",
              " 'ER2': 20,\n",
              " 'V': 21,\n",
              " 'IY2': 22,\n",
              " 'G': 23,\n",
              " 'M': 24,\n",
              " 'AH1': 25,\n",
              " 'irish': 26,\n",
              " 'AA0': 27,\n",
              " 'AY0': 28,\n",
              " 'OW1': 29,\n",
              " 'ER1': 30,\n",
              " 'AO0': 31,\n",
              " 'EY1': 32,\n",
              " 'L': 33,\n",
              " 'AA1': 34,\n",
              " 'AY1': 35,\n",
              " 'SH': 36,\n",
              " 'ER0': 37,\n",
              " 'AH0': 38,\n",
              " 'UH1': 39,\n",
              " 'K': 40,\n",
              " 'S': 41,\n",
              " 'AH2': 42,\n",
              " 'AO2': 43,\n",
              " 'IY1': 44,\n",
              " 'OW0': 45,\n",
              " 'T': 46,\n",
              " 'IH1': 47,\n",
              " 'AW2': 48,\n",
              " 'EH0': 49,\n",
              " 'org,': 50,\n",
              " 'AO1': 51,\n",
              " 'NG': 52,\n",
              " 'EH2': 53,\n",
              " 'R': 54,\n",
              " '#': 55,\n",
              " 'EY0': 56,\n",
              " 'AW0': 57,\n",
              " 'AE2': 58,\n",
              " 'AE1': 59,\n",
              " 'OW2': 60,\n",
              " 'N': 61,\n",
              " 'B': 62,\n",
              " 'AE0': 63,\n",
              " 'F': 64,\n",
              " 'AW1': 65,\n",
              " 'P': 66,\n",
              " 'TH': 67,\n",
              " 'EY2': 68,\n",
              " 'IH2': 69,\n",
              " 'UW1': 70,\n",
              " 'OY1': 71}"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ],
      "source": [
        "phoneme_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "awVYC9p28oCP"
      },
      "outputs": [],
      "source": [
        "## Adding Stop Token\n",
        "phoneme_list['<end/>'] = 72\n",
        "reverse_phoneme[72] = '<end/>'\n",
        "\n",
        "phoneme_list['<start/>'] = 73\n",
        "reverse_phoneme[73] = '<start/>'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDCZn_3O833e",
        "outputId": "26f79faa-4963-4522-da67-9bddaef7eba7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Phonemes after adding reverse:  74\n",
            "Total Phonemes after adding:  74\n"
          ]
        }
      ],
      "source": [
        "print(\"Total Phonemes after adding reverse: \", len(reverse_phoneme))\n",
        "print(\"Total Phonemes after adding: \", len(phoneme_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFZHlP_ljuJj"
      },
      "source": [
        "#### Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "W1moqnozfmbx"
      },
      "outputs": [],
      "source": [
        "def load_wav_to_torch(full_path):\n",
        "    # print(full_path)\n",
        "    sampling_rate, data = read(full_path)\n",
        "    return torch.FloatTensor(data.astype(np.float32)), sampling_rate\n",
        "\n",
        "# def load_flac_to_torch(full_path):\n",
        "#     print(\"Path: \", full_path)\n",
        "#     # Read FLAC file using pydub\n",
        "#     audio = AudioSegment.from_file(full_path, format=\"flac\")\n",
        "\n",
        "#     # Convert to a numpy array\n",
        "#     data = audio.get_array_of_samples()\n",
        "\n",
        "#     # Save the data as a temporary WAV file\n",
        "\n",
        "#     temp_wav_path = \"temp.wav\"\n",
        "#     audio.export(temp_wav_path, format=\"wav\")\n",
        "\n",
        "#     # Read the temporary WAV file using scipy.io.wavfile.read\n",
        "#     sample_rate, data = read(temp_wav_path)\n",
        "#     return torch.FloatTensor(data.astype(np.float32)), sample_rate\n",
        "\n",
        "def load_flac_to_torch(full_path):\n",
        "    # Read FLAC file using pydub\n",
        "    audio = AudioSegment.from_file(full_path, format=\"flac\")\n",
        "\n",
        "    # Convert to a numpy array\n",
        "    data = np.array(audio.get_array_of_samples(), dtype=np.float32)\n",
        "\n",
        "    # Normalize the data to the range [-1, 1]\n",
        "    data /= np.abs(data).max()\n",
        "\n",
        "    # Convert numpy array to PyTorch tensor\n",
        "    tensor_data = torch.from_numpy(data)\n",
        "\n",
        "    # Print the sample rate\n",
        "    sample_rate = audio.frame_rate\n",
        "\n",
        "    return tensor_data, sample_rate\n",
        "\n",
        "def dynamic_range_compression(x, C=1, clip_val=1e-5):\n",
        "    \"\"\"\n",
        "    PARAMS\n",
        "    ------\n",
        "    C: compression factor\n",
        "    \"\"\"\n",
        "    return torch.log(torch.clamp(x, min=clip_val) * C)\n",
        "\n",
        "def dynamic_range_decompression(x, C=1):\n",
        "    \"\"\"\n",
        "    PARAMS\n",
        "    ------\n",
        "    C: compression factor used to compress\n",
        "    \"\"\"\n",
        "    return torch.exp(x) / C\n",
        "\n",
        "def window_sumsquare(window, n_frames, hop_length=200, win_length=800,\n",
        "                     n_fft=800, dtype=np.float32, norm=None):\n",
        "    \"\"\"\n",
        "    # from librosa 0.6\n",
        "    Compute the sum-square envelope of a window function at a given hop length.\n",
        "\n",
        "    This is used to estimate modulation effects induced by windowing\n",
        "    observations in short-time fourier transforms.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    window : string, tuple, number, callable, or list-like\n",
        "        Window specification, as in `get_window`\n",
        "\n",
        "    n_frames : int > 0\n",
        "        The number of analysis frames\n",
        "\n",
        "    hop_length : int > 0\n",
        "        The number of samples to advance between frames\n",
        "\n",
        "    win_length : [optional]\n",
        "        The length of the window function.  By default, this matches `n_fft`.\n",
        "\n",
        "    n_fft : int > 0\n",
        "        The length of each analysis frame.\n",
        "\n",
        "    dtype : np.dtype\n",
        "        The data type of the output\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    wss : np.ndarray, shape=`(n_fft + hop_length * (n_frames - 1))`\n",
        "        The sum-squared envelope of the window function\n",
        "    \"\"\"\n",
        "    if win_length is None:\n",
        "        win_length = n_fft\n",
        "\n",
        "    n = n_fft + hop_length * (n_frames - 1)\n",
        "    x = np.zeros(n, dtype=dtype)\n",
        "\n",
        "    # Compute the squared window at the desired length\n",
        "    win_sq = get_window(window, win_length, fftbins=True)\n",
        "    win_sq = librosa_util.normalize(win_sq, norm=norm)**2\n",
        "    win_sq = librosa_util.pad_center(win_sq, n_fft)\n",
        "\n",
        "    # Fill the envelope\n",
        "    for i in range(n_frames):\n",
        "        sample = i * hop_length\n",
        "        x[sample:min(n, sample + n_fft)] += win_sq[:max(0, min(n_fft, n - sample))]\n",
        "    return x\n",
        "\n",
        "\n",
        "def get_mask_from_lengths(lengths):\n",
        "    max_len = torch.max(lengths).item()\n",
        "    ids = torch.arange(0, max_len, out=torch.cuda.LongTensor(max_len))\n",
        "    mask = (ids < lengths.unsqueeze(1)).bool()\n",
        "    return mask\n",
        "\n",
        "\n",
        "def load_filepaths_and_text(filename, split=\"|\"):\n",
        "    with open(filename, encoding='utf-8') as f:\n",
        "        filepaths_and_text = [line.strip().split(split) for line in f]\n",
        "    return filepaths_and_text\n",
        "\n",
        "\n",
        "def to_gpu(x):\n",
        "    x = x.contiguous()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        x = x.cuda(non_blocking=True)\n",
        "    return torch.autograd.Variable(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "4vuS3LX3f4bN"
      },
      "outputs": [],
      "source": [
        "## stft.py\n",
        "\n",
        "\"\"\"\n",
        "BSD 3-Clause License\n",
        "\n",
        "Copyright (c) 2017, Prem Seetharaman\n",
        "All rights reserved.\n",
        "\n",
        "* Redistribution and use in source and binary forms, with or without\n",
        "  modification, are permitted provided that the following conditions are met:\n",
        "\n",
        "* Redistributions of source code must retain the above copyright notice,\n",
        "  this list of conditions and the following disclaimer.\n",
        "\n",
        "* Redistributions in binary form must reproduce the above copyright notice, this\n",
        "  list of conditions and the following disclaimer in the\n",
        "  documentation and/or other materials provided with the distribution.\n",
        "\n",
        "* Neither the name of the copyright holder nor the names of its\n",
        "  contributors may be used to endorse or promote products derived from this\n",
        "  software without specific prior written permission.\n",
        "\n",
        "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n",
        "ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n",
        "WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
        "DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR\n",
        "ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n",
        "(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n",
        "LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\n",
        "ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n",
        "(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n",
        "SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
        "\"\"\"\n",
        "\n",
        "class STFT(torch.nn.Module):\n",
        "    \"\"\"adapted from Prem Seetharaman's https://github.com/pseeth/pytorch-stft\"\"\"\n",
        "    def __init__(self, filter_length=800, hop_length=200, win_length=800,\n",
        "                 window='hann'):\n",
        "        super(STFT, self).__init__()\n",
        "        self.filter_length = filter_length\n",
        "        self.hop_length = hop_length\n",
        "        self.win_length = win_length\n",
        "        self.window = window\n",
        "        self.forward_transform = None\n",
        "        scale = self.filter_length / self.hop_length\n",
        "        fourier_basis = np.fft.fft(np.eye(self.filter_length))\n",
        "\n",
        "        cutoff = int((self.filter_length / 2 + 1))\n",
        "        fourier_basis = np.vstack([np.real(fourier_basis[:cutoff, :]),\n",
        "                                   np.imag(fourier_basis[:cutoff, :])])\n",
        "\n",
        "        forward_basis = torch.FloatTensor(fourier_basis[:, None, :])\n",
        "        inverse_basis = torch.FloatTensor(\n",
        "            np.linalg.pinv(scale * fourier_basis).T[:, None, :])\n",
        "\n",
        "        if window is not None:\n",
        "            assert(filter_length >= win_length)\n",
        "            # get window and zero center pad it to filter_length\n",
        "            fft_window = get_window(window, win_length, fftbins=True)\n",
        "            fft_window = pad_center(data=fft_window, size=filter_length)\n",
        "            fft_window = torch.from_numpy(fft_window).float()\n",
        "\n",
        "            # window the bases\n",
        "            forward_basis *= fft_window\n",
        "            inverse_basis *= fft_window\n",
        "\n",
        "        self.register_buffer('forward_basis', forward_basis.float())\n",
        "        self.register_buffer('inverse_basis', inverse_basis.float())\n",
        "\n",
        "    def transform(self, input_data):\n",
        "        num_batches = input_data.size(0)\n",
        "        num_samples = input_data.size(1)\n",
        "\n",
        "        self.num_samples = num_samples\n",
        "\n",
        "        # similar to librosa, reflect-pad the input\n",
        "        input_data = input_data.view(num_batches, 1, num_samples)\n",
        "        input_data = F.pad(\n",
        "            input_data.unsqueeze(1),\n",
        "            (int(self.filter_length / 2), int(self.filter_length / 2), 0, 0),\n",
        "            mode='reflect')\n",
        "        input_data = input_data.squeeze(1)\n",
        "\n",
        "        forward_transform = F.conv1d(\n",
        "            input_data,\n",
        "            Variable(self.forward_basis, requires_grad=False),\n",
        "            stride=self.hop_length,\n",
        "            padding=0)\n",
        "\n",
        "        cutoff = int((self.filter_length / 2) + 1)\n",
        "        real_part = forward_transform[:, :cutoff, :]\n",
        "        imag_part = forward_transform[:, cutoff:, :]\n",
        "\n",
        "        magnitude = torch.sqrt(real_part**2 + imag_part**2)\n",
        "        phase = torch.autograd.Variable(\n",
        "            torch.atan2(imag_part.data, real_part.data))\n",
        "\n",
        "        return magnitude, phase\n",
        "\n",
        "    def inverse(self, magnitude, phase):\n",
        "        recombine_magnitude_phase = torch.cat(\n",
        "            [magnitude*torch.cos(phase), magnitude*torch.sin(phase)], dim=1)\n",
        "\n",
        "        inverse_transform = F.conv_transpose1d(\n",
        "            recombine_magnitude_phase,\n",
        "            Variable(self.inverse_basis, requires_grad=False),\n",
        "            stride=self.hop_length,\n",
        "            padding=0)\n",
        "\n",
        "        if self.window is not None:\n",
        "            window_sum = window_sumsquare(\n",
        "                self.window, magnitude.size(-1), hop_length=self.hop_length,\n",
        "                win_length=self.win_length, n_fft=self.filter_length,\n",
        "                dtype=np.float32)\n",
        "            # remove modulation effects\n",
        "            approx_nonzero_indices = torch.from_numpy(\n",
        "                np.where(window_sum > tiny(window_sum))[0])\n",
        "            window_sum = torch.autograd.Variable(\n",
        "                torch.from_numpy(window_sum), requires_grad=False)\n",
        "            window_sum = window_sum.cuda() if magnitude.is_cuda else window_sum\n",
        "            inverse_transform[:, :, approx_nonzero_indices] /= window_sum[approx_nonzero_indices]\n",
        "\n",
        "            # scale by hop ratio\n",
        "            inverse_transform *= float(self.filter_length) / self.hop_length\n",
        "\n",
        "        inverse_transform = inverse_transform[:, :, int(self.filter_length/2):]\n",
        "        inverse_transform = inverse_transform[:, :, :-int(self.filter_length/2):]\n",
        "\n",
        "        return inverse_transform\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        self.magnitude, self.phase = self.transform(input_data)\n",
        "        reconstruction = self.inverse(self.magnitude, self.phase)\n",
        "        return reconstruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "Cs085jA5fyzG"
      },
      "outputs": [],
      "source": [
        "class TacotronSTFT(torch.nn.Module):\n",
        "    def __init__(self, filter_length=1024, hop_length=256, win_length=1024,\n",
        "                 n_mel_channels=80, sampling_rate=22050, mel_fmin=0.0,\n",
        "                 mel_fmax=8000.0):\n",
        "        super(TacotronSTFT, self).__init__()\n",
        "        self.n_mel_channels = n_mel_channels\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.stft_fn = STFT(filter_length, hop_length, win_length)\n",
        "        mel_basis = librosa_mel_fn(\n",
        "            sr=sampling_rate, n_fft=filter_length, n_mels=n_mel_channels, fmin=mel_fmin, fmax=mel_fmax)\n",
        "        mel_basis = torch.from_numpy(mel_basis).float()\n",
        "        self.register_buffer('mel_basis', mel_basis)\n",
        "\n",
        "    def spectral_normalize(self, magnitudes):\n",
        "        output = dynamic_range_compression(magnitudes)\n",
        "        return output\n",
        "\n",
        "    def spectral_de_normalize(self, magnitudes):\n",
        "        output = dynamic_range_decompression(magnitudes)\n",
        "        return output\n",
        "\n",
        "    def spectrogram(self,y):\n",
        "        assert(torch.min(y.data) >= -1)\n",
        "        assert(torch.max(y.data) <= 1)\n",
        "\n",
        "        magnitudes, phases = self.stft_fn.transform(y)\n",
        "        magnitudes = magnitudes.data\n",
        "\n",
        "        return magnitudes\n",
        "\n",
        "    def mel_spectrogram(self, y):\n",
        "        \"\"\"Computes mel-spectrograms from a batch of waves\n",
        "        PARAMS\n",
        "        ------\n",
        "        y: Variable(torch.FloatTensor) with shape (B, T) in range [-1, 1]\n",
        "\n",
        "        RETURNS\n",
        "        -------\n",
        "        mel_output: torch.FloatTensor of shape (B, n_mel_channels, T)\n",
        "        \"\"\"\n",
        "        # assert(torch.min(y.data) >= -1)\n",
        "        # assert(torch.max(y.data) <= 1)\n",
        "        magnitudes, phases = self.stft_fn.transform(y)\n",
        "        magnitudes = magnitudes.data\n",
        "        mel_output = torch.matmul(self.mel_basis, magnitudes)\n",
        "        mel_output = self.spectral_normalize(mel_output)\n",
        "        return mel_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "5kuyVHFb9ieA"
      },
      "outputs": [],
      "source": [
        "train_audio_list = [i['file_path'] for i in train_data.values()]\n",
        "train_phonemes_list = [i['phonemes'] for i in train_data.values()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "Ssf1kwMtRqJ2"
      },
      "outputs": [],
      "source": [
        "### data_utils.py\n",
        "class AudioPhonemeDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "        1) loads audio,text pairs\n",
        "        2) normalizes text and converts them to sequences of one-hot vectors\n",
        "        3) computes mel-spectrograms from audio files.\n",
        "    \"\"\"\n",
        "    def __init__(self, audio_paths, phoneme_list, hparams):\n",
        "        self.inputs = audio_paths\n",
        "        self.outputs = phoneme_list\n",
        "        self.text_cleaners = hparams.text_cleaners\n",
        "        self.max_wav_value = hparams.max_wav_value\n",
        "        self.sampling_rate = hparams.sampling_rate\n",
        "        self.load_mel_from_disk = hparams.load_mel_from_disk\n",
        "        self.stft = TacotronSTFT(\n",
        "            hparams.filter_length, hparams.hop_length, hparams.win_length,\n",
        "            hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,\n",
        "            hparams.mel_fmax)\n",
        "\n",
        "    def get_mel_spec_pair(self, index):\n",
        "        # separate filename and text\n",
        "        # lin = self.get_spec(self.outputs[index])\n",
        "        # mel = self.get_mel(self.inputs[index])\n",
        "        inputs = self.get_mel(self.inputs[index])\n",
        "        outputs = self.get_phonemes_list(self.outputs[index])\n",
        "\n",
        "        return (inputs,outputs)\n",
        "\n",
        "    def get_mel(self, filename):\n",
        "        if not self.load_mel_from_disk:\n",
        "            audio, sampling_rate = load_flac_to_torch(filename)\n",
        "            # audio = audio[:, 0]\n",
        "            # if sampling_rate != self.stft.sampling_rate:\n",
        "            #     raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
        "            #         sampling_rate, self.stft.sampling_rate))\n",
        "            audio_norm = audio / self.max_wav_value\n",
        "            audio_norm = audio_norm.unsqueeze(0)\n",
        "            audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
        "            melspec = self.stft.mel_spectrogram(audio_norm)\n",
        "            melspec = torch.squeeze(melspec, 0)\n",
        "            melspec_arr = melspec[0].numpy()\n",
        "        else:\n",
        "            melspec = torch.from_numpy(np.load(filename))\n",
        "            assert melspec.size(0) == self.stft.n_mel_channels, (\n",
        "                'Mel dimension mismatch: given {}, expected {}'.format(\n",
        "                    melspec.size(0), self.stft.n_mel_channels))\n",
        "\n",
        "        return melspec\n",
        "\n",
        "    def get_phonemes_list(self, phonemes):\n",
        "        phonemes.insert(0, '<start/>')\n",
        "        phonemes.append('<end/>')\n",
        "\n",
        "        tensor_phonemes = torch.zeros(hparams.vocab_size, len(phonemes))\n",
        "\n",
        "        for i, phoneme in enumerate(phonemes):\n",
        "            tensor_phonemes[phoneme_list[phoneme], i] = 1\n",
        "\n",
        "        return tensor_phonemes\n",
        "\n",
        "\n",
        "    def get_spec(self, filename):\n",
        "        if not self.load_mel_from_disk:\n",
        "            audio, sampling_rate = load_wav_to_torch(filename)\n",
        "            if sampling_rate != self.stft.sampling_rate:\n",
        "                raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
        "                    sampling_rate, self.stft.sampling_rate))\n",
        "            audio_norm = audio / self.max_wav_value\n",
        "            audio_norm = audio_norm.unsqueeze(0)\n",
        "            audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
        "            spec = self.stft.spectrogram(audio_norm)\n",
        "            spec = torch.squeeze(spec, 0)\n",
        "        else:\n",
        "            spec = torch.from_numpy(np.load(filename))\n",
        "\n",
        "        return spec\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.get_mel_spec_pair(index)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "\n",
        "class AudioPhonemesCollate():\n",
        "    \"\"\" Zero-pads model inputs and targets based on number of frames per setep\n",
        "    \"\"\"\n",
        "    def __init__(self, n_frames_per_step):\n",
        "        self.n_frames_per_step = n_frames_per_step\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        \"\"\"Collate's training batch from normalized text and mel-spectrogram\n",
        "        PARAMS\n",
        "        ------\n",
        "        batch: [mel_normalized, text_normalized]\n",
        "        \"\"\"\n",
        "\n",
        "        num_mels = batch[0][0].size(0)\n",
        "\n",
        "\n",
        "        # Sort sequences by input length in descending order\n",
        "        input_lengths, ids_sorted_decreasing = torch.sort(\n",
        "            torch.LongTensor([x[0].size(1) for x in batch]),\n",
        "            dim=0, descending=True\n",
        "        )\n",
        "\n",
        "        max_input_len = input_lengths[0]\n",
        "\n",
        "        # Pad input sequences\n",
        "        input_padded = torch.FloatTensor(len(batch), num_mels, max_input_len)\n",
        "        input_padded.zero_()\n",
        "        for i in ids_sorted_decreasing:\n",
        "            mel = batch[i][0]\n",
        "            input_padded[i, :, :mel.size(1)] = mel\n",
        "\n",
        "        max_target_len = max([x[1].size(1) for x in batch])\n",
        "\n",
        "        # include mel padded and gate padded\n",
        "        target_padded = torch.FloatTensor(len(batch), hparams.vocab_size, max_target_len)\n",
        "        target_padded.zero_()\n",
        "        gate_padded = torch.FloatTensor(len(batch), max_target_len)\n",
        "        gate_padded.zero_()\n",
        "        output_lengths = torch.LongTensor(len(batch))\n",
        "\n",
        "        for i in ids_sorted_decreasing:\n",
        "            target = batch[i][1]\n",
        "            target_padded[i, :, :target.size(1)] = target\n",
        "            gate_padded[i, :len(target)] = 1\n",
        "            output_lengths[i] = len(target)\n",
        "\n",
        "        return input_padded, input_lengths, target_padded, gate_padded, \\\n",
        "            output_lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "hDl-pVB5UO0Q"
      },
      "outputs": [],
      "source": [
        "hparams = create_hparams()\n",
        "\n",
        "torch.manual_seed(hparams.seed)\n",
        "torch.cuda.manual_seed(hparams.seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "IcdZxG3-XsqR"
      },
      "outputs": [],
      "source": [
        "hparams.vocab_size = len(phoneme_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "uAFn9E1cdvDT"
      },
      "outputs": [],
      "source": [
        "for audio in train_audio_list:\n",
        "  if not audio:\n",
        "    print(audio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "UsGWWvX-TNKd"
      },
      "outputs": [],
      "source": [
        "train_audio_list = [i['file_path'] for i in train_data.values()]\n",
        "train_phonemes_list = [i['phonemes'] for i in train_data.values()]\n",
        "\n",
        "train_dataset = AudioPhonemeDataset(train_audio_list, train_phonemes_list, hparams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "It-9DxY7a1Nw"
      },
      "outputs": [],
      "source": [
        "valid_audio_list = [i['file_path'] for i in valid_data.values()]\n",
        "valid_phonemes_list = [i['phonemes'] for i in valid_data.values()]\n",
        "\n",
        "val_dataset = AudioPhonemeDataset(valid_audio_list, valid_phonemes_list, hparams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "QTBsasJcTVf4"
      },
      "outputs": [],
      "source": [
        "audio, phonemes = train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnCIHgcEVgzn",
        "outputId": "f39277dd-5102-4348-8732-b1785ac9a3c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([80, 937]), torch.Size([74, 157]))"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ],
      "source": [
        "audio.shape, phonemes.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "nmzfSPl1Wvg6"
      },
      "outputs": [],
      "source": [
        "collate_fn = AudioPhonemesCollate(hparams.n_frames_per_step)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, num_workers=2, shuffle=True,\n",
        "                          batch_size=3, pin_memory=False,\n",
        "                          drop_last=True, collate_fn=collate_fn)\n",
        "\n",
        "valid_dataloader = DataLoader(val_dataset, num_workers=2, shuffle=True,\n",
        "                          batch_size=3, pin_memory=False,\n",
        "                          drop_last=True, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xH-RG8rCaEz4",
        "outputId": "c833c9f5-0e6a-4aa5-9e13-4003f537d14b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs Shape:  torch.Size([3, 80, 926]) torch.Size([3])\n",
            "Outputs: \n",
            "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         [1., 0., 0.,  ..., 0., 0., 0.]]])\n",
            "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "for i, batch in enumerate(train_loader):\n",
        "  input_padded, input_lengths, target_padded, gate_padded, \\\n",
        "            output_lengths = batch\n",
        "\n",
        "  print(\"Inputs Shape: \", input_padded.shape, input_lengths.shape)\n",
        "\n",
        "  print(\"Outputs: \",)\n",
        "  print(target_padded)\n",
        "  print(gate_padded)\n",
        "\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWlOX8Kxj9fl"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "4uXHgfbGaJcG"
      },
      "outputs": [],
      "source": [
        "## layers.py\n",
        "\n",
        "class LinearNorm(torch.nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, bias=True, w_init_gain='linear'):\n",
        "        super(LinearNorm, self).__init__()\n",
        "        self.linear_layer = torch.nn.Linear(in_dim, out_dim, bias=bias)\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(\n",
        "            self.linear_layer.weight,\n",
        "            gain=torch.nn.init.calculate_gain(w_init_gain))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear_layer(x)\n",
        "\n",
        "\n",
        "class ConvNorm(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,\n",
        "                 padding=None, dilation=1, bias=True, w_init_gain='linear'):\n",
        "        super(ConvNorm, self).__init__()\n",
        "        if padding is None:\n",
        "            assert(kernel_size % 2 == 1)\n",
        "            padding = int(dilation * (kernel_size - 1) / 2)\n",
        "\n",
        "        self.conv = torch.nn.Conv1d(in_channels, out_channels,\n",
        "                                    kernel_size=kernel_size, stride=stride,\n",
        "                                    padding=padding, dilation=dilation,\n",
        "                                    bias=bias)\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(\n",
        "            self.conv.weight, gain=torch.nn.init.calculate_gain(w_init_gain))\n",
        "\n",
        "    def forward(self, signal):\n",
        "        conv_signal = self.conv(signal)\n",
        "        return conv_signal\n",
        "\n",
        "\n",
        "class TacotronSTFT(torch.nn.Module):\n",
        "    def __init__(self, filter_length=1024, hop_length=256, win_length=1024,\n",
        "                 n_mel_channels=80, sampling_rate=22050, mel_fmin=0.0,\n",
        "                 mel_fmax=8000.0):\n",
        "        super(TacotronSTFT, self).__init__()\n",
        "        self.n_mel_channels = n_mel_channels\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.stft_fn = STFT(filter_length, hop_length, win_length)\n",
        "        mel_basis = librosa_mel_fn(\n",
        "            sr=sampling_rate, n_fft=filter_length, n_mels=n_mel_channels, fmin=mel_fmin, fmax=mel_fmax)\n",
        "        mel_basis = torch.from_numpy(mel_basis).float()\n",
        "        self.register_buffer('mel_basis', mel_basis)\n",
        "\n",
        "    def spectral_normalize(self, magnitudes):\n",
        "        output = dynamic_range_compression(magnitudes)\n",
        "        return output\n",
        "\n",
        "    def spectral_de_normalize(self, magnitudes):\n",
        "        output = dynamic_range_decompression(magnitudes)\n",
        "        return output\n",
        "\n",
        "    def spectrogram(self,y):\n",
        "        assert(torch.min(y.data) >= -1)\n",
        "        assert(torch.max(y.data) <= 1)\n",
        "\n",
        "        magnitudes, phases = self.stft_fn.transform(y)\n",
        "        magnitudes = magnitudes.data\n",
        "\n",
        "        return magnitudes\n",
        "\n",
        "    def mel_spectrogram(self, y):\n",
        "        \"\"\"Computes mel-spectrograms from a batch of waves\n",
        "        PARAMS\n",
        "        ------\n",
        "        y: Variable(torch.FloatTensor) with shape (B, T) in range [-1, 1]\n",
        "\n",
        "        RETURNS\n",
        "        -------\n",
        "        mel_output: torch.FloatTensor of shape (B, n_mel_channels, T)\n",
        "        \"\"\"\n",
        "        # assert(torch.min(y.data) >= -1)\n",
        "        # assert(torch.max(y.data) <= 1)\n",
        "        magnitudes, phases = self.stft_fn.transform(y)\n",
        "        magnitudes = magnitudes.data\n",
        "        mel_output = torch.matmul(self.mel_basis, magnitudes)\n",
        "        mel_output = self.spectral_normalize(mel_output)\n",
        "        return mel_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "tUncKz0ckAGB"
      },
      "outputs": [],
      "source": [
        "## model.py\n",
        "\n",
        "class LocationLayer(nn.Module):\n",
        "    def __init__(self, attention_n_filters, attention_kernel_size,\n",
        "                 attention_dim):\n",
        "        super(LocationLayer, self).__init__()\n",
        "        padding = int((attention_kernel_size - 1) / 2)\n",
        "        self.location_conv = ConvNorm(2, attention_n_filters,\n",
        "                                      kernel_size=attention_kernel_size,\n",
        "                                      padding=padding, bias=False, stride=1,\n",
        "                                      dilation=1)\n",
        "        self.location_dense = LinearNorm(attention_n_filters, attention_dim,\n",
        "                                         bias=False, w_init_gain='tanh')\n",
        "\n",
        "    def forward(self, attention_weights_cat):\n",
        "        processed_attention = self.location_conv(attention_weights_cat)\n",
        "        processed_attention = processed_attention.transpose(1, 2)\n",
        "        processed_attention = self.location_dense(processed_attention)\n",
        "        return processed_attention\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, attention_rnn_dim, embedding_dim, attention_dim,\n",
        "                 attention_location_n_filters, attention_location_kernel_size,num_heads=4):\n",
        "        super(Attention, self).__init__()\n",
        "        self.query_layer = LinearNorm(attention_rnn_dim, attention_dim,\n",
        "                                      bias=False, w_init_gain='tanh')\n",
        "        self.memory_layer = LinearNorm(embedding_dim, attention_dim, bias=False,\n",
        "                                       w_init_gain='tanh')\n",
        "        # self.v = LinearNorm(attention_dim, 1, bias=False)\n",
        "        self.attention = nn.MultiheadAttention(attention_dim,num_heads,batch_first=True)\n",
        "        self.location_layer = LocationLayer(attention_location_n_filters,\n",
        "                                            attention_location_kernel_size,\n",
        "                                            attention_dim)\n",
        "        self.score_mask_value = -float(\"inf\")\n",
        "\n",
        "    def get_alignment_energies(self, query, processed_memory,\n",
        "                               attention_weights_cat):\n",
        "        \"\"\"\n",
        "        PARAMS\n",
        "        ------\n",
        "        query: decoder output (batch, n_mel_channels * n_frames_per_step)\n",
        "        processed_memory: processed encoder outputs (B, T_in, attention_dim)\n",
        "        attention_weights_cat: cumulative and prev. att weights (B, 2, max_time)\n",
        "\n",
        "        RETURNS\n",
        "        -------\n",
        "        alignment (batch, max_time)\n",
        "        \"\"\"\n",
        "\n",
        "        processed_query = self.query_layer(query.unsqueeze(1))\n",
        "        processed_attention_weights = self.location_layer(attention_weights_cat)\n",
        "        # energies = self.v(torch.tanh(\n",
        "        #     processed_query + processed_attention_weights + processed_memory))\n",
        "        # energies = energies.squeeze(-1)\n",
        "        return processed_query,processed_attention_weights\n",
        "\n",
        "    def forward(self, attention_hidden_state, memory, processed_memory,\n",
        "                attention_weights_cat, mask):\n",
        "        \"\"\"\n",
        "        PARAMS\n",
        "        ------\n",
        "        attention_hidden_state: attention rnn last output\n",
        "        memory: encoder outputs\n",
        "        processed_memory: processed encoder outputs\n",
        "        attention_weights_cat: previous and cummulative attention weights\n",
        "        mask: binary mask for padded data\n",
        "        \"\"\"\n",
        "        # alignment = self.get_alignment_energies(\n",
        "            # attention_hidden_state, processed_memory, attention_weights_cat)\n",
        "\n",
        "        processed_query,processed_attention_weights = self.get_alignment_energies(\n",
        "        attention_hidden_state, processed_memory, attention_weights_cat)\n",
        "\n",
        "        # if mask is not None:\n",
        "        #     alignment.data.masked_fill_(mask, self.score_mask_value)\n",
        "\n",
        "        # attention_weights = F.softmax(alignment, dim=1)\n",
        "        # attention_context = torch.bmm(attention_weights.unsqueeze(1), memory)\n",
        "        # attention_context = attention_context.squeeze(1)\n",
        "\n",
        "        attention_context,attention_weights = self.attention(processed_query,processed_attention_weights,processed_memory)\n",
        "        return attention_context.squeeze(1), attention_weights.squeeze(1)\n",
        "\n",
        "\n",
        "# class Attention(nn.Module):\n",
        "#     def __init__(self, attention_rnn_dim, embedding_dim, attention_dim,\n",
        "#                  attention_location_n_filters, attention_location_kernel_size):\n",
        "#         super(Attention, self).__init__()\n",
        "#         self.query_layer = LinearNorm(attention_rnn_dim, attention_dim,\n",
        "#                                       bias=False, w_init_gain='tanh')\n",
        "#         self.memory_layer = LinearNorm(embedding_dim, attention_dim, bias=False,\n",
        "#                                        w_init_gain='tanh')\n",
        "#         self.v = LinearNorm(attention_dim, 1, bias=False)\n",
        "#         self.location_layer = LocationLayer(attention_location_n_filters,\n",
        "#                                             attention_location_kernel_size,\n",
        "#                                             attention_dim)\n",
        "#         self.score_mask_value = -float(\"inf\")\n",
        "\n",
        "#     def get_alignment_energies(self, query, processed_memory,\n",
        "#                                attention_weights_cat):\n",
        "#         \"\"\"\n",
        "#         PARAMS\n",
        "#         ------\n",
        "#         query: decoder output (batch, n_mel_channels * n_frames_per_step)\n",
        "#         processed_memory: processed encoder outputs (B, T_in, attention_dim)\n",
        "#         attention_weights_cat: cumulative and prev. att weights (B, 2, max_time)\n",
        "\n",
        "#         RETURNS\n",
        "#         -------\n",
        "#         alignment (batch, max_time)\n",
        "#         \"\"\"\n",
        "\n",
        "#         processed_query = self.query_layer(query.unsqueeze(1))\n",
        "#         processed_attention_weights = self.location_layer(attention_weights_cat)\n",
        "#         energies = self.v(torch.tanh(\n",
        "#             processed_query + processed_attention_weights + processed_memory))\n",
        "\n",
        "#         energies = energies.squeeze(-1)\n",
        "#         return energies\n",
        "\n",
        "#     def forward(self, attention_hidden_state, memory, processed_memory,\n",
        "#                 attention_weights_cat, mask):\n",
        "#         \"\"\"\n",
        "#         PARAMS\n",
        "#         ------\n",
        "#         attention_hidden_state: attention rnn last output\n",
        "#         memory: encoder outputs\n",
        "#         processed_memory: processed encoder outputs\n",
        "#         attention_weights_cat: previous and cummulative attention weights\n",
        "#         mask: binary mask for padded data\n",
        "#         \"\"\"\n",
        "#         alignment = self.get_alignment_energies(\n",
        "#             attention_hidden_state, processed_memory, attention_weights_cat)\n",
        "\n",
        "#         if mask is not None:\n",
        "#             alignment.data.masked_fill_(mask, self.score_mask_value)\n",
        "\n",
        "#         attention_weights = F.softmax(alignment, dim=1)\n",
        "#         attention_context = torch.bmm(attention_weights.unsqueeze(1), memory)\n",
        "#         attention_context = attention_context.squeeze(1)\n",
        "\n",
        "#         return attention_context, attention_weights\n",
        "\n",
        "\n",
        "class Prenet(nn.Module):\n",
        "    def __init__(self, in_dim, sizes):\n",
        "        super(Prenet, self).__init__()\n",
        "        in_sizes = [in_dim] + sizes[:-1]\n",
        "        self.layers = nn.ModuleList(\n",
        "            [LinearNorm(in_size, out_size, bias=False)\n",
        "             for (in_size, out_size) in zip(in_sizes, sizes)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for linear in self.layers:\n",
        "            x = F.dropout(F.relu(linear(x)), p=0.5, training=True)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Postnet(nn.Module):\n",
        "    \"\"\"Postnet\n",
        "        - Five 1-d convolution with 512 channels and kernel size 5\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hparams):\n",
        "        super(Postnet, self).__init__()\n",
        "        self.convolutions = nn.ModuleList()\n",
        "\n",
        "        self.convolutions.append(\n",
        "            nn.Sequential(\n",
        "                ConvNorm(hparams.vocab_size, hparams.postnet_embedding_dim,\n",
        "                         kernel_size=hparams.postnet_kernel_size, stride=1,\n",
        "                         padding=int((hparams.postnet_kernel_size - 1) / 2),\n",
        "                         dilation=1, w_init_gain='tanh'),\n",
        "                nn.BatchNorm1d(hparams.postnet_embedding_dim))\n",
        "        )\n",
        "\n",
        "        for i in range(1, hparams.postnet_n_convolutions - 1):\n",
        "            self.convolutions.append(\n",
        "                nn.Sequential(\n",
        "                    ConvNorm(hparams.postnet_embedding_dim,\n",
        "                             hparams.postnet_embedding_dim,\n",
        "                             kernel_size=hparams.postnet_kernel_size, stride=1,\n",
        "                             padding=int((hparams.postnet_kernel_size - 1) / 2),\n",
        "                             dilation=1, w_init_gain='tanh'),\n",
        "                    nn.BatchNorm1d(hparams.postnet_embedding_dim))\n",
        "            )\n",
        "\n",
        "        self.convolutions.append(\n",
        "            nn.Sequential(\n",
        "                ConvNorm(hparams.postnet_embedding_dim, hparams.vocab_size,\n",
        "                         kernel_size=hparams.postnet_kernel_size, stride=1,\n",
        "                         padding=int((hparams.postnet_kernel_size - 1) / 2),\n",
        "                         dilation=1, w_init_gain='linear'),\n",
        "                nn.BatchNorm1d(hparams.vocab_size))\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(len(self.convolutions) - 1):\n",
        "            x = F.dropout(torch.tanh(self.convolutions[i](x)), 0.5, self.training)\n",
        "        x = F.dropout(self.convolutions[-1](x), 0.5, self.training)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"Encoder module:\n",
        "        - Three 1-d convolution banks\n",
        "        - Bidirectional LSTM\n",
        "    \"\"\"\n",
        "    def __init__(self, hparams):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        # convolutions = []\n",
        "        # for _ in range(hparams.encoder_n_convolutions):\n",
        "        #     conv_layer = nn.Sequential(\n",
        "        #         ConvNorm(hparams.encoder_embedding_dim,\n",
        "        #                  hparams.encoder_embedding_dim,\n",
        "        #                  kernel_size=hparams.encoder_kernel_size, stride=1,\n",
        "        #                  padding=int((hparams.encoder_kernel_size - 1) / 2),\n",
        "        #                  dilation=1, w_init_gain='relu'),\n",
        "        #         nn.BatchNorm1d(hparams.encoder_embedding_dim))\n",
        "        #     convolutions.append(conv_layer)\n",
        "        # self.convolutions = nn.ModuleList(convolutions)\n",
        "\n",
        "        self.lstm = nn.LSTM(hparams.encoder_embedding_dim,\n",
        "                            int(hparams.encoder_embedding_dim/2), num_layers= 1, # According to paper it should be 8\n",
        "                            batch_first=True, bidirectional=True)\n",
        "\n",
        "    def forward(self, x, input_lengths):\n",
        "        # for conv in self.convolutions:\n",
        "        #     x = F.dropout(F.relu(conv(x)), 0.5, self.training)\n",
        "\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        # pytorch tensor are not reversible, hence the conversion\n",
        "        input_lengths = input_lengths.cpu().numpy()\n",
        "        x = nn.utils.rnn.pack_padded_sequence(\n",
        "            x, input_lengths, batch_first=True)\n",
        "\n",
        "        self.lstm.flatten_parameters()\n",
        "        outputs, _ = self.lstm(x)\n",
        "\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(\n",
        "            outputs, batch_first=True)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def inference(self, x):\n",
        "        # for conv in self.convolutions:\n",
        "        #     x = F.dropout(F.relu(conv(x)), 0.5, self.training)\n",
        "\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        self.lstm.flatten_parameters()\n",
        "        outputs, _ = self.lstm(x)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class PhonemesDecoder(nn.Module):\n",
        "    def __init__(self, hparams):\n",
        "        super(PhonemesDecoder, self).__init__()\n",
        "        self.n_mel_channels = hparams.n_mel_channels\n",
        "        self.out_channels = hparams.out_channels\n",
        "        self.n_frames_per_step = hparams.n_frames_per_step\n",
        "        self.encoder_embedding_dim = hparams.encoder_embedding_dim\n",
        "        self.attention_rnn_dim = hparams.attention_rnn_dim\n",
        "        self.decoder_rnn_dim = hparams.decoder_rnn_dim\n",
        "        self.prenet_dim = hparams.prenet_dim\n",
        "        self.max_decoder_steps = hparams.max_decoder_steps\n",
        "        self.gate_threshold = hparams.gate_threshold\n",
        "        self.p_attention_dropout = hparams.p_attention_dropout\n",
        "        self.p_decoder_dropout = hparams.p_decoder_dropout\n",
        "        self.attention_dim = hparams.attention_dim\n",
        "\n",
        "        self.prenet = Prenet(\n",
        "            hparams.vocab_size,\n",
        "            [hparams.prenet_dim, hparams.prenet_dim])\n",
        "\n",
        "        self.attention_rnn = nn.LSTMCell(\n",
        "            hparams.prenet_dim + hparams.encoder_embedding_dim,\n",
        "            hparams.attention_rnn_dim)\n",
        "\n",
        "        self.attention_layer = Attention(\n",
        "            hparams.attention_rnn_dim, hparams.encoder_embedding_dim,\n",
        "            hparams.attention_dim, hparams.attention_location_n_filters,\n",
        "            hparams.attention_location_kernel_size)\n",
        "\n",
        "        self.decoder_rnn = nn.LSTMCell(\n",
        "            hparams.attention_rnn_dim + hparams.encoder_embedding_dim,\n",
        "            hparams.decoder_rnn_dim, 1)\n",
        "\n",
        "        self.linear_projection = LinearNorm(\n",
        "            hparams.decoder_rnn_dim + hparams.encoder_embedding_dim,\n",
        "            hparams.vocab_size, w_init_gain='sigmoid')\n",
        "\n",
        "        self.gate_layer = LinearNorm(\n",
        "            hparams.decoder_rnn_dim + hparams.encoder_embedding_dim, 1,\n",
        "            bias=True, w_init_gain='sigmoid')\n",
        "\n",
        "    def get_go_frame(self, memory):\n",
        "        \"\"\" Gets all zeros frames to use as first decoder input\n",
        "        PARAMS\n",
        "        ------\n",
        "        memory: decoder outputs\n",
        "\n",
        "        RETURNS\n",
        "        -------\n",
        "        decoder_input: all zeros frames\n",
        "        \"\"\"\n",
        "        B = memory.size(0)\n",
        "        decoder_input = Variable(memory.data.new(\n",
        "            B, hparams.vocab_size).zero_())\n",
        "        return decoder_input\n",
        "\n",
        "    def initialize_decoder_states(self, memory, mask):\n",
        "        \"\"\" Initializes attention rnn states, decoder rnn states, attention\n",
        "        weights, attention cumulative weights, attention context, stores memory\n",
        "        and stores processed memory\n",
        "        PARAMS\n",
        "        ------\n",
        "        memory: Encoder outputs\n",
        "        mask: Mask for padded data if training, expects None for inference\n",
        "        \"\"\"\n",
        "        B = memory.size(0)\n",
        "        MAX_TIME = memory.size(1)\n",
        "\n",
        "        self.attention_hidden = Variable(memory.data.new(\n",
        "            B, self.attention_rnn_dim).zero_())\n",
        "        self.attention_cell = Variable(memory.data.new(\n",
        "            B, self.attention_rnn_dim).zero_())\n",
        "\n",
        "        self.decoder_hidden = Variable(memory.data.new(\n",
        "            B, self.decoder_rnn_dim).zero_())\n",
        "        self.decoder_cell = Variable(memory.data.new(\n",
        "            B, self.decoder_rnn_dim).zero_())\n",
        "\n",
        "        self.attention_weights = Variable(memory.data.new(\n",
        "            B, MAX_TIME).zero_())\n",
        "        self.attention_weights_cum = Variable(memory.data.new(\n",
        "            B, MAX_TIME).zero_())\n",
        "        self.attention_context = Variable(memory.data.new(\n",
        "            B, self.encoder_embedding_dim).zero_())\n",
        "\n",
        "        self.memory = memory\n",
        "        self.processed_memory = self.attention_layer.memory_layer(memory)\n",
        "        self.mask = mask\n",
        "\n",
        "    def parse_decoder_inputs(self, decoder_inputs):\n",
        "        \"\"\" Prepares decoder inputs, i.e. mel outputs\n",
        "        PARAMS\n",
        "        ------\n",
        "        decoder_inputs: inputs used for teacher-forced training, i.e. mel-specs\n",
        "\n",
        "        RETURNS\n",
        "        -------\n",
        "        inputs: processed decoder inputs\n",
        "\n",
        "        \"\"\"\n",
        "        # (B, n_mel_channels, T_out) -> (B, T_out, n_mel_channels)\n",
        "        decoder_inputs = decoder_inputs.transpose(1, 2)\n",
        "        decoder_inputs = decoder_inputs.view(\n",
        "            decoder_inputs.size(0),\n",
        "            int(decoder_inputs.size(1)/self.n_frames_per_step), -1)\n",
        "        # (B, T_out, n_mel_channels) -> (T_out, B, n_mel_channels)\n",
        "        decoder_inputs = decoder_inputs.transpose(0, 1)\n",
        "        return decoder_inputs\n",
        "\n",
        "    def parse_decoder_outputs(self, mel_outputs, gate_outputs, alignments):\n",
        "        \"\"\" Prepares decoder outputs for output\n",
        "        PARAMS\n",
        "        ------\n",
        "        mel_outputs:\n",
        "        gate_outputs: gate output energies\n",
        "        alignments:\n",
        "\n",
        "        RETURNS\n",
        "        -------\n",
        "        mel_outputs:\n",
        "        gate_outpust: gate output energies\n",
        "        alignments:\n",
        "        \"\"\"\n",
        "        # (T_out, B) -> (B, T_out)\n",
        "        alignments = torch.stack(alignments).transpose(0, 1)\n",
        "        # (T_out, B) -> (B, T_out)\n",
        "        gate_outputs = torch.stack(gate_outputs).transpose(0, 1)\n",
        "        gate_outputs = gate_outputs.contiguous()\n",
        "        # (T_out, B, n_mel_channels) -> (B, T_out, n_mel_channels)\n",
        "        mel_outputs = torch.stack(mel_outputs).transpose(0, 1).contiguous()\n",
        "        # decouple frames per step\n",
        "        mel_outputs = mel_outputs.view(\n",
        "            mel_outputs.size(0), -1, hparams.vocab_size)\n",
        "        # (B, T_out, n_mel_channels) -> (B, n_mel_channels, T_out)\n",
        "        mel_outputs = mel_outputs.transpose(1, 2)\n",
        "\n",
        "        return mel_outputs, gate_outputs, alignments\n",
        "\n",
        "    def decode(self, decoder_input):\n",
        "        \"\"\" Decoder step using stored states, attention and memory\n",
        "        PARAMS\n",
        "        ------\n",
        "        decoder_input: previous mel output\n",
        "\n",
        "        RETURNS\n",
        "        -------\n",
        "        mel_output:\n",
        "        gate_output: gate output energies\n",
        "        attention_weights:\n",
        "        \"\"\"\n",
        "        cell_input = torch.cat((decoder_input, self.attention_context), -1)\n",
        "        self.attention_hidden, self.attention_cell = self.attention_rnn(\n",
        "            cell_input, (self.attention_hidden, self.attention_cell))\n",
        "        self.attention_hidden = F.dropout(\n",
        "            self.attention_hidden, self.p_attention_dropout, self.training)\n",
        "\n",
        "        attention_weights_cat = torch.cat(\n",
        "            (self.attention_weights.unsqueeze(1),\n",
        "              self.attention_weights_cum.unsqueeze(1)), dim=1)\n",
        "        self.attention_context, self.attention_weights = self.attention_layer(\n",
        "            self.attention_hidden, self.memory, self.processed_memory,\n",
        "            attention_weights_cat, self.mask)\n",
        "\n",
        "        # print(self.attention_context.shape,self.attention_weights.shape)\n",
        "        self.attention_weights_cum += self.attention_weights\n",
        "        decoder_input = torch.cat(\n",
        "            (self.attention_hidden, self.attention_context), -1)\n",
        "        self.decoder_hidden, self.decoder_cell = self.decoder_rnn(\n",
        "            decoder_input, (self.decoder_hidden, self.decoder_cell))\n",
        "        self.decoder_hidden = F.dropout(\n",
        "            self.decoder_hidden, self.p_decoder_dropout, self.training)\n",
        "\n",
        "        decoder_hidden_attention_context = torch.cat(\n",
        "            (self.decoder_hidden, self.attention_context), dim=1)\n",
        "        decoder_output = self.linear_projection(\n",
        "            decoder_hidden_attention_context)\n",
        "\n",
        "        gate_prediction = self.gate_layer(decoder_hidden_attention_context)\n",
        "        return decoder_output, gate_prediction, self.attention_weights\n",
        "\n",
        "    def forward(self, memory,decoder_inputs,memory_lengths,output_lengths):\n",
        "        \"\"\" Decoder forward pass for training\n",
        "        PARAMS\n",
        "        ------\n",
        "        memory: Encoder outputs\n",
        "        decoder_inputs: Decoder inputs for teacher forcing. i.e. mel-specs\n",
        "        memory_lengths: Encoder output lengths for attention masking.\n",
        "\n",
        "        RETURNS\n",
        "        -------\n",
        "        mel_outputs: mel outputs from the decoder\n",
        "        gate_outputs: gate outputs from the decoder\n",
        "        alignments: sequence of attention weights from the decoder\n",
        "        \"\"\"\n",
        "        # print(memory.shape)\n",
        "        decoder_input = self.get_go_frame(memory).unsqueeze(0)\n",
        "        decoder_inputs = self.parse_decoder_inputs(decoder_inputs)\n",
        "        decoder_inputs = torch.cat((decoder_input, decoder_inputs), dim=0)\n",
        "        decoder_inputs = self.prenet(decoder_inputs)\n",
        "        self.initialize_decoder_states(\n",
        "            memory, mask=~get_mask_from_lengths(memory_lengths))\n",
        "        # print(decoder_inputs.shape)\n",
        "        mel_outputs, gate_outputs, alignments = [], [], []\n",
        "        while len(mel_outputs) < decoder_inputs.size(0) - 1:\n",
        "            decoder_input = decoder_inputs[len(mel_outputs)]\n",
        "            mel_output, gate_output, attention_weights = self.decode(\n",
        "                decoder_input)\n",
        "            mel_outputs += [mel_output.squeeze(1)]\n",
        "            gate_outputs += [gate_output.squeeze(1)]\n",
        "            alignments += [attention_weights]\n",
        "\n",
        "        mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
        "            mel_outputs, gate_outputs, alignments)\n",
        "\n",
        "        # print(mel_outputs.shape)\n",
        "        return mel_outputs, gate_outputs, alignments\n",
        "\n",
        "    def inference(self, memory):\n",
        "        \"\"\" Decoder inference\n",
        "        PARAMS\n",
        "        ------\n",
        "        memory: Encoder outputs\n",
        "\n",
        "        RETURNS\n",
        "        -------\n",
        "        mel_outputs: mel outputs from the decoder\n",
        "        gate_outputs: gate outputs from the decoder\n",
        "        alignments: sequence of attention weights from the decoder\n",
        "        \"\"\"\n",
        "        decoder_input = self.get_go_frame(memory)\n",
        "\n",
        "        self.initialize_decoder_states(memory, mask=None)\n",
        "\n",
        "        mel_outputs, gate_outputs, alignments = [], [], []\n",
        "        while True:\n",
        "            decoder_input = self.prenet(decoder_input)\n",
        "            mel_output, gate_output, alignment = self.decode(decoder_input)\n",
        "\n",
        "            mel_outputs += [mel_output.squeeze(1)]\n",
        "            gate_outputs += [gate_output]\n",
        "            alignments += [alignment]\n",
        "\n",
        "            if torch.sigmoid(gate_output.data) > self.gate_threshold:\n",
        "                break\n",
        "            elif len(mel_outputs) == self.max_decoder_steps:\n",
        "                print(\"Warning! Reached max decoder steps\")\n",
        "                break\n",
        "\n",
        "            decoder_input = mel_output\n",
        "\n",
        "        mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
        "            mel_outputs, gate_outputs, alignments)\n",
        "\n",
        "        return mel_outputs, gate_outputs, alignments\n",
        "\n",
        "\n",
        "class AuxiliaryPhonemesModel(nn.Module, PyTorchModelHubMixin):\n",
        "    def __init__(self, hparams):\n",
        "        super(AuxiliaryPhonemesModel, self).__init__()\n",
        "        self.mask_padding = hparams.mask_padding\n",
        "        self.fp16_run = hparams.fp16_run\n",
        "        self.n_mel_channels = hparams.n_mel_channels\n",
        "        self.n_frames_per_step = hparams.n_frames_per_step\n",
        "        # self.embedding = nn.Embedding(\n",
        "        #     hparams.n_symbols, hparams.symbols_embedding_dim)\n",
        "        self.linear = nn.Linear(hparams.n_mel_channels,hparams.encoder_embedding_dim)\n",
        "        # std = sqrt(2.0 / (hparams.n_symbols + hparams.symbols_embedding_dim))\n",
        "        # val = sqrt(3.0) * std  # uniform bounds for std\n",
        "        # self.embedding.weight.data.uniform_(-val, val)\n",
        "        self.encoder = Encoder(hparams)\n",
        "        self.decoder = PhonemesDecoder(hparams)\n",
        "        self.postnet = Postnet(hparams)\n",
        "\n",
        "    def parse_batch(self, batch):\n",
        "        # # text_padded, input_lengths, mel_padded, gate_padded, \\\n",
        "        # #     output_lengths = batch\n",
        "        # mels, gate,input_lengths, spec, output_lengths = batch\n",
        "        # mels = to_gpu(mels).float()\n",
        "        # gate = to_gpu(gate).float()\n",
        "        # input_lengths = to_gpu(input_lengths).long()\n",
        "        # inp_len = torch.max(input_lengths.data).item()\n",
        "        # spec = to_gpu(spec).float()\n",
        "        # out_lengths = to_gpu(out_lenghts).long()\n",
        "        # out_len = torch.max(out_lengths).item()\n",
        "        # # gate_padded = to_gpu(gate_padded).float()\n",
        "        # # output_lengths = to_gpu(output_lengths).long()\n",
        "\n",
        "        # return (\n",
        "        #     (mels,gate,input_lengths,max_len,spec,output_lengths,out_len),\n",
        "        #     (spec,output_lengths,out_len))\n",
        "        input_padded, input_lengths, mel_padded, gate_padded, \\\n",
        "            output_lengths = batch\n",
        "        input_padded = to_gpu(input_padded).float()\n",
        "        input_lengths = to_gpu(input_lengths).long()\n",
        "        max_len = torch.max(input_lengths.data).item()\n",
        "        mel_padded = to_gpu(mel_padded).float()\n",
        "        gate_padded = to_gpu(gate_padded).float()\n",
        "        output_lengths = to_gpu(output_lengths).long()\n",
        "\n",
        "        return (\n",
        "            (input_padded, input_lengths, mel_padded, max_len, output_lengths),\n",
        "            (mel_padded, gate_padded))\n",
        "\n",
        "    def parse_output(self, outputs, output_lengths=None):\n",
        "        if self.mask_padding and output_lengths is not None:\n",
        "            mask = ~get_mask_from_lengths(output_lengths)\n",
        "            mask = mask.expand(hparams.vocab_size, mask.size(0), mask.size(1))\n",
        "            mask = mask.permute(1, 0, 2)\n",
        "\n",
        "            outputs[0].data.masked_fill_(mask, 0.0)\n",
        "            outputs[1].data.masked_fill_(mask, 0.0)\n",
        "            outputs[2].data.masked_fill_(mask[:, 0, :], 1e3)  # gate energies\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # mels,gate,input_lengths,max_len,spec,output_lengths,out_len = inputs\n",
        "        # mels, input_lengths = mels.data, input_lengths.data\n",
        "\n",
        "        # # embedded_inputs = self.embedding(text_inputs).transpose(1, 2)\n",
        "\n",
        "        # encoder_outputs = self.encoder(mels, input_lengths)\n",
        "\n",
        "        # mel_outputs, gate_outputs, alignments = self.decoder(\n",
        "        #     encoder_outputs, spec,input_lengths,output_lengths)\n",
        "\n",
        "        # mel_outputs_postnet = self.postnet(mel_outputs)\n",
        "        # mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
        "\n",
        "        # return self.parse_output(\n",
        "        #     [mel_outputs, mel_outputs_postnet, gate_outputs, alignments],\n",
        "        #     output_lengths)\n",
        "        inputs, input_lengths, mels, max_len, output_lengths = inputs\n",
        "        input_lengths, output_lengths = input_lengths.data, output_lengths.data\n",
        "\n",
        "        embedded_inputs = self.linear(inputs.transpose(1,2)).transpose(1, 2)\n",
        "\n",
        "        encoder_outputs = self.encoder(embedded_inputs, input_lengths)\n",
        "        # print(encoder_outputs.shape)\n",
        "        mel_outputs, gate_outputs, alignments = self.decoder(\n",
        "            encoder_outputs, mels, input_lengths,output_lengths)\n",
        "\n",
        "        mel_outputs_postnet = self.postnet(mel_outputs)\n",
        "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
        "\n",
        "        return mel_outputs, mel_outputs_postnet, gate_outputs, alignments\n",
        "\n",
        "    def inference(self, inputs):\n",
        "        # embedded_inputs = self.embedding(inputs).transpose(1, 2)\n",
        "        encoder_outputs = self.encoder.inference(inputs)\n",
        "        mel_outputs, gate_outputs, alignments = self.decoder.inference(\n",
        "            encoder_outputs)\n",
        "\n",
        "        mel_outputs_postnet = self.postnet(mel_outputs)\n",
        "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
        "\n",
        "        outputs = self.parse_output(\n",
        "            [mel_outputs, mel_outputs_postnet, gate_outputs, alignments])\n",
        "\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "OrJw5XUp5QqI"
      },
      "outputs": [],
      "source": [
        "## loss_function.py\n",
        "class Tacotron2Loss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Tacotron2Loss, self).__init__()\n",
        "\n",
        "    def forward(self, model_output, targets):\n",
        "        phoneme_target, gate_target = targets[0], targets[1]\n",
        "        mel_out, mel_out_postnet, gate_out, _ = model_output\n",
        "\n",
        "        gate_loss = nn.BCEWithLogitsLoss()(gate_out, gate_target)\n",
        "\n",
        "        mel_out_indices = torch.argmax(mel_out, dim=2)\n",
        "\n",
        "        mel_out_postnet_indices = torch.argmax(mel_out_postnet, dim=2)\n",
        "\n",
        "        phoneme_target_indices = torch.argmax(mel_out, dim=2)\n",
        "\n",
        "        mel_loss = F.cross_entropy(mel_out, phoneme_target) + F.cross_entropy(mel_out_postnet, phoneme_target)\n",
        "        return mel_loss + gate_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "e_Wib6djkGYZ"
      },
      "outputs": [],
      "source": [
        "## loss_scaler.py\n",
        "\n",
        "import torch\n",
        "\n",
        "class LossScaler:\n",
        "\n",
        "    def __init__(self, scale=1):\n",
        "        self.cur_scale = scale\n",
        "\n",
        "    # `params` is a list / generator of torch.Variable\n",
        "    def has_overflow(self, params):\n",
        "        return False\n",
        "\n",
        "    # `x` is a torch.Tensor\n",
        "    def _has_inf_or_nan(x):\n",
        "        return False\n",
        "\n",
        "    # `overflow` is boolean indicating whether we overflowed in gradient\n",
        "    def update_scale(self, overflow):\n",
        "        pass\n",
        "\n",
        "    @property\n",
        "    def loss_scale(self):\n",
        "        return self.cur_scale\n",
        "\n",
        "    def scale_gradient(self, module, grad_in, grad_out):\n",
        "        return tuple(self.loss_scale * g for g in grad_in)\n",
        "\n",
        "    def backward(self, loss):\n",
        "        scaled_loss = loss*self.loss_scale\n",
        "        scaled_loss.backward()\n",
        "\n",
        "class DynamicLossScaler:\n",
        "\n",
        "    def __init__(self,\n",
        "                 init_scale=2**32,\n",
        "                 scale_factor=2.,\n",
        "                 scale_window=1000):\n",
        "        self.cur_scale = init_scale\n",
        "        self.cur_iter = 0\n",
        "        self.last_overflow_iter = -1\n",
        "        self.scale_factor = scale_factor\n",
        "        self.scale_window = scale_window\n",
        "\n",
        "    # `params` is a list / generator of torch.Variable\n",
        "    def has_overflow(self, params):\n",
        "#        return False\n",
        "        for p in params:\n",
        "            if p.grad is not None and DynamicLossScaler._has_inf_or_nan(p.grad.data):\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    # `x` is a torch.Tensor\n",
        "    def _has_inf_or_nan(x):\n",
        "        cpu_sum = float(x.float().sum())\n",
        "        if cpu_sum == float('inf') or cpu_sum == -float('inf') or cpu_sum != cpu_sum:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    # `overflow` is boolean indicating whether we overflowed in gradient\n",
        "    def update_scale(self, overflow):\n",
        "        if overflow:\n",
        "            #self.cur_scale /= self.scale_factor\n",
        "            self.cur_scale = max(self.cur_scale/self.scale_factor, 1)\n",
        "            self.last_overflow_iter = self.cur_iter\n",
        "        else:\n",
        "            if (self.cur_iter - self.last_overflow_iter) % self.scale_window == 0:\n",
        "                self.cur_scale *= self.scale_factor\n",
        "#        self.cur_scale = 1\n",
        "        self.cur_iter += 1\n",
        "\n",
        "    @property\n",
        "    def loss_scale(self):\n",
        "        return self.cur_scale\n",
        "\n",
        "    def scale_gradient(self, module, grad_in, grad_out):\n",
        "        return tuple(self.loss_scale * g for g in grad_in)\n",
        "\n",
        "    def backward(self, loss):\n",
        "        scaled_loss = loss*self.loss_scale\n",
        "        scaled_loss.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_eMEbm9Bgau"
      },
      "source": [
        "##### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "OyRa7iFGkJ_a"
      },
      "outputs": [],
      "source": [
        "model = AuxiliaryPhonemesModel(hparams).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "LrNM-0beisTz"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_loader = DataLoader(train_dataset, num_workers=2, shuffle=True,\n",
        "                          batch_size=hparams.batch_size, pin_memory=False,\n",
        "                          drop_last=True, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "sEU13qCSZw8w"
      },
      "outputs": [],
      "source": [
        "spec_to_emb_linear = model.linear\n",
        "encoder = model.encoder\n",
        "decoder = model.decoder\n",
        "postnet = model.postnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "6hwHpATuhlN4"
      },
      "outputs": [],
      "source": [
        "def get_go_frame(memory):\n",
        "    \"\"\" Gets all zeros frames to use as first decoder input\n",
        "    PARAMS\n",
        "    ------\n",
        "    memory: decoder outputs\n",
        "\n",
        "    RETURNS\n",
        "    -------\n",
        "    decoder_input: all zeros frames\n",
        "    \"\"\"\n",
        "    B = memory.size(0)\n",
        "    decoder_input = Variable(memory.data.new(\n",
        "        B, hparams.vocab_size).zero_())\n",
        "    return decoder_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ggdULYOGpHIr"
      },
      "outputs": [],
      "source": [
        "def parse_decoder_inputs(self, decoder_inputs):\n",
        "    \"\"\" Prepares decoder inputs, i.e. mel outputs\n",
        "    PARAMS\n",
        "    ------\n",
        "    decoder_inputs: inputs used for teacher-forced training, i.e. mel-specs\n",
        "\n",
        "    RETURNS\n",
        "    -------\n",
        "    inputs: processed decoder inputs\n",
        "\n",
        "    \"\"\"\n",
        "    # (B, n_mel_channels, T_out) -> (B, T_out, n_mel_channels)\n",
        "    decoder_inputs = decoder_inputs.transpose(1, 2)\n",
        "    decoder_inputs = decoder_inputs.view(\n",
        "        decoder_inputs.size(0),\n",
        "        int(decoder_inputs.size(1)/self.n_frames_per_step), -1)\n",
        "    # (B, T_out, n_mel_channels) -> (T_out, B, n_mel_channels)\n",
        "    decoder_inputs = decoder_inputs.transpose(0, 1)\n",
        "    return decoder_inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "iOKaSpkvpcob"
      },
      "outputs": [],
      "source": [
        "prenet = Prenet(\n",
        "    hparams.vocab_size,\n",
        "    [hparams.prenet_dim, hparams.prenet_dim])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "KitQPlEf1IVg"
      },
      "outputs": [],
      "source": [
        "def parse_decoder_inputs(decoder_inputs):\n",
        "    \"\"\" Prepares decoder inputs, i.e. mel outputs\n",
        "    PARAMS\n",
        "    ------\n",
        "    decoder_inputs: inputs used for teacher-forced training, i.e. mel-specs\n",
        "\n",
        "    RETURNS\n",
        "    -------\n",
        "    inputs: processed decoder inputs\n",
        "\n",
        "    \"\"\"\n",
        "    # (B, n_mel_channels, T_out) -> (B, T_out, n_mel_channels)\n",
        "    decoder_inputs = decoder_inputs.transpose(1, 2)\n",
        "    decoder_inputs = decoder_inputs.view(\n",
        "        decoder_inputs.size(0),\n",
        "        int(decoder_inputs.size(1)/hparams.n_frames_per_step), -1)\n",
        "    # (B, T_out, n_mel_channels) -> (T_out, B, n_mel_channels)\n",
        "    decoder_inputs = decoder_inputs.transpose(0, 1)\n",
        "    return decoder_inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "k9DgSAa_ALQf"
      },
      "outputs": [],
      "source": [
        "criterion = Tacotron2Loss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrurCqazZClC",
        "outputId": "c482eedd-cafc-42de-dc34-260d714b09cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs Shape:  torch.Size([32, 80, 999])\n",
            "Encoder Outputs Shape:  torch.Size([32, 999, 128])\n",
            "Decoder Input Array:  tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') torch.Size([1, 32, 74])\n",
            "Output Shape before parsing:  torch.Size([32, 74, 180])\n",
            "Output Shape after parsing:  torch.Size([181, 32, 74])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-d073357cb1e3>:111: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  ids = torch.arange(0, max_len, out=torch.cuda.LongTensor(max_len))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.731008052825928\n"
          ]
        }
      ],
      "source": [
        "for i, batch in enumerate(train_loader):\n",
        "  inputs, input_lengths, target, gate_padded, output_lengths = batch\n",
        "\n",
        "  print(\"Inputs Shape: \", inputs.shape)\n",
        "  inputs = inputs.cuda()\n",
        "  target = target.cuda()\n",
        "\n",
        "  input_lengths, output_lengths = input_lengths.data, output_lengths.data\n",
        "\n",
        "  embedded_inputs = spec_to_emb_linear(inputs.transpose(1,2)).transpose(1, 2)\n",
        "\n",
        "  encoder_outputs = encoder(embedded_inputs.cuda(), input_lengths)\n",
        "\n",
        "  print(\"Encoder Outputs Shape: \", encoder_outputs.shape)\n",
        "  ## get_go_frame() -- get the zeros values of encoder in\n",
        "  decoder_input = get_go_frame(encoder_outputs).unsqueeze(0)\n",
        "\n",
        "  print(\"Decoder Input Array: \", decoder_input, decoder_input.shape)\n",
        "\n",
        "  print(\"Output Shape before parsing: \", target.shape)\n",
        "\n",
        "  decoder_inputs = parse_decoder_inputs(target)\n",
        "  decoder_inputs = torch.cat((decoder_input.cuda(), decoder_inputs.cuda()), dim=0)\n",
        "\n",
        "  print(\"Output Shape after parsing: \", decoder_inputs.shape)\n",
        "\n",
        "  mel_outputs, gate_outputs, alignments = decoder(\n",
        "      encoder_outputs, target, input_lengths.cuda(),output_lengths.cuda())\n",
        "\n",
        "  mel_outputs_postnet = postnet(mel_outputs)\n",
        "\n",
        "\n",
        "  loss = criterion((mel_outputs, mel_outputs_postnet, gate_outputs, alignments), (target, gate_padded.cuda()))\n",
        "\n",
        "  print(loss.item())\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73rnqapD-h5Z",
        "outputId": "12ab909d-2005-43d7-a4e3-b08921b2516c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 73])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "target_indices = torch.randint(0, 102, (1, 73))\n",
        "target_indices.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTTclH3H911j",
        "outputId": "43795a64-09bd-449f-ac16-1e39c6a7f691"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 74, 180]), torch.Size([32, 74, 180]))"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "mel_outputs.shape, target.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJDmAI3C-LW1",
        "outputId": "9437e388-bb2d-4079-eed5-d10d28fadeef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.679483413696289"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "loss = nn.CrossEntropyLoss()(mel_outputs, target)\n",
        "loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUjbGl9P4ll0",
        "outputId": "1a8c4e36-71de-40ae-9c2b-099a9c63a5eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 74, 180])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
        "mel_outputs_postnet.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mp23JBK84OAk",
        "outputId": "121f3577-e9ed-4ddc-a6c1-234786580cb5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 74, 180]),\n",
              " tensor([[-0.1785, -0.2063, -0.1970,  ..., -0.2740, -0.2541, -0.2296],\n",
              "         [-0.2193, -0.2056, -0.2584,  ..., -0.2476, -0.2999, -0.2636],\n",
              "         [-0.0764, -0.0835, -0.1099,  ..., -0.1066, -0.1000, -0.1153],\n",
              "         ...,\n",
              "         [-0.0604, -0.0607, -0.0538,  ..., -0.0446, -0.0672, -0.0741],\n",
              "         [-0.0148, -0.0260, -0.0252,  ..., -0.0358, -0.0763, -0.0445],\n",
              "         [-0.0367, -0.0212, -0.0238,  ..., -0.0176, -0.0296, -0.0320]],\n",
              "        device='cuda:0', grad_fn=<CloneBackward0>),\n",
              " torch.Size([32, 180, 999]),\n",
              " torch.Size([32, 74, 180]))"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "mel_outputs.shape, gate_outputs, alignments.shape, mel_outputs_postnet.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w99-N1TdBmHh"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "VrJLkhsqCEDB"
      },
      "outputs": [],
      "source": [
        "model = AuxiliaryPhonemesModel(hparams).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "OKkG6psOB2fP"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, num_workers=2, shuffle=True,\n",
        "                          batch_size=hparams.batch_size, pin_memory=False,\n",
        "                          drop_last=True, collate_fn=collate_fn)\n",
        "\n",
        "valid_loader = DataLoader(val_dataset, num_workers=2, shuffle=True,\n",
        "                          batch_size=hparams.batch_size, pin_memory=False,\n",
        "                          drop_last=True, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "qvXoVBFKCHGz"
      },
      "outputs": [],
      "source": [
        "criterion = Tacotron2Loss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "JiORJ1D1CvJ5"
      },
      "outputs": [],
      "source": [
        "learning_rate = hparams.learning_rate\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
        "                              weight_decay=hparams.weight_decay)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Blz2WNZUC1KX",
        "outputId": "0baf1fb5-5832-4e98-fc09-84aee36c8089"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AuxiliaryPhonemesModel(\n",
              "  (linear): Linear(in_features=80, out_features=128, bias=True)\n",
              "  (encoder): Encoder(\n",
              "    (lstm): LSTM(128, 64, batch_first=True, bidirectional=True)\n",
              "  )\n",
              "  (decoder): PhonemesDecoder(\n",
              "    (prenet): Prenet(\n",
              "      (layers): ModuleList(\n",
              "        (0): LinearNorm(\n",
              "          (linear_layer): Linear(in_features=74, out_features=32, bias=False)\n",
              "        )\n",
              "        (1): LinearNorm(\n",
              "          (linear_layer): Linear(in_features=32, out_features=32, bias=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (attention_rnn): LSTMCell(160, 256)\n",
              "    (attention_layer): Attention(\n",
              "      (query_layer): LinearNorm(\n",
              "        (linear_layer): Linear(in_features=256, out_features=128, bias=False)\n",
              "      )\n",
              "      (memory_layer): LinearNorm(\n",
              "        (linear_layer): Linear(in_features=128, out_features=128, bias=False)\n",
              "      )\n",
              "      (attention): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (location_layer): LocationLayer(\n",
              "        (location_conv): ConvNorm(\n",
              "          (conv): Conv1d(2, 32, kernel_size=(31,), stride=(1,), padding=(15,), bias=False)\n",
              "        )\n",
              "        (location_dense): LinearNorm(\n",
              "          (linear_layer): Linear(in_features=32, out_features=128, bias=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (decoder_rnn): LSTMCell(384, 256, bias=1)\n",
              "    (linear_projection): LinearNorm(\n",
              "      (linear_layer): Linear(in_features=384, out_features=74, bias=True)\n",
              "    )\n",
              "    (gate_layer): LinearNorm(\n",
              "      (linear_layer): Linear(in_features=384, out_features=1, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (postnet): Postnet(\n",
              "    (convolutions): ModuleList(\n",
              "      (0): Sequential(\n",
              "        (0): ConvNorm(\n",
              "          (conv): Conv1d(74, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "        )\n",
              "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): Sequential(\n",
              "        (0): ConvNorm(\n",
              "          (conv): Conv1d(128, 74, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "        )\n",
              "        (1): BatchNorm1d(74, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "model.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSzwUbYE4coJ",
        "outputId": "68b5d1b3-6dcc-4de2-9511-a9471c3b8a2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) \n",
            "Token is valid (permission: write).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "#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\n",
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "JkwBTTnl4ebY"
      },
      "outputs": [],
      "source": [
        "def train(model, train_dataloader, valid_dataloader, save_path='/content/drive/MyDrive/Dubbing Project/models'):\n",
        "\n",
        "  criterion = Tacotron2Loss()\n",
        "\n",
        "  learning_rate = hparams.learning_rate\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
        "                                weight_decay=hparams.weight_decay)\n",
        "\n",
        "  best_valid_loss = float('inf')\n",
        "\n",
        "  wandb.init(\n",
        "      project=\"S2ST\",\n",
        "      config={\n",
        "        \"learning_rate\": hparams.learning_rate,\n",
        "        \"architecture\": \"Translatotron\",\n",
        "        \"dataset\": \"Librspeech 100 Hours of data\",\n",
        "        \"epochs\": hparams.epochs,\n",
        "        'batch_size': hparams.batch_size,\n",
        "        \"description\": f\"translatoron first run\",\n",
        "      }\n",
        "  )\n",
        "\n",
        "  for epoch in range(hparams.epochs):\n",
        "      # Training\n",
        "      model.train()\n",
        "      total_loss = 0.0\n",
        "\n",
        "      with tqdm(train_dataloader, desc=f'Training Epoch {epoch + 1}/{hparams.epochs}', unit='batch') as t:\n",
        "        for i, batch in enumerate(t):\n",
        "            inputs, input_lengths, target, gate_padded, output_lengths = batch\n",
        "\n",
        "            batch = inputs.cuda(), input_lengths.cuda(), target.cuda(), gate_padded.cuda(), output_lengths.cuda()\n",
        "\n",
        "            mel_outputs, mel_outputs_postnet, gate_outputs, alignments = model(batch)\n",
        "\n",
        "            loss = criterion((mel_outputs, mel_outputs_postnet, gate_outputs, alignments), (target.cuda(), gate_padded.cuda()))\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            t.set_postfix(loss=loss.item(), total_loss=total_loss / (i + 1))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            wandb.log({\"train_loss\": total_loss / (i + 1)})\n",
        "\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "      # Validation\n",
        "      model.eval()\n",
        "      total_valid_loss = 0.0\n",
        "\n",
        "      gc.collect()\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "      with torch.no_grad():\n",
        "          with tqdm(valid_dataloader, desc=f'Validation Epoch {epoch + 1}/{hparams.epochs}', unit='batch') as v:\n",
        "            for i, batch in enumerate(v):\n",
        "                inputs, input_lengths, target, gate_padded, output_lengths = batch\n",
        "\n",
        "                batch = inputs.cuda(), input_lengths.cuda(), target.cuda(), gate_padded.cuda(), output_lengths.cuda()\n",
        "\n",
        "                mel_outputs, mel_outputs_postnet, gate_outputs, alignments = model(batch)\n",
        "\n",
        "                loss = criterion((mel_outputs, mel_outputs_postnet, gate_outputs, alignments), (target.cuda(), gate_padded.cuda()))\n",
        "                v.set_postfix(loss=loss.item(), total_loss=total_valid_loss / (i + 1))\n",
        "\n",
        "                total_valid_loss += loss.item()\n",
        "                wandb.log({\"valid_loss\": total_valid_loss / (i + 1)})\n",
        "\n",
        "                gc.collect()\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "      # Save the model\n",
        "      save_filename = os.path.join(save_path, f'phoneme_predictor_{epoch + 1}.pt')\n",
        "      torch.save(model.state_dict(), save_filename)\n",
        "\n",
        "      gc.collect()\n",
        "      torch.cuda.empty_cache()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "8jcr-slf_Wav"
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 275,
          "referenced_widgets": [
            "434de0c9e8db4f9ca6e9e2227863f219",
            "ae955f23c7fe4417852ba45970931708",
            "82d952bb69ca42a9986f087e95aa1a17",
            "b5dbfff1dede42a4a7bdeeb119b53dc4",
            "c383865e32ea4532996fb1c106f2a313",
            "84628f8683744a8c8869cf47b56324d8",
            "3a0159f9dc5243d6a6053eb087edfb07",
            "5c41c71fd786421ba9c7c34966099e0c"
          ]
        },
        "id": "wNUGc8TE6vr9",
        "outputId": "e18bbc2c-57aa-4603-b286-0f47ea3c713b"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "434de0c9e8db4f9ca6e9e2227863f219",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112426744445101, max=1.0"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240223_121848-lwuqtm8h</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/wasimmadha/S2ST/runs/lwuqtm8h' target=\"_blank\">golden-fuse-7</a></strong> to <a href='https://wandb.ai/wasimmadha/S2ST' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/wasimmadha/S2ST' target=\"_blank\">https://wandb.ai/wasimmadha/S2ST</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/wasimmadha/S2ST/runs/lwuqtm8h' target=\"_blank\">https://wandb.ai/wasimmadha/S2ST/runs/lwuqtm8h</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 1/7: 100%|| 891/891 [1:07:59<00:00,  4.58s/batch, loss=3.61, total_loss=4.49]\n",
            "Validation Epoch 1/7: 100%|| 84/84 [05:58<00:00,  4.27s/batch, loss=1.7, total_loss=1.8]\n",
            "Training Epoch 2/7: 100%|| 891/891 [1:07:52<00:00,  4.57s/batch, loss=3.41, total_loss=3.45]\n",
            "Validation Epoch 2/7: 100%|| 84/84 [05:58<00:00,  4.27s/batch, loss=1.53, total_loss=1.44]\n",
            "Training Epoch 3/7: 100%|| 891/891 [1:08:16<00:00,  4.60s/batch, loss=3.18, total_loss=3.04]\n",
            "Validation Epoch 3/7: 100%|| 84/84 [06:05<00:00,  4.35s/batch, loss=1.25, total_loss=1.22]\n",
            "Training Epoch 4/7: 100%|| 891/891 [1:08:20<00:00,  4.60s/batch, loss=3.03, total_loss=2.83]\n",
            "Validation Epoch 4/7: 100%|| 84/84 [06:05<00:00,  4.35s/batch, loss=1.43, total_loss=1.15]\n",
            "Training Epoch 5/7: 100%|| 891/891 [1:08:22<00:00,  4.60s/batch, loss=2.93, total_loss=2.7]\n",
            "Validation Epoch 5/7: 100%|| 84/84 [06:00<00:00,  4.29s/batch, loss=1.32, total_loss=1.08]\n",
            "Training Epoch 6/7: 100%|| 891/891 [1:08:13<00:00,  4.59s/batch, loss=2.49, total_loss=2.62]\n",
            "Validation Epoch 6/7: 100%|| 84/84 [05:57<00:00,  4.26s/batch, loss=1.39, total_loss=1.02]\n",
            "Training Epoch 7/7: 100%|| 891/891 [1:08:00<00:00,  4.58s/batch, loss=2.29, total_loss=2.54]\n",
            "Validation Epoch 7/7: 100%|| 84/84 [05:59<00:00,  4.28s/batch, loss=0.709, total_loss=1.01]\n"
          ]
        }
      ],
      "source": [
        "save_path = '/content/drive/MyDrive/Dubbing Project/models'\n",
        "\n",
        "## f42e9dfb9ecc6347595dd9aa95ce1ce04e08004d\n",
        "train(model, train_loader, valid_loader, save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "E6pi8d7533OG"
      },
      "outputs": [],
      "source": [
        "save_filename = os.path.join(save_path, f'final_phoneme_predictor.pt')\n",
        "torch.save(model.state_dict(), save_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzNfYryKPjj2"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6KajgpqrED_"
      },
      "outputs": [],
      "source": [
        "mel_outputs_postnet.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmWM5USJqWCC"
      },
      "outputs": [],
      "source": [
        "mel_out_postnet_indices = torch.argmax(mel_outputs_postnet, dim=1)\n",
        "\n",
        "phoneme_target_indices = torch.argmax(target, dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gc_zJ34qjS3"
      },
      "outputs": [],
      "source": [
        "mel_out_postnet_indices, phoneme_target_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQb0scZ-1go_"
      },
      "outputs": [],
      "source": [
        "# for i, batch in enumerate(train_loader):\n",
        "#   inputs, input_lengths, target, gate_padded, output_lengths = batch\n",
        "\n",
        "#   batch = inputs.cuda(), input_lengths.cuda(), target.cuda(), gate_padded.cuda(), output_lengths.cuda()\n",
        "\n",
        "#   mel_outputs, mel_outputs_postnet, gate_outputs, alignments = model(batch)\n",
        "\n",
        "#   loss = criterion((mel_outputs, mel_outputs_postnet, gate_outputs, alignments), (target.cuda(), gate_padded.cuda()))\n",
        "\n",
        "#   if i % 10 == 0:\n",
        "#     print(\"Loss is: \", loss.item())\n",
        "\n",
        "#   loss.backward()\n",
        "\n",
        "#   optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "NNjvkQ7227j3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_path='/content/drive/MyDrive/Dubbing Project/models'"
      ],
      "metadata": {
        "id": "rOr1iriJ29XV"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "fSDUjezjC0l4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6ce51eb-97aa-4ce3-9fa9-889748952554"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(os.path.join(save_path, f'final_phoneme_predictor.pt')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "k0GmvODy1g98"
      },
      "outputs": [],
      "source": [
        "for i, batch in enumerate(valid_dataloader):\n",
        "    inputs, input_lengths, target, gate_padded, output_lengths = batch\n",
        "\n",
        "    batch = inputs.cuda(), input_lengths.cuda(), target.cuda(), gate_padded.cuda(), output_lengths.cuda()\n",
        "\n",
        "    mel_outputs, mel_outputs_postnet, gate_outputs, alignments = model(batch)\n",
        "\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mel_out_postnet_indices = torch.argmax(mel_outputs_postnet, dim=1)\n",
        "\n",
        "phoneme_target_indices = torch.argmax(target, dim=1)\n"
      ],
      "metadata": {
        "id": "eEA6wsFB2XFJ"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "binary_tensor = torch.where(gate_outputs > 0, torch.tensor(1), torch.tensor(0))"
      ],
      "metadata": {
        "id": "m6d1HxA95CnW"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mel_out_postnet_indices[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZ68lw723EI7",
        "outputId": "9c6579d7-72d3-4b2c-b730-7b66e70452ff"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 3, 61,  9, 38, 46, 38, 11, 41, 47,  3, 38, 46,  3, 25, 54,  5, 11, 61,\n",
              "        29, 46, 38, 54, 15, 52, 59, 62, 65, 46,  3, 25, 62, 16,  9, 37, 38, 62,\n",
              "        65, 46,  3, 25, 66, 54, 41,  9, 25,  3, 37, 65, 61, 46, 61,  9, 61,  9,\n",
              "        61, 61, 46, 24, 61,  9, 41,  5, 15, 24, 24, 24, 15, 61, 24, 61, 61, 54,\n",
              "        61, 33, 66, 24, 54, 61, 33, 61, 11, 61, 62, 61,  3, 54],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "phoneme_target_indices[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoiESdRd3aD0",
        "outputId": "93e3144e-7341-4735-a146-d56912174e3d"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([59, 61,  9, 35,  5, 38, 33, 41, 44,  3, 38, 46,  3, 16, 54, 15, 11, 61,\n",
              "        29, 36, 30, 40, 15, 52, 38, 62, 65, 46,  3, 25, 62, 71, 11, 37, 38, 62,\n",
              "        65, 46,  3, 25, 23, 30, 33, 11, 35,  3, 37,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "binary_tensor[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66YdsTmh5dav",
        "outputId": "03236447-de29-4bcc-94b5-214898249520"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8kIlkmMf5nhO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1oSlNe1TfykdLvTCYZLujZDOOM83cBvsw",
      "authorship_tag": "ABX9TyMxjQ9H6wQ+Ywubyy3sxvl4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3a0159f9dc5243d6a6053eb087edfb07": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "434de0c9e8db4f9ca6e9e2227863f219": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ae955f23c7fe4417852ba45970931708",
              "IPY_MODEL_82d952bb69ca42a9986f087e95aa1a17"
            ],
            "layout": "IPY_MODEL_b5dbfff1dede42a4a7bdeeb119b53dc4"
          }
        },
        "5c41c71fd786421ba9c7c34966099e0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "82d952bb69ca42a9986f087e95aa1a17": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a0159f9dc5243d6a6053eb087edfb07",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5c41c71fd786421ba9c7c34966099e0c",
            "value": 1
          }
        },
        "84628f8683744a8c8869cf47b56324d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae955f23c7fe4417852ba45970931708": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c383865e32ea4532996fb1c106f2a313",
            "placeholder": "",
            "style": "IPY_MODEL_84628f8683744a8c8869cf47b56324d8",
            "value": "Waiting for wandb.init()...\r"
          }
        },
        "b5dbfff1dede42a4a7bdeeb119b53dc4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c383865e32ea4532996fb1c106f2a313": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}