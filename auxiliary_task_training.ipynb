{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wasimmadha/dubbing-project-research/blob/main/auxiliary_task_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tXe1PPPtUsFn"
      },
      "outputs": [],
      "source": [
        "!tar -xzf '/content/drive/MyDrive/Dubbing Project/libspeech/dev-clean.tar.gz' -C /content/\n",
        "!tar -xzf '/content/drive/MyDrive/Dubbing Project/libspeech/train-clean-100.tar.gz' -C /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbH8Ke1TWO0V",
        "outputId": "fa3b8374-2e90-42aa-b8b0-a1354210cfec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.16.3-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.40.5-py2.py3-none-any.whl (258 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.5/258.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: pydub, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.42 docker-pycreds-0.4.0 gitdb-4.0.11 pydub-0.25.1 sentry-sdk-1.40.5 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.3\n"
          ]
        }
      ],
      "source": [
        "!pip install pydub wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IQ21N94Z05f4"
      },
      "outputs": [],
      "source": [
        "## Imports\n",
        "\n",
        "import torch\n",
        "from pydub import AudioSegment\n",
        "import numpy as np\n",
        "from scipy.signal import get_window\n",
        "import librosa.util as librosa_util\n",
        "import random\n",
        "import torch.utils.data\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from math import sqrt\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import read\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "import random\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "import torch.distributed as dist\n",
        "from torch.nn.modules import Module\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from scipy.signal import get_window\n",
        "from librosa.util import pad_center, tiny\n",
        "\n",
        "from librosa.filters import mel as librosa_mel_fn\n",
        "\n",
        "from huggingface_hub import PyTorchModelHubMixin\n",
        "import gc\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PS7T9YT0UV4O"
      },
      "source": [
        "#### hparams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "B7pkLxXlUYnh"
      },
      "outputs": [],
      "source": [
        "### hparams.py\n",
        "\n",
        "class mapDict(dict):\n",
        "  __getattr__ = dict.get\n",
        "  __setattr__ = dict.__setitem__\n",
        "  __delattr__ = dict.__delitem__\n",
        "\n",
        "\n",
        "\n",
        "def create_hparams(hparams_string=None,verbose=False):\n",
        "  hparams = {\n",
        "    ################################\n",
        "    # Experiment Parameters        #\n",
        "    ################################\n",
        "    \"epochs\":7,\n",
        "    \"iters_per_checkpoint\":10,\n",
        "    \"seed\":1234,\n",
        "    \"dynamic_loss_scaling\":True,\n",
        "    \"fp16_run\":False,\n",
        "    \"distributed_run\":False,\n",
        "    \"dist_backend\":\"nccl\",\n",
        "    \"dist_url\":\"tcp://localhost:54321\",\n",
        "    \"cudnn_enabled\":True,\n",
        "    \"cudnn_benchmark\":False,\n",
        "    \"ignore_layers\":['embedding.weight'],\n",
        "\n",
        "    ################################\n",
        "    # Data Parameters             #\n",
        "    ################################\n",
        "    \"load_mel_from_disk\":False,\n",
        "    \"training_files\":'data/train',\n",
        "    \"validation_files\":'data/val',\n",
        "    \"text_cleaners\":['english_cleaners'],\n",
        "\n",
        "    ################################\n",
        "    # Audio Parameters             #\n",
        "    ################################\n",
        "    \"max_wav_value\":32768.0,\n",
        "    \"sampling_rate\":22050,\n",
        "    \"filter_length\":1024,\n",
        "    \"hop_length\":256,\n",
        "    \"win_length\":1024,\n",
        "    \"n_mel_channels\":80,\n",
        "    \"mel_fmin\":0.0,\n",
        "    \"mel_fmax\":8000.0,\n",
        "\n",
        "    #Data parameters\n",
        "    \"input_data_root\": r'C:\\Users\\Wasim\\DubbingProject\\Speech2Speech\\google_research\\translatotron\\data\\prepared_data\\source\\train',\n",
        "    \"output_data_root\": r'C:\\Users\\Wasim\\DubbingProject\\Speech2Speech\\google_research\\translatotron\\data\\prepared_data\\target\\train',\n",
        "    \"train_size\": 0.75,\n",
        "    #Output Audio Parameters\n",
        "    \"out_channels\":1025,\n",
        "    ################################\n",
        "    # Model Parameters             #\n",
        "    ################################\n",
        "    \"symbols_embedding_dim\":512,\n",
        "\n",
        "    # Encoder parameters\n",
        "    \"encoder_kernel_size\":5,\n",
        "    \"encoder_n_convolutions\":3,\n",
        "    \"encoder_embedding_dim\":128,\n",
        "\n",
        "    # Decoder parameters\n",
        "    \"n_frames_per_step\":1,  # currently only 1 is supported\n",
        "    \"decoder_rnn_dim\":256,\n",
        "    \"prenet_dim\":32,\n",
        "    \"max_decoder_steps\":1000,\n",
        "    \"gate_threshold\":0.5,\n",
        "    \"p_attention_dropout\":0.1,\n",
        "    \"p_decoder_dropout\":0.1,\n",
        "\n",
        "    # Attention parameters\n",
        "    \"attention_rnn_dim\":256,\n",
        "    \"attention_dim\":128,\n",
        "    \"attention_heads\": 4,\n",
        "\n",
        "    # Location Layer parameters\n",
        "    \"attention_location_n_filters\":32,\n",
        "    \"attention_location_kernel_size\":31,\n",
        "\n",
        "    # Mel-post processing network parameters\n",
        "    \"postnet_embedding_dim\":128,\n",
        "    \"postnet_kernel_size\":5,\n",
        "    \"postnet_n_convolutions\":2,\n",
        "\n",
        "    ################################\n",
        "    # Optimization Hyperparameters #\n",
        "    ################################\n",
        "    \"use_saved_learning_rate\":False,\n",
        "    \"learning_rate\":1e-3,\n",
        "    \"weight_decay\":1e-6,\n",
        "    \"grad_clip_thresh\":1.0,\n",
        "    \"batch_size\":32,\n",
        "    \"mask_padding\":True\n",
        "    # set model's padded outputs to padded values\n",
        "  }\n",
        "\n",
        "  hparams = mapDict(hparams)\n",
        "\n",
        "  return hparams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "st3RpVqf7JzX"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGZjv9RsEc4t"
      },
      "source": [
        "#### Reading Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2jAkFk6g8HVQ"
      },
      "outputs": [],
      "source": [
        "def read_json(file_path):\n",
        "  with open(file_path, 'r') as file:\n",
        "    file_content = json.load(file)\n",
        "  return file_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "szfPsiJR6_BI"
      },
      "outputs": [],
      "source": [
        "train_data = read_json('/content/drive/MyDrive/Dubbing Project/libspeech/train_output_dict.json')\n",
        "valid_data = read_json('/content/drive/MyDrive/Dubbing Project/libspeech/valid_output_dict.json')\n",
        "reverse_phoneme = read_json('/content/drive/MyDrive/Dubbing Project/libspeech/reverse_phoneme_dict.json')\n",
        "phoneme_list = read_json('/content/drive/MyDrive/Dubbing Project/libspeech/phoneme_dict.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEOTSNoW76ti",
        "outputId": "8e87a64e-3e59-47dd-e23b-6578bda0cd62"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28539, 2703)"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ],
      "source": [
        "len(train_data.keys()), len(valid_data.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2F-56Nvm9tEO",
        "outputId": "22bb6041-eabc-4bd3-ef10-75a2cb962f16"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('5678-43302-0000', dict_keys(['file_path', 'text', 'phonemes']))"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ],
      "source": [
        "list(train_data.keys())[0], train_data[list(train_data.keys())[0]].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-YPS22X9wD9",
        "outputId": "a5f44ff2-b0d0-4494-be5b-506c8c91e8d0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('6313-66125-0000', dict_keys(['file_path', 'text', 'phonemes']))"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ],
      "source": [
        "list(valid_data.keys())[0], valid_data[list(valid_data.keys())[0]].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyVREL0b8Srg",
        "outputId": "a7b126e2-8f7b-49e0-b87a-f13b14915dd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Phonemes in reverse:  72\n"
          ]
        }
      ],
      "source": [
        "print(\"Total Phonemes in reverse: \", len(reverse_phoneme))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZumhJHiB8lWg",
        "outputId": "f72e82b2-9a91-4124-dbb5-9f686fbf552e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Phonemes:  72\n"
          ]
        }
      ],
      "source": [
        "print(\"Total Phonemes: \", len(phoneme_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRf1F-ppat46",
        "outputId": "830fe684-96b4-40a2-b04a-f14f0adea957"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'AA2': 0,\n",
              " 'CH': 1,\n",
              " 'ZH': 2,\n",
              " 'DH': 3,\n",
              " 'UW0': 4,\n",
              " 'W': 5,\n",
              " 'IY0': 6,\n",
              " 'OY0': 7,\n",
              " 'UH2': 8,\n",
              " 'D': 9,\n",
              " 'Y': 10,\n",
              " 'Z': 11,\n",
              " 'OY2': 12,\n",
              " 'HH': 13,\n",
              " 'AY2': 14,\n",
              " 'IH0': 15,\n",
              " 'EH1': 16,\n",
              " 'UW2': 17,\n",
              " 'UH0': 18,\n",
              " 'JH': 19,\n",
              " 'ER2': 20,\n",
              " 'V': 21,\n",
              " 'IY2': 22,\n",
              " 'G': 23,\n",
              " 'M': 24,\n",
              " 'AH1': 25,\n",
              " 'irish': 26,\n",
              " 'AA0': 27,\n",
              " 'AY0': 28,\n",
              " 'OW1': 29,\n",
              " 'ER1': 30,\n",
              " 'AO0': 31,\n",
              " 'EY1': 32,\n",
              " 'L': 33,\n",
              " 'AA1': 34,\n",
              " 'AY1': 35,\n",
              " 'SH': 36,\n",
              " 'ER0': 37,\n",
              " 'AH0': 38,\n",
              " 'UH1': 39,\n",
              " 'K': 40,\n",
              " 'S': 41,\n",
              " 'AH2': 42,\n",
              " 'AO2': 43,\n",
              " 'IY1': 44,\n",
              " 'OW0': 45,\n",
              " 'T': 46,\n",
              " 'IH1': 47,\n",
              " 'AW2': 48,\n",
              " 'EH0': 49,\n",
              " 'org,': 50,\n",
              " 'AO1': 51,\n",
              " 'NG': 52,\n",
              " 'EH2': 53,\n",
              " 'R': 54,\n",
              " '#': 55,\n",
              " 'EY0': 56,\n",
              " 'AW0': 57,\n",
              " 'AE2': 58,\n",
              " 'AE1': 59,\n",
              " 'OW2': 60,\n",
              " 'N': 61,\n",
              " 'B': 62,\n",
              " 'AE0': 63,\n",
              " 'F': 64,\n",
              " 'AW1': 65,\n",
              " 'P': 66,\n",
              " 'TH': 67,\n",
              " 'EY2': 68,\n",
              " 'IH2': 69,\n",
              " 'UW1': 70,\n",
              " 'OY1': 71}"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ],
      "source": [
        "phoneme_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "awVYC9p28oCP"
      },
      "outputs": [],
      "source": [
        "## Adding Stop Token\n",
        "phoneme_list['<end/>'] = 72\n",
        "reverse_phoneme[str(72)] = '<end/>'\n",
        "\n",
        "phoneme_list['<start/>'] = 73\n",
        "reverse_phoneme[str(73)] = '<start/>'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDCZn_3O833e",
        "outputId": "9f6c50ff-b31a-41c2-d511-4e412442f129"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Phonemes after adding reverse:  74\n",
            "Total Phonemes after adding:  74\n"
          ]
        }
      ],
      "source": [
        "print(\"Total Phonemes after adding reverse: \", len(reverse_phoneme))\n",
        "print(\"Total Phonemes after adding: \", len(phoneme_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFZHlP_ljuJj"
      },
      "source": [
        "#### Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "W1moqnozfmbx"
      },
      "outputs": [],
      "source": [
        "def load_wav_to_torch(full_path):\n",
        "    # print(full_path)\n",
        "    sampling_rate, data = read(full_path)\n",
        "    return torch.FloatTensor(data.astype(np.float32)), sampling_rate\n",
        "\n",
        "# def load_flac_to_torch(full_path):\n",
        "#     print(\"Path: \", full_path)\n",
        "#     # Read FLAC file using pydub\n",
        "#     audio = AudioSegment.from_file(full_path, format=\"flac\")\n",
        "\n",
        "#     # Convert to a numpy array\n",
        "#     data = audio.get_array_of_samples()\n",
        "\n",
        "#     # Save the data as a temporary WAV file\n",
        "\n",
        "#     temp_wav_path = \"temp.wav\"\n",
        "#     audio.export(temp_wav_path, format=\"wav\")\n",
        "\n",
        "#     # Read the temporary WAV file using scipy.io.wavfile.read\n",
        "#     sample_rate, data = read(temp_wav_path)\n",
        "#     return torch.FloatTensor(data.astype(np.float32)), sample_rate\n",
        "\n",
        "def load_flac_to_torch(full_path):\n",
        "    # Read FLAC file using pydub\n",
        "    audio = AudioSegment.from_file(full_path, format=\"flac\")\n",
        "\n",
        "    # Convert to a numpy array\n",
        "    data = np.array(audio.get_array_of_samples(), dtype=np.float32)\n",
        "\n",
        "    # Normalize the data to the range [-1, 1]\n",
        "    data /= np.abs(data).max()\n",
        "\n",
        "    # Convert numpy array to PyTorch tensor\n",
        "    tensor_data = torch.from_numpy(data)\n",
        "\n",
        "    # Print the sample rate\n",
        "    sample_rate = audio.frame_rate\n",
        "\n",
        "    return tensor_data, sample_rate\n",
        "\n",
        "def dynamic_range_compression(x, C=1, clip_val=1e-5):\n",
        "    \"\"\"\n",
        "    PARAMS\n",
        "    ------\n",
        "    C: compression factor\n",
        "    \"\"\"\n",
        "    return torch.log(torch.clamp(x, min=clip_val) * C)\n",
        "\n",
        "def dynamic_range_decompression(x, C=1):\n",
        "    \"\"\"\n",
        "    PARAMS\n",
        "    ------\n",
        "    C: compression factor used to compress\n",
        "    \"\"\"\n",
        "    return torch.exp(x) / C\n",
        "\n",
        "def window_sumsquare(window, n_frames, hop_length=200, win_length=800,\n",
        "                     n_fft=800, dtype=np.float32, norm=None):\n",
        "    \"\"\"\n",
        "    # from librosa 0.6\n",
        "    Compute the sum-square envelope of a window function at a given hop length.\n",
        "\n",
        "    This is used to estimate modulation effects induced by windowing\n",
        "    observations in short-time fourier transforms.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    window : string, tuple, number, callable, or list-like\n",
        "        Window specification, as in `get_window`\n",
        "\n",
        "    n_frames : int > 0\n",
        "        The number of analysis frames\n",
        "\n",
        "    hop_length : int > 0\n",
        "        The number of samples to advance between frames\n",
        "\n",
        "    win_length : [optional]\n",
        "        The length of the window function.  By default, this matches `n_fft`.\n",
        "\n",
        "    n_fft : int > 0\n",
        "        The length of each analysis frame.\n",
        "\n",
        "    dtype : np.dtype\n",
        "        The data type of the output\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    wss : np.ndarray, shape=`(n_fft + hop_length * (n_frames - 1))`\n",
        "        The sum-squared envelope of the window function\n",
        "    \"\"\"\n",
        "    if win_length is None:\n",
        "        win_length = n_fft\n",
        "\n",
        "    n = n_fft + hop_length * (n_frames - 1)\n",
        "    x = np.zeros(n, dtype=dtype)\n",
        "\n",
        "    # Compute the squared window at the desired length\n",
        "    win_sq = get_window(window, win_length, fftbins=True)\n",
        "    win_sq = librosa_util.normalize(win_sq, norm=norm)**2\n",
        "    win_sq = librosa_util.pad_center(win_sq, n_fft)\n",
        "\n",
        "    # Fill the envelope\n",
        "    for i in range(n_frames):\n",
        "        sample = i * hop_length\n",
        "        x[sample:min(n, sample + n_fft)] += win_sq[:max(0, min(n_fft, n - sample))]\n",
        "    return x\n",
        "\n",
        "\n",
        "def get_mask_from_lengths(lengths):\n",
        "    max_len = torch.max(lengths).item()\n",
        "    ids = torch.arange(0, max_len, out=torch.cuda.LongTensor(max_len))\n",
        "    mask = (ids < lengths.unsqueeze(1)).bool()\n",
        "    return mask\n",
        "\n",
        "\n",
        "def load_filepaths_and_text(filename, split=\"|\"):\n",
        "    with open(filename, encoding='utf-8') as f:\n",
        "        filepaths_and_text = [line.strip().split(split) for line in f]\n",
        "    return filepaths_and_text\n",
        "\n",
        "\n",
        "def to_gpu(x):\n",
        "    x = x.contiguous()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        x = x.cuda(non_blocking=True)\n",
        "    return torch.autograd.Variable(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4vuS3LX3f4bN"
      },
      "outputs": [],
      "source": [
        "## stft.py\n",
        "\n",
        "\"\"\"\n",
        "BSD 3-Clause License\n",
        "\n",
        "Copyright (c) 2017, Prem Seetharaman\n",
        "All rights reserved.\n",
        "\n",
        "* Redistribution and use in source and binary forms, with or without\n",
        "  modification, are permitted provided that the following conditions are met:\n",
        "\n",
        "* Redistributions of source code must retain the above copyright notice,\n",
        "  this list of conditions and the following disclaimer.\n",
        "\n",
        "* Redistributions in binary form must reproduce the above copyright notice, this\n",
        "  list of conditions and the following disclaimer in the\n",
        "  documentation and/or other materials provided with the distribution.\n",
        "\n",
        "* Neither the name of the copyright holder nor the names of its\n",
        "  contributors may be used to endorse or promote products derived from this\n",
        "  software without specific prior written permission.\n",
        "\n",
        "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n",
        "ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n",
        "WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
        "DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR\n",
        "ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n",
        "(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n",
        "LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\n",
        "ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n",
        "(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n",
        "SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
        "\"\"\"\n",
        "\n",
        "class STFT(torch.nn.Module):\n",
        "    \"\"\"adapted from Prem Seetharaman's https://github.com/pseeth/pytorch-stft\"\"\"\n",
        "    def __init__(self, filter_length=800, hop_length=200, win_length=800,\n",
        "                 window='hann'):\n",
        "        super(STFT, self).__init__()\n",
        "        self.filter_length = filter_length\n",
        "        self.hop_length = hop_length\n",
        "        self.win_length = win_length\n",
        "        self.window = window\n",
        "        self.forward_transform = None\n",
        "        scale = self.filter_length / self.hop_length\n",
        "        fourier_basis = np.fft.fft(np.eye(self.filter_length))\n",
        "\n",
        "        cutoff = int((self.filter_length / 2 + 1))\n",
        "        fourier_basis = np.vstack([np.real(fourier_basis[:cutoff, :]),\n",
        "                                   np.imag(fourier_basis[:cutoff, :])])\n",
        "\n",
        "        forward_basis = torch.FloatTensor(fourier_basis[:, None, :])\n",
        "        inverse_basis = torch.FloatTensor(\n",
        "            np.linalg.pinv(scale * fourier_basis).T[:, None, :])\n",
        "\n",
        "        if window is not None:\n",
        "            assert(filter_length >= win_length)\n",
        "            # get window and zero center pad it to filter_length\n",
        "            fft_window = get_window(window, win_length, fftbins=True)\n",
        "            fft_window = pad_center(data=fft_window, size=filter_length)\n",
        "            fft_window = torch.from_numpy(fft_window).float()\n",
        "\n",
        "            # window the bases\n",
        "            forward_basis *= fft_window\n",
        "            inverse_basis *= fft_window\n",
        "\n",
        "        self.register_buffer('forward_basis', forward_basis.float())\n",
        "        self.register_buffer('inverse_basis', inverse_basis.float())\n",
        "\n",
        "    def transform(self, input_data):\n",
        "        num_batches = input_data.size(0)\n",
        "        num_samples = input_data.size(1)\n",
        "\n",
        "        self.num_samples = num_samples\n",
        "\n",
        "        # similar to librosa, reflect-pad the input\n",
        "        input_data = input_data.view(num_batches, 1, num_samples)\n",
        "        input_data = F.pad(\n",
        "            input_data.unsqueeze(1),\n",
        "            (int(self.filter_length / 2), int(self.filter_length / 2), 0, 0),\n",
        "            mode='reflect')\n",
        "        input_data = input_data.squeeze(1)\n",
        "\n",
        "        forward_transform = F.conv1d(\n",
        "            input_data,\n",
        "            Variable(self.forward_basis, requires_grad=False),\n",
        "            stride=self.hop_length,\n",
        "            padding=0)\n",
        "\n",
        "        cutoff = int((self.filter_length / 2) + 1)\n",
        "        real_part = forward_transform[:, :cutoff, :]\n",
        "        imag_part = forward_transform[:, cutoff:, :]\n",
        "\n",
        "        magnitude = torch.sqrt(real_part**2 + imag_part**2)\n",
        "        phase = torch.autograd.Variable(\n",
        "            torch.atan2(imag_part.data, real_part.data))\n",
        "\n",
        "        return magnitude, phase\n",
        "\n",
        "    def inverse(self, magnitude, phase):\n",
        "        recombine_magnitude_phase = torch.cat(\n",
        "            [magnitude*torch.cos(phase), magnitude*torch.sin(phase)], dim=1)\n",
        "\n",
        "        inverse_transform = F.conv_transpose1d(\n",
        "            recombine_magnitude_phase,\n",
        "            Variable(self.inverse_basis, requires_grad=False),\n",
        "            stride=self.hop_length,\n",
        "            padding=0)\n",
        "\n",
        "        if self.window is not None:\n",
        "            window_sum = window_sumsquare(\n",
        "                self.window, magnitude.size(-1), hop_length=self.hop_length,\n",
        "                win_length=self.win_length, n_fft=self.filter_length,\n",
        "                dtype=np.float32)\n",
        "            # remove modulation effects\n",
        "            approx_nonzero_indices = torch.from_numpy(\n",
        "                np.where(window_sum > tiny(window_sum))[0])\n",
        "            window_sum = torch.autograd.Variable(\n",
        "                torch.from_numpy(window_sum), requires_grad=False)\n",
        "            window_sum = window_sum.cuda() if magnitude.is_cuda else window_sum\n",
        "            inverse_transform[:, :, approx_nonzero_indices] /= window_sum[approx_nonzero_indices]\n",
        "\n",
        "            # scale by hop ratio\n",
        "            inverse_transform *= float(self.filter_length) / self.hop_length\n",
        "\n",
        "        inverse_transform = inverse_transform[:, :, int(self.filter_length/2):]\n",
        "        inverse_transform = inverse_transform[:, :, :-int(self.filter_length/2):]\n",
        "\n",
        "        return inverse_transform\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        self.magnitude, self.phase = self.transform(input_data)\n",
        "        reconstruction = self.inverse(self.magnitude, self.phase)\n",
        "        return reconstruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Cs085jA5fyzG"
      },
      "outputs": [],
      "source": [
        "class TacotronSTFT(torch.nn.Module):\n",
        "    def __init__(self, filter_length=1024, hop_length=256, win_length=1024,\n",
        "                 n_mel_channels=80, sampling_rate=22050, mel_fmin=0.0,\n",
        "                 mel_fmax=8000.0):\n",
        "        super(TacotronSTFT, self).__init__()\n",
        "        self.n_mel_channels = n_mel_channels\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.stft_fn = STFT(filter_length, hop_length, win_length)\n",
        "        mel_basis = librosa_mel_fn(\n",
        "            sr=sampling_rate, n_fft=filter_length, n_mels=n_mel_channels, fmin=mel_fmin, fmax=mel_fmax)\n",
        "        mel_basis = torch.from_numpy(mel_basis).float()\n",
        "        self.register_buffer('mel_basis', mel_basis)\n",
        "\n",
        "    def spectral_normalize(self, magnitudes):\n",
        "        output = dynamic_range_compression(magnitudes)\n",
        "        return output\n",
        "\n",
        "    def spectral_de_normalize(self, magnitudes):\n",
        "        output = dynamic_range_decompression(magnitudes)\n",
        "        return output\n",
        "\n",
        "    def spectrogram(self,y):\n",
        "        assert(torch.min(y.data) >= -1)\n",
        "        assert(torch.max(y.data) <= 1)\n",
        "\n",
        "        magnitudes, phases = self.stft_fn.transform(y)\n",
        "        magnitudes = magnitudes.data\n",
        "\n",
        "        return magnitudes\n",
        "\n",
        "    def mel_spectrogram(self, y):\n",
        "        \"\"\"Computes mel-spectrograms from a batch of waves\n",
        "        PARAMS\n",
        "        ------\n",
        "        y: Variable(torch.FloatTensor) with shape (B, T) in range [-1, 1]\n",
        "\n",
        "        RETURNS\n",
        "        -------\n",
        "        mel_output: torch.FloatTensor of shape (B, n_mel_channels, T)\n",
        "        \"\"\"\n",
        "        # assert(torch.min(y.data) >= -1)\n",
        "        # assert(torch.max(y.data) <= 1)\n",
        "        magnitudes, phases = self.stft_fn.transform(y)\n",
        "        magnitudes = magnitudes.data\n",
        "        mel_output = torch.matmul(self.mel_basis, magnitudes)\n",
        "        mel_output = self.spectral_normalize(mel_output)\n",
        "        return mel_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5kuyVHFb9ieA"
      },
      "outputs": [],
      "source": [
        "train_audio_list = [i['file_path'] for i in train_data.values()]\n",
        "train_phonemes_list = [i['phonemes'] for i in train_data.values()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "Ssf1kwMtRqJ2"
      },
      "outputs": [],
      "source": [
        "### data_utils.py\n",
        "class AudioPhonemeDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "        1) loads audio,text pairs\n",
        "        2) normalizes text and converts them to sequences of one-hot vectors\n",
        "        3) computes mel-spectrograms from audio files.\n",
        "    \"\"\"\n",
        "    def __init__(self, audio_paths, phoneme_list, hparams):\n",
        "        self.inputs = audio_paths\n",
        "        self.outputs = phoneme_list\n",
        "        self.text_cleaners = hparams.text_cleaners\n",
        "        self.max_wav_value = hparams.max_wav_value\n",
        "        self.sampling_rate = hparams.sampling_rate\n",
        "        self.load_mel_from_disk = hparams.load_mel_from_disk\n",
        "        self.stft = TacotronSTFT(\n",
        "            hparams.filter_length, hparams.hop_length, hparams.win_length,\n",
        "            hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,\n",
        "            hparams.mel_fmax)\n",
        "\n",
        "    def get_mel_spec_pair(self, index):\n",
        "        # separate filename and text\n",
        "        # lin = self.get_spec(self.outputs[index])\n",
        "        # mel = self.get_mel(self.inputs[index])\n",
        "        inputs = self.get_mel(self.inputs[index])\n",
        "        outputs = self.get_phonemes_list(self.outputs[index])\n",
        "\n",
        "        return (inputs,outputs)\n",
        "\n",
        "    def get_mel(self, filename):\n",
        "        if not self.load_mel_from_disk:\n",
        "            audio, sampling_rate = load_flac_to_torch(filename)\n",
        "            # audio = audio[:, 0]\n",
        "            # if sampling_rate != self.stft.sampling_rate:\n",
        "            #     raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
        "            #         sampling_rate, self.stft.sampling_rate))\n",
        "            audio_norm = audio / self.max_wav_value\n",
        "            audio_norm = audio_norm.unsqueeze(0)\n",
        "            audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
        "            melspec = self.stft.mel_spectrogram(audio_norm)\n",
        "            melspec = torch.squeeze(melspec, 0)\n",
        "            melspec_arr = melspec[0].numpy()\n",
        "        else:\n",
        "            melspec = torch.from_numpy(np.load(filename))\n",
        "            assert melspec.size(0) == self.stft.n_mel_channels, (\n",
        "                'Mel dimension mismatch: given {}, expected {}'.format(\n",
        "                    melspec.size(0), self.stft.n_mel_channels))\n",
        "\n",
        "        return melspec\n",
        "\n",
        "    def get_phonemes_list(self, phonemes):\n",
        "        phonemes.insert(0, '<start/>')\n",
        "        phonemes.append('<end/>')\n",
        "\n",
        "        tensor_phonemes = torch.zeros(hparams.vocab_size, len(phonemes))\n",
        "\n",
        "        for i, phoneme in enumerate(phonemes):\n",
        "            tensor_phonemes[phoneme_list[phoneme], i] = 1\n",
        "\n",
        "        return tensor_phonemes\n",
        "\n",
        "\n",
        "    def get_spec(self, filename):\n",
        "        if not self.load_mel_from_disk:\n",
        "            audio, sampling_rate = load_wav_to_torch(filename)\n",
        "            if sampling_rate != self.stft.sampling_rate:\n",
        "                raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
        "                    sampling_rate, self.stft.sampling_rate))\n",
        "            audio_norm = audio / self.max_wav_value\n",
        "            audio_norm = audio_norm.unsqueeze(0)\n",
        "            audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
        "            spec = self.stft.spectrogram(audio_norm)\n",
        "            spec = torch.squeeze(spec, 0)\n",
        "        else:\n",
        "            spec = torch.from_numpy(np.load(filename))\n",
        "\n",
        "        return spec\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.get_mel_spec_pair(index)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "\n",
        "class AudioPhonemesCollate():\n",
        "    \"\"\" Zero-pads model inputs and targets based on number of frames per setep\n",
        "    \"\"\"\n",
        "    def __init__(self, n_frames_per_step):\n",
        "        self.n_frames_per_step = n_frames_per_step\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        \"\"\"Collate's training batch from normalized text and mel-spectrogram\n",
        "        PARAMS\n",
        "        ------\n",
        "        batch: [mel_normalized, text_normalized]\n",
        "        \"\"\"\n",
        "\n",
        "        num_mels = batch[0][0].size(0)\n",
        "\n",
        "\n",
        "        # Sort sequences by input length in descending order\n",
        "        input_lengths, ids_sorted_decreasing = torch.sort(\n",
        "            torch.LongTensor([x[0].size(1) for x in batch]),\n",
        "            dim=0, descending=True\n",
        "        )\n",
        "\n",
        "        max_input_len = input_lengths[0]\n",
        "\n",
        "        # Pad input sequences\n",
        "        input_padded = torch.FloatTensor(len(batch), num_mels, max_input_len)\n",
        "        input_padded.zero_()\n",
        "        for i in ids_sorted_decreasing:\n",
        "            mel = batch[i][0]\n",
        "            input_padded[i, :, :mel.size(1)] = mel\n",
        "\n",
        "        max_target_len = max([x[1].size(1) for x in batch])\n",
        "\n",
        "        # include mel padded and gate padded\n",
        "        target_padded = torch.FloatTensor(len(batch), hparams.vocab_size, max_target_len)\n",
        "        target_padded.zero_()\n",
        "        gate_padded = torch.FloatTensor(len(batch), max_target_len)\n",
        "        gate_padded.zero_()\n",
        "        output_lengths = torch.LongTensor(len(batch))\n",
        "\n",
        "        for i in ids_sorted_decreasing:\n",
        "            target = batch[i][1]\n",
        "            print(target.shape)\n",
        "            target_padded[i, :, :target.size(1)] = target\n",
        "            gate_padded[i, :target.size(1)] = 1\n",
        "            output_lengths[i] = target.size(1)\n",
        "\n",
        "        return input_padded, input_lengths, target_padded, gate_padded, \\\n",
        "            output_lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "hDl-pVB5UO0Q"
      },
      "outputs": [],
      "source": [
        "hparams = create_hparams()\n",
        "\n",
        "torch.manual_seed(hparams.seed)\n",
        "torch.cuda.manual_seed(hparams.seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "IcdZxG3-XsqR"
      },
      "outputs": [],
      "source": [
        "hparams.vocab_size = len(phoneme_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "uAFn9E1cdvDT"
      },
      "outputs": [],
      "source": [
        "for audio in train_audio_list:\n",
        "  if not audio:\n",
        "    print(audio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "UsGWWvX-TNKd"
      },
      "outputs": [],
      "source": [
        "train_audio_list = [i['file_path'] for i in train_data.values()]\n",
        "train_phonemes_list = [i['phonemes'] for i in train_data.values()]\n",
        "\n",
        "train_dataset = AudioPhonemeDataset(train_audio_list, train_phonemes_list, hparams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "It-9DxY7a1Nw"
      },
      "outputs": [],
      "source": [
        "valid_audio_list = [i['file_path'] for i in valid_data.values()]\n",
        "valid_phonemes_list = [i['phonemes'] for i in valid_data.values()]\n",
        "\n",
        "val_dataset = AudioPhonemeDataset(valid_audio_list, valid_phonemes_list, hparams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "QTBsasJcTVf4"
      },
      "outputs": [],
      "source": [
        "audio, phonemes = train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnCIHgcEVgzn",
        "outputId": "384607a3-50cd-4027-8060-d465cb6d441b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([80, 937]), torch.Size([74, 163]))"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ],
      "source": [
        "audio.shape, phonemes.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "nmzfSPl1Wvg6"
      },
      "outputs": [],
      "source": [
        "collate_fn = AudioPhonemesCollate(hparams.n_frames_per_step)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, num_workers=2, shuffle=True,\n",
        "                          batch_size=3, pin_memory=False,\n",
        "                          drop_last=True, collate_fn=collate_fn)\n",
        "\n",
        "valid_dataloader = DataLoader(val_dataset, num_workers=2, shuffle=True,\n",
        "                          batch_size=3, pin_memory=False,\n",
        "                          drop_last=True, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xH-RG8rCaEz4",
        "outputId": "66a3fc20-fb5f-49b9-e627-f7bd53279bd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([74, 159])\n",
            "torch.Size([74, 109])\n",
            "torch.Size([74, 52])\n",
            "torch.Size([74, 150])\n",
            "torch.Size([74, 148])\n",
            "torch.Size([74, 156])\n",
            "Inputs Shape:  torch.Size([3, 80, 926]) torch.Size([3])\n",
            "Outputs: \n",
            "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         [1., 0., 0.,  ..., 0., 0., 0.]]])\n",
            "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
            "torch.Size([74, 208])\n",
            "torch.Size([74, 154])\n",
            "torch.Size([74, 139])\n",
            "torch.Size([74, 146])\n",
            "torch.Size([74, 141])\n",
            "torch.Size([74, 76])\n"
          ]
        }
      ],
      "source": [
        "for i, batch in enumerate(train_loader):\n",
        "  input_padded, input_lengths, target_padded, gate_padded, \\\n",
        "            output_lengths = batch\n",
        "\n",
        "  print(\"Inputs Shape: \", input_padded.shape, input_lengths.shape)\n",
        "\n",
        "  print(\"Outputs: \",)\n",
        "  print(target_padded)\n",
        "  print(gate_padded)\n",
        "\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWlOX8Kxj9fl"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "4uXHgfbGaJcG"
      },
      "outputs": [],
      "source": [
        "## layers.py\n",
        "\n",
        "class LinearNorm(torch.nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, bias=True, w_init_gain='linear'):\n",
        "        super(LinearNorm, self).__init__()\n",
        "        self.linear_layer = torch.nn.Linear(in_dim, out_dim, bias=bias)\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(\n",
        "            self.linear_layer.weight,\n",
        "            gain=torch.nn.init.calculate_gain(w_init_gain))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear_layer(x)\n",
        "\n",
        "\n",
        "class ConvNorm(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,\n",
        "                 padding=None, dilation=1, bias=True, w_init_gain='linear'):\n",
        "        super(ConvNorm, self).__init__()\n",
        "        if padding is None:\n",
        "            assert(kernel_size % 2 == 1)\n",
        "            padding = int(dilation * (kernel_size - 1) / 2)\n",
        "\n",
        "        self.conv = torch.nn.Conv1d(in_channels, out_channels,\n",
        "                                    kernel_size=kernel_size, stride=stride,\n",
        "                                    padding=padding, dilation=dilation,\n",
        "                                    bias=bias)\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(\n",
        "            self.conv.weight, gain=torch.nn.init.calculate_gain(w_init_gain))\n",
        "\n",
        "    def forward(self, signal):\n",
        "        conv_signal = self.conv(signal)\n",
        "        return conv_signal\n",
        "\n",
        "\n",
        "class TacotronSTFT(torch.nn.Module):\n",
        "    def __init__(self, filter_length=1024, hop_length=256, win_length=1024,\n",
        "                 n_mel_channels=80, sampling_rate=22050, mel_fmin=0.0,\n",
        "                 mel_fmax=8000.0):\n",
        "        super(TacotronSTFT, self).__init__()\n",
        "        self.n_mel_channels = n_mel_channels\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.stft_fn = STFT(filter_length, hop_length, win_length)\n",
        "        mel_basis = librosa_mel_fn(\n",
        "            sr=sampling_rate, n_fft=filter_length, n_mels=n_mel_channels, fmin=mel_fmin, fmax=mel_fmax)\n",
        "        mel_basis = torch.from_numpy(mel_basis).float()\n",
        "        self.register_buffer('mel_basis', mel_basis)\n",
        "\n",
        "    def spectral_normalize(self, magnitudes):\n",
        "        output = dynamic_range_compression(magnitudes)\n",
        "        return output\n",
        "\n",
        "    def spectral_de_normalize(self, magnitudes):\n",
        "        output = dynamic_range_decompression(magnitudes)\n",
        "        return output\n",
        "\n",
        "    def spectrogram(self,y):\n",
        "        assert(torch.min(y.data) >= -1)\n",
        "        assert(torch.max(y.data) <= 1)\n",
        "\n",
        "        magnitudes, phases = self.stft_fn.transform(y)\n",
        "        magnitudes = magnitudes.data\n",
        "\n",
        "        return magnitudes\n",
        "\n",
        "    def mel_spectrogram(self, y):\n",
        "        \"\"\"Computes mel-spectrograms from a batch of waves\n",
        "        PARAMS\n",
        "        ------\n",
        "        y: Variable(torch.FloatTensor) with shape (B, T) in range [-1, 1]\n",
        "\n",
        "        RETURNS\n",
        "        -------\n",
        "        mel_output: torch.FloatTensor of shape (B, n_mel_channels, T)\n",
        "        \"\"\"\n",
        "        # assert(torch.min(y.data) >= -1)\n",
        "        # assert(torch.max(y.data) <= 1)\n",
        "        magnitudes, phases = self.stft_fn.transform(y)\n",
        "        magnitudes = magnitudes.data\n",
        "        mel_output = torch.matmul(self.mel_basis, magnitudes)\n",
        "        mel_output = self.spectral_normalize(mel_output)\n",
        "        return mel_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "tUncKz0ckAGB"
      },
      "outputs": [],
      "source": [
        "## model.py\n",
        "\n",
        "class LocationLayer(nn.Module):\n",
        "    def __init__(self, attention_n_filters, attention_kernel_size,\n",
        "                 attention_dim):\n",
        "        super(LocationLayer, self).__init__()\n",
        "        padding = int((attention_kernel_size - 1) / 2)\n",
        "        self.location_conv = ConvNorm(2, attention_n_filters,\n",
        "                                      kernel_size=attention_kernel_size,\n",
        "                                      padding=padding, bias=False, stride=1,\n",
        "                                      dilation=1)\n",
        "        self.location_dense = LinearNorm(attention_n_filters, attention_dim,\n",
        "                                         bias=False, w_init_gain='tanh')\n",
        "\n",
        "    def forward(self, attention_weights_cat):\n",
        "        processed_attention = self.location_conv(attention_weights_cat)\n",
        "        processed_attention = processed_attention.transpose(1, 2)\n",
        "        processed_attention = self.location_dense(processed_attention)\n",
        "        return processed_attention\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, attention_rnn_dim, embedding_dim, attention_dim,\n",
        "                 attention_location_n_filters, attention_location_kernel_size,num_heads=4):\n",
        "        super(Attention, self).__init__()\n",
        "        self.query_layer = LinearNorm(attention_rnn_dim, attention_dim,\n",
        "                                      bias=False, w_init_gain='tanh')\n",
        "        self.memory_layer = LinearNorm(embedding_dim, attention_dim, bias=False,\n",
        "                                       w_init_gain='tanh')\n",
        "        # self.v = LinearNorm(attention_dim, 1, bias=False)\n",
        "        self.attention = nn.MultiheadAttention(attention_dim,num_heads,batch_first=True)\n",
        "        self.location_layer = LocationLayer(attention_location_n_filters,\n",
        "                                            attention_location_kernel_size,\n",
        "                                            attention_dim)\n",
        "        self.score_mask_value = -float(\"inf\")\n",
        "\n",
        "    def get_alignment_energies(self, query, processed_memory,\n",
        "                               attention_weights_cat):\n",
        "        \"\"\"\n",
        "        PARAMS\n",
        "        ------\n",
        "        query: decoder output (batch, n_mel_channels * n_frames_per_step)\n",
        "        processed_memory: processed encoder outputs (B, T_in, attention_dim)\n",
        "        attention_weights_cat: cumulative and prev. att weights (B, 2, max_time)\n",
        "\n",
        "        RETURNS\n",
        "        -------\n",
        "        alignment (batch, max_time)\n",
        "        \"\"\"\n",
        "\n",
        "        processed_query = self.query_layer(query.unsqueeze(1))\n",
        "        processed_attention_weights = self.location_layer(attention_weights_cat)\n",
        "        # energies = self.v(torch.tanh(\n",
        "        #     processed_query + processed_attention_weights + processed_memory))\n",
        "        # energies = energies.squeeze(-1)\n",
        "        return processed_query,processed_attention_weights\n",
        "\n",
        "    def forward(self, attention_hidden_state, memory, processed_memory,\n",
        "                attention_weights_cat, mask):\n",
        "        \"\"\"\n",
        "        PARAMS\n",
        "        ------\n",
        "        attention_hidden_state: attention rnn last output\n",
        "        memory: encoder outputs\n",
        "        processed_memory: processed encoder outputs\n",
        "        attention_weights_cat: previous and cummulative attention weights\n",
        "        mask: binary mask for padded data\n",
        "        \"\"\"\n",
        "        # alignment = self.get_alignment_energies(\n",
        "            # attention_hidden_state, processed_memory, attention_weights_cat)\n",
        "\n",
        "        processed_query,processed_attention_weights = self.get_alignment_energies(\n",
        "        attention_hidden_state, processed_memory, attention_weights_cat)\n",
        "\n",
        "        # if mask is not None:\n",
        "        #     alignment.data.masked_fill_(mask, self.score_mask_value)\n",
        "\n",
        "        # attention_weights = F.softmax(alignment, dim=1)\n",
        "        # attention_context = torch.bmm(attention_weights.unsqueeze(1), memory)\n",
        "        # attention_context = attention_context.squeeze(1)\n",
        "\n",
        "        attention_context,attention_weights = self.attention(processed_query,processed_attention_weights,processed_memory)\n",
        "        return attention_context.squeeze(1), attention_weights.squeeze(1)\n",
        "\n",
        "\n",
        "# class Attention(nn.Module):\n",
        "#     def __init__(self, attention_rnn_dim, embedding_dim, attention_dim,\n",
        "#                  attention_location_n_filters, attention_location_kernel_size):\n",
        "#         super(Attention, self).__init__()\n",
        "#         self.query_layer = LinearNorm(attention_rnn_dim, attention_dim,\n",
        "#                                       bias=False, w_init_gain='tanh')\n",
        "#         self.memory_layer = LinearNorm(embedding_dim, attention_dim, bias=False,\n",
        "#                                        w_init_gain='tanh')\n",
        "#         self.v = LinearNorm(attention_dim, 1, bias=False)\n",
        "#         self.location_layer = LocationLayer(attention_location_n_filters,\n",
        "#                                             attention_location_kernel_size,\n",
        "#                                             attention_dim)\n",
        "#         self.score_mask_value = -float(\"inf\")\n",
        "\n",
        "#     def get_alignment_energies(self, query, processed_memory,\n",
        "#                                attention_weights_cat):\n",
        "#         \"\"\"\n",
        "#         PARAMS\n",
        "#         ------\n",
        "#         query: decoder output (batch, n_mel_channels * n_frames_per_step)\n",
        "#         processed_memory: processed encoder outputs (B, T_in, attention_dim)\n",
        "#         attention_weights_cat: cumulative and prev. att weights (B, 2, max_time)\n",
        "\n",
        "#         RETURNS\n",
        "#         -------\n",
        "#         alignment (batch, max_time)\n",
        "#         \"\"\"\n",
        "\n",
        "#         processed_query = self.query_layer(query.unsqueeze(1))\n",
        "#         processed_attention_weights = self.location_layer(attention_weights_cat)\n",
        "#         energies = self.v(torch.tanh(\n",
        "#             processed_query + processed_attention_weights + processed_memory))\n",
        "\n",
        "#         energies = energies.squeeze(-1)\n",
        "#         return energies\n",
        "\n",
        "#     def forward(self, attention_hidden_state, memory, processed_memory,\n",
        "#                 attention_weights_cat, mask):\n",
        "#         \"\"\"\n",
        "#         PARAMS\n",
        "#         ------\n",
        "#         attention_hidden_state: attention rnn last output\n",
        "#         memory: encoder outputs\n",
        "#         processed_memory: processed encoder outputs\n",
        "#         attention_weights_cat: previous and cummulative attention weights\n",
        "#         mask: binary mask for padded data\n",
        "#         \"\"\"\n",
        "#         alignment = self.get_alignment_energies(\n",
        "#             attention_hidden_state, processed_memory, attention_weights_cat)\n",
        "\n",
        "#         if mask is not None:\n",
        "#             alignment.data.masked_fill_(mask, self.score_mask_value)\n",
        "\n",
        "#         attention_weights = F.softmax(alignment, dim=1)\n",
        "#         attention_context = torch.bmm(attention_weights.unsqueeze(1), memory)\n",
        "#         attention_context = attention_context.squeeze(1)\n",
        "\n",
        "#         return attention_context, attention_weights\n",
        "\n",
        "\n",
        "class Prenet(nn.Module):\n",
        "    def __init__(self, in_dim, sizes):\n",
        "        super(Prenet, self).__init__()\n",
        "        in_sizes = [in_dim] + sizes[:-1]\n",
        "        self.layers = nn.ModuleList(\n",
        "            [LinearNorm(in_size, out_size, bias=False)\n",
        "             for (in_size, out_size) in zip(in_sizes, sizes)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for linear in self.layers:\n",
        "            x = F.dropout(F.relu(linear(x)), p=0.5, training=True)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Postnet(nn.Module):\n",
        "    \"\"\"Postnet\n",
        "        - Five 1-d convolution with 512 channels and kernel size 5\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hparams):\n",
        "        super(Postnet, self).__init__()\n",
        "        self.convolutions = nn.ModuleList()\n",
        "\n",
        "        self.convolutions.append(\n",
        "            nn.Sequential(\n",
        "                ConvNorm(hparams.vocab_size, hparams.postnet_embedding_dim,\n",
        "                         kernel_size=hparams.postnet_kernel_size, stride=1,\n",
        "                         padding=int((hparams.postnet_kernel_size - 1) / 2),\n",
        "                         dilation=1, w_init_gain='tanh'),\n",
        "                nn.BatchNorm1d(hparams.postnet_embedding_dim))\n",
        "        )\n",
        "\n",
        "        for i in range(1, hparams.postnet_n_convolutions - 1):\n",
        "            self.convolutions.append(\n",
        "                nn.Sequential(\n",
        "                    ConvNorm(hparams.postnet_embedding_dim,\n",
        "                             hparams.postnet_embedding_dim,\n",
        "                             kernel_size=hparams.postnet_kernel_size, stride=1,\n",
        "                             padding=int((hparams.postnet_kernel_size - 1) / 2),\n",
        "                             dilation=1, w_init_gain='tanh'),\n",
        "                    nn.BatchNorm1d(hparams.postnet_embedding_dim))\n",
        "            )\n",
        "\n",
        "        self.convolutions.append(\n",
        "            nn.Sequential(\n",
        "                ConvNorm(hparams.postnet_embedding_dim, hparams.vocab_size,\n",
        "                         kernel_size=hparams.postnet_kernel_size, stride=1,\n",
        "                         padding=int((hparams.postnet_kernel_size - 1) / 2),\n",
        "                         dilation=1, w_init_gain='linear'),\n",
        "                nn.BatchNorm1d(hparams.vocab_size))\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(len(self.convolutions) - 1):\n",
        "            x = F.dropout(torch.tanh(self.convolutions[i](x)), 0.5, self.training)\n",
        "        x = F.dropout(self.convolutions[-1](x), 0.5, self.training)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"Encoder module:\n",
        "        - Three 1-d convolution banks\n",
        "        - Bidirectional LSTM\n",
        "    \"\"\"\n",
        "    def __init__(self, hparams):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        # convolutions = []\n",
        "        # for _ in range(hparams.encoder_n_convolutions):\n",
        "        #     conv_layer = nn.Sequential(\n",
        "        #         ConvNorm(hparams.encoder_embedding_dim,\n",
        "        #                  hparams.encoder_embedding_dim,\n",
        "        #                  kernel_size=hparams.encoder_kernel_size, stride=1,\n",
        "        #                  padding=int((hparams.encoder_kernel_size - 1) / 2),\n",
        "        #                  dilation=1, w_init_gain='relu'),\n",
        "        #         nn.BatchNorm1d(hparams.encoder_embedding_dim))\n",
        "        #     convolutions.append(conv_layer)\n",
        "        # self.convolutions = nn.ModuleList(convolutions)\n",
        "\n",
        "        self.lstm = nn.LSTM(hparams.encoder_embedding_dim,\n",
        "                            int(hparams.encoder_embedding_dim/2), num_layers= 1, # According to paper it should be 8\n",
        "                            batch_first=True, bidirectional=True)\n",
        "\n",
        "    def forward(self, x, input_lengths):\n",
        "        # for conv in self.convolutions:\n",
        "        #     x = F.dropout(F.relu(conv(x)), 0.5, self.training)\n",
        "\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        # pytorch tensor are not reversible, hence the conversion\n",
        "        input_lengths = input_lengths.cpu().numpy()\n",
        "        x = nn.utils.rnn.pack_padded_sequence(\n",
        "            x, input_lengths, batch_first=True)\n",
        "\n",
        "        self.lstm.flatten_parameters()\n",
        "        outputs, _ = self.lstm(x)\n",
        "\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(\n",
        "            outputs, batch_first=True)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def inference(self, x):\n",
        "        # for conv in self.convolutions:\n",
        "        #     x = F.dropout(F.relu(conv(x)), 0.5, self.training)\n",
        "\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        self.lstm.flatten_parameters()\n",
        "        outputs, _ = self.lstm(x)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class PhonemesDecoder(nn.Module):\n",
        "    def __init__(self, hparams):\n",
        "        super(PhonemesDecoder, self).__init__()\n",
        "        self.n_mel_channels = hparams.n_mel_channels\n",
        "        self.out_channels = hparams.out_channels\n",
        "        self.n_frames_per_step = hparams.n_frames_per_step\n",
        "        self.encoder_embedding_dim = hparams.encoder_embedding_dim\n",
        "        self.attention_rnn_dim = hparams.attention_rnn_dim\n",
        "        self.decoder_rnn_dim = hparams.decoder_rnn_dim\n",
        "        self.prenet_dim = hparams.prenet_dim\n",
        "        self.max_decoder_steps = hparams.max_decoder_steps\n",
        "        self.gate_threshold = hparams.gate_threshold\n",
        "        self.p_attention_dropout = hparams.p_attention_dropout\n",
        "        self.p_decoder_dropout = hparams.p_decoder_dropout\n",
        "        self.attention_dim = hparams.attention_dim\n",
        "\n",
        "        self.prenet = Prenet(\n",
        "            hparams.vocab_size,\n",
        "            [hparams.prenet_dim, hparams.prenet_dim])\n",
        "\n",
        "        self.attention_rnn = nn.LSTMCell(\n",
        "            hparams.prenet_dim + hparams.encoder_embedding_dim,\n",
        "            hparams.attention_rnn_dim)\n",
        "\n",
        "        self.attention_layer = Attention(\n",
        "            hparams.attention_rnn_dim, hparams.encoder_embedding_dim,\n",
        "            hparams.attention_dim, hparams.attention_location_n_filters,\n",
        "            hparams.attention_location_kernel_size)\n",
        "\n",
        "        self.decoder_rnn = nn.LSTMCell(\n",
        "            hparams.attention_rnn_dim + hparams.encoder_embedding_dim,\n",
        "            hparams.decoder_rnn_dim, 1)\n",
        "\n",
        "        self.linear_projection = LinearNorm(\n",
        "            hparams.decoder_rnn_dim + hparams.encoder_embedding_dim,\n",
        "            hparams.vocab_size, w_init_gain='sigmoid')\n",
        "\n",
        "        self.gate_layer = LinearNorm(\n",
        "            hparams.decoder_rnn_dim + hparams.encoder_embedding_dim, 1,\n",
        "            bias=True, w_init_gain='sigmoid')\n",
        "\n",
        "    def get_go_frame(self, memory):\n",
        "        \"\"\" Gets all zeros frames to use as first decoder input\n",
        "        PARAMS\n",
        "        ------\n",
        "        memory: decoder outputs\n",
        "\n",
        "        RETURNS\n",
        "        -------\n",
        "        decoder_input: all zeros frames\n",
        "        \"\"\"\n",
        "        B = memory.size(0)\n",
        "        decoder_input = Variable(memory.data.new(\n",
        "            B, hparams.vocab_size).zero_())\n",
        "        return decoder_input\n",
        "\n",
        "    def initialize_decoder_states(self, memory, mask):\n",
        "        \"\"\" Initializes attention rnn states, decoder rnn states, attention\n",
        "        weights, attention cumulative weights, attention context, stores memory\n",
        "        and stores processed memory\n",
        "        PARAMS\n",
        "        ------\n",
        "        memory: Encoder outputs\n",
        "        mask: Mask for padded data if training, expects None for inference\n",
        "        \"\"\"\n",
        "        B = memory.size(0)\n",
        "        MAX_TIME = memory.size(1)\n",
        "\n",
        "        self.attention_hidden = Variable(memory.data.new(\n",
        "            B, self.attention_rnn_dim).zero_())\n",
        "        self.attention_cell = Variable(memory.data.new(\n",
        "            B, self.attention_rnn_dim).zero_())\n",
        "\n",
        "        self.decoder_hidden = Variable(memory.data.new(\n",
        "            B, self.decoder_rnn_dim).zero_())\n",
        "        self.decoder_cell = Variable(memory.data.new(\n",
        "            B, self.decoder_rnn_dim).zero_())\n",
        "\n",
        "        self.attention_weights = Variable(memory.data.new(\n",
        "            B, MAX_TIME).zero_())\n",
        "        self.attention_weights_cum = Variable(memory.data.new(\n",
        "            B, MAX_TIME).zero_())\n",
        "        self.attention_context = Variable(memory.data.new(\n",
        "            B, self.encoder_embedding_dim).zero_())\n",
        "\n",
        "        self.memory = memory\n",
        "        self.processed_memory = self.attention_layer.memory_layer(memory)\n",
        "        self.mask = mask\n",
        "\n",
        "    def parse_decoder_inputs(self, decoder_inputs):\n",
        "        \"\"\" Prepares decoder inputs, i.e. mel outputs\n",
        "        PARAMS\n",
        "        ------\n",
        "        decoder_inputs: inputs used for teacher-forced training, i.e. mel-specs\n",
        "\n",
        "        RETURNS\n",
        "        -------\n",
        "        inputs: processed decoder inputs\n",
        "\n",
        "        \"\"\"\n",
        "        # (B, n_mel_channels, T_out) -> (B, T_out, n_mel_channels)\n",
        "        decoder_inputs = decoder_inputs.transpose(1, 2)\n",
        "        decoder_inputs = decoder_inputs.view(\n",
        "            decoder_inputs.size(0),\n",
        "            int(decoder_inputs.size(1)/self.n_frames_per_step), -1)\n",
        "        # (B, T_out, n_mel_channels) -> (T_out, B, n_mel_channels)\n",
        "        decoder_inputs = decoder_inputs.transpose(0, 1)\n",
        "        return decoder_inputs\n",
        "\n",
        "    def parse_decoder_outputs(self, mel_outputs, gate_outputs, alignments):\n",
        "        \"\"\" Prepares decoder outputs for output\n",
        "        PARAMS\n",
        "        ------\n",
        "        mel_outputs:\n",
        "        gate_outputs: gate output energies\n",
        "        alignments:\n",
        "\n",
        "        RETURNS\n",
        "        -------\n",
        "        mel_outputs:\n",
        "        gate_outpust: gate output energies\n",
        "        alignments:\n",
        "        \"\"\"\n",
        "        # (T_out, B) -> (B, T_out)\n",
        "        alignments = torch.stack(alignments).transpose(0, 1)\n",
        "        # (T_out, B) -> (B, T_out)\n",
        "        gate_outputs = torch.stack(gate_outputs).transpose(0, 1)\n",
        "        gate_outputs = gate_outputs.contiguous()\n",
        "        # (T_out, B, n_mel_channels) -> (B, T_out, n_mel_channels)\n",
        "        mel_outputs = torch.stack(mel_outputs).transpose(0, 1).contiguous()\n",
        "        # decouple frames per step\n",
        "        mel_outputs = mel_outputs.view(\n",
        "            mel_outputs.size(0), -1, hparams.vocab_size)\n",
        "        # (B, T_out, n_mel_channels) -> (B, n_mel_channels, T_out)\n",
        "        mel_outputs = mel_outputs.transpose(1, 2)\n",
        "\n",
        "        return mel_outputs, gate_outputs, alignments\n",
        "\n",
        "    def decode(self, decoder_input):\n",
        "        \"\"\" Decoder step using stored states, attention and memory\n",
        "        PARAMS\n",
        "        ------\n",
        "        decoder_input: previous mel output\n",
        "\n",
        "        RETURNS\n",
        "        -------\n",
        "        mel_output:\n",
        "        gate_output: gate output energies\n",
        "        attention_weights:\n",
        "        \"\"\"\n",
        "        cell_input = torch.cat((decoder_input, self.attention_context), -1)\n",
        "        self.attention_hidden, self.attention_cell = self.attention_rnn(\n",
        "            cell_input, (self.attention_hidden, self.attention_cell))\n",
        "        self.attention_hidden = F.dropout(\n",
        "            self.attention_hidden, self.p_attention_dropout, self.training)\n",
        "\n",
        "        attention_weights_cat = torch.cat(\n",
        "            (self.attention_weights.unsqueeze(1),\n",
        "              self.attention_weights_cum.unsqueeze(1)), dim=1)\n",
        "        self.attention_context, self.attention_weights = self.attention_layer(\n",
        "            self.attention_hidden, self.memory, self.processed_memory,\n",
        "            attention_weights_cat, self.mask)\n",
        "\n",
        "        # print(self.attention_context.shape,self.attention_weights.shape)\n",
        "        self.attention_weights_cum += self.attention_weights\n",
        "        decoder_input = torch.cat(\n",
        "            (self.attention_hidden, self.attention_context), -1)\n",
        "        self.decoder_hidden, self.decoder_cell = self.decoder_rnn(\n",
        "            decoder_input, (self.decoder_hidden, self.decoder_cell))\n",
        "        self.decoder_hidden = F.dropout(\n",
        "            self.decoder_hidden, self.p_decoder_dropout, self.training)\n",
        "\n",
        "        decoder_hidden_attention_context = torch.cat(\n",
        "            (self.decoder_hidden, self.attention_context), dim=1)\n",
        "        decoder_output = self.linear_projection(\n",
        "            decoder_hidden_attention_context)\n",
        "\n",
        "        gate_prediction = self.gate_layer(decoder_hidden_attention_context)\n",
        "        return decoder_output, gate_prediction, self.attention_weights\n",
        "\n",
        "    def forward(self, memory,decoder_inputs,memory_lengths,output_lengths):\n",
        "        \"\"\" Decoder forward pass for training\n",
        "        PARAMS\n",
        "        ------\n",
        "        memory: Encoder outputs\n",
        "        decoder_inputs: Decoder inputs for teacher forcing. i.e. mel-specs\n",
        "        memory_lengths: Encoder output lengths for attention masking.\n",
        "\n",
        "        RETURNS\n",
        "        -------\n",
        "        mel_outputs: mel outputs from the decoder\n",
        "        gate_outputs: gate outputs from the decoder\n",
        "        alignments: sequence of attention weights from the decoder\n",
        "        \"\"\"\n",
        "        # print(memory.shape)\n",
        "        decoder_input = self.get_go_frame(memory).unsqueeze(0)\n",
        "        decoder_inputs = self.parse_decoder_inputs(decoder_inputs)\n",
        "        decoder_inputs = torch.cat((decoder_input, decoder_inputs), dim=0)\n",
        "        decoder_inputs = self.prenet(decoder_inputs)\n",
        "        self.initialize_decoder_states(\n",
        "            memory, mask=~get_mask_from_lengths(memory_lengths))\n",
        "        # print(decoder_inputs.shape)\n",
        "        mel_outputs, gate_outputs, alignments = [], [], []\n",
        "        while len(mel_outputs) < decoder_inputs.size(0) - 1:\n",
        "            decoder_input = decoder_inputs[len(mel_outputs)]\n",
        "            mel_output, gate_output, attention_weights = self.decode(\n",
        "                decoder_input)\n",
        "            mel_outputs += [mel_output.squeeze(1)]\n",
        "            gate_outputs += [gate_output.squeeze(1)]\n",
        "            alignments += [attention_weights]\n",
        "\n",
        "        mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
        "            mel_outputs, gate_outputs, alignments)\n",
        "\n",
        "        # print(mel_outputs.shape)\n",
        "        return mel_outputs, gate_outputs, alignments\n",
        "\n",
        "    def inference(self, memory):\n",
        "        \"\"\" Decoder inference\n",
        "        PARAMS\n",
        "        ------\n",
        "        memory: Encoder outputs\n",
        "\n",
        "        RETURNS\n",
        "        -------\n",
        "        mel_outputs: mel outputs from the decoder\n",
        "        gate_outputs: gate outputs from the decoder\n",
        "        alignments: sequence of attention weights from the decoder\n",
        "        \"\"\"\n",
        "        decoder_input = self.get_go_frame(memory)\n",
        "\n",
        "        self.initialize_decoder_states(memory, mask=None)\n",
        "\n",
        "        mel_outputs, gate_outputs, alignments = [], [], []\n",
        "        while True:\n",
        "            decoder_input = self.prenet(decoder_input)\n",
        "            mel_output, gate_output, alignment = self.decode(decoder_input)\n",
        "\n",
        "            mel_outputs += [mel_output.squeeze(1)]\n",
        "            gate_outputs += [gate_output]\n",
        "            alignments += [alignment]\n",
        "\n",
        "            if torch.sigmoid(gate_output.data) > self.gate_threshold:\n",
        "                break\n",
        "            elif len(mel_outputs) == self.max_decoder_steps:\n",
        "                print(\"Warning! Reached max decoder steps\")\n",
        "                break\n",
        "\n",
        "            decoder_input = mel_output\n",
        "\n",
        "        mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
        "            mel_outputs, gate_outputs, alignments)\n",
        "\n",
        "        return mel_outputs, gate_outputs, alignments\n",
        "\n",
        "\n",
        "class AuxiliaryPhonemesModel(nn.Module, PyTorchModelHubMixin):\n",
        "    def __init__(self, hparams):\n",
        "        super(AuxiliaryPhonemesModel, self).__init__()\n",
        "        self.mask_padding = hparams.mask_padding\n",
        "        self.fp16_run = hparams.fp16_run\n",
        "        self.n_mel_channels = hparams.n_mel_channels\n",
        "        self.n_frames_per_step = hparams.n_frames_per_step\n",
        "        # self.embedding = nn.Embedding(\n",
        "        #     hparams.n_symbols, hparams.symbols_embedding_dim)\n",
        "        self.linear = nn.Linear(hparams.n_mel_channels,hparams.encoder_embedding_dim)\n",
        "        # std = sqrt(2.0 / (hparams.n_symbols + hparams.symbols_embedding_dim))\n",
        "        # val = sqrt(3.0) * std  # uniform bounds for std\n",
        "        # self.embedding.weight.data.uniform_(-val, val)\n",
        "        self.encoder = Encoder(hparams)\n",
        "        self.decoder = PhonemesDecoder(hparams)\n",
        "        self.postnet = Postnet(hparams)\n",
        "\n",
        "    def parse_batch(self, batch):\n",
        "        # # text_padded, input_lengths, mel_padded, gate_padded, \\\n",
        "        # #     output_lengths = batch\n",
        "        # mels, gate,input_lengths, spec, output_lengths = batch\n",
        "        # mels = to_gpu(mels).float()\n",
        "        # gate = to_gpu(gate).float()\n",
        "        # input_lengths = to_gpu(input_lengths).long()\n",
        "        # inp_len = torch.max(input_lengths.data).item()\n",
        "        # spec = to_gpu(spec).float()\n",
        "        # out_lengths = to_gpu(out_lenghts).long()\n",
        "        # out_len = torch.max(out_lengths).item()\n",
        "        # # gate_padded = to_gpu(gate_padded).float()\n",
        "        # # output_lengths = to_gpu(output_lengths).long()\n",
        "\n",
        "        # return (\n",
        "        #     (mels,gate,input_lengths,max_len,spec,output_lengths,out_len),\n",
        "        #     (spec,output_lengths,out_len))\n",
        "        input_padded, input_lengths, mel_padded, gate_padded, \\\n",
        "            output_lengths = batch\n",
        "        input_padded = to_gpu(input_padded).float()\n",
        "        input_lengths = to_gpu(input_lengths).long()\n",
        "        max_len = torch.max(input_lengths.data).item()\n",
        "        mel_padded = to_gpu(mel_padded).float()\n",
        "        gate_padded = to_gpu(gate_padded).float()\n",
        "        output_lengths = to_gpu(output_lengths).long()\n",
        "\n",
        "        return (\n",
        "            (input_padded, input_lengths, mel_padded, max_len, output_lengths),\n",
        "            (mel_padded, gate_padded))\n",
        "\n",
        "    def parse_output(self, outputs, output_lengths=None):\n",
        "        if self.mask_padding and output_lengths is not None:\n",
        "            mask = ~get_mask_from_lengths(output_lengths)\n",
        "            mask = mask.expand(hparams.vocab_size, mask.size(0), mask.size(1))\n",
        "            mask = mask.permute(1, 0, 2)\n",
        "\n",
        "            outputs[0].data.masked_fill_(mask, 0.0)\n",
        "            outputs[1].data.masked_fill_(mask, 0.0)\n",
        "            outputs[2].data.masked_fill_(mask[:, 0, :], 1e3)  # gate energies\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # mels,gate,input_lengths,max_len,spec,output_lengths,out_len = inputs\n",
        "        # mels, input_lengths = mels.data, input_lengths.data\n",
        "\n",
        "        # # embedded_inputs = self.embedding(text_inputs).transpose(1, 2)\n",
        "\n",
        "        # encoder_outputs = self.encoder(mels, input_lengths)\n",
        "\n",
        "        # mel_outputs, gate_outputs, alignments = self.decoder(\n",
        "        #     encoder_outputs, spec,input_lengths,output_lengths)\n",
        "\n",
        "        # mel_outputs_postnet = self.postnet(mel_outputs)\n",
        "        # mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
        "\n",
        "        # return self.parse_output(\n",
        "        #     [mel_outputs, mel_outputs_postnet, gate_outputs, alignments],\n",
        "        #     output_lengths)\n",
        "        inputs, input_lengths, mels, max_len, output_lengths = inputs\n",
        "        input_lengths, output_lengths = input_lengths.data, output_lengths.data\n",
        "\n",
        "        embedded_inputs = self.linear(inputs.transpose(1,2)).transpose(1, 2)\n",
        "\n",
        "        encoder_outputs = self.encoder(embedded_inputs, input_lengths)\n",
        "        # print(encoder_outputs.shape)\n",
        "        mel_outputs, gate_outputs, alignments = self.decoder(\n",
        "            encoder_outputs, mels, input_lengths,output_lengths)\n",
        "\n",
        "        mel_outputs_postnet = self.postnet(mel_outputs)\n",
        "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
        "\n",
        "        return mel_outputs, mel_outputs_postnet, gate_outputs, alignments\n",
        "\n",
        "    def inference(self, inputs):\n",
        "        # embedded_inputs = self.embedding(inputs).transpose(1, 2)\n",
        "        encoder_outputs = self.encoder.inference(inputs)\n",
        "        mel_outputs, gate_outputs, alignments = self.decoder.inference(\n",
        "            encoder_outputs)\n",
        "\n",
        "        mel_outputs_postnet = self.postnet(mel_outputs)\n",
        "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
        "\n",
        "        outputs = self.parse_output(\n",
        "            [mel_outputs, mel_outputs_postnet, gate_outputs, alignments])\n",
        "\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "OrJw5XUp5QqI"
      },
      "outputs": [],
      "source": [
        "## loss_function.py\n",
        "class Tacotron2Loss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Tacotron2Loss, self).__init__()\n",
        "\n",
        "    def forward(self, model_output, targets):\n",
        "        phoneme_target, gate_target = targets[0], targets[1]\n",
        "        mel_out, mel_out_postnet, gate_out, _ = model_output\n",
        "\n",
        "        gate_loss = nn.BCEWithLogitsLoss()(gate_out, gate_target)\n",
        "\n",
        "        mel_out_indices = torch.argmax(mel_out, dim=2)\n",
        "\n",
        "        mel_out_postnet_indices = torch.argmax(mel_out_postnet, dim=2)\n",
        "\n",
        "        phoneme_target_indices = torch.argmax(mel_out, dim=2)\n",
        "\n",
        "        mel_loss = F.cross_entropy(mel_out, phoneme_target) + F.cross_entropy(mel_out_postnet, phoneme_target)\n",
        "        return mel_loss + gate_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "e_Wib6djkGYZ"
      },
      "outputs": [],
      "source": [
        "## loss_scaler.py\n",
        "\n",
        "import torch\n",
        "\n",
        "class LossScaler:\n",
        "\n",
        "    def __init__(self, scale=1):\n",
        "        self.cur_scale = scale\n",
        "\n",
        "    # `params` is a list / generator of torch.Variable\n",
        "    def has_overflow(self, params):\n",
        "        return False\n",
        "\n",
        "    # `x` is a torch.Tensor\n",
        "    def _has_inf_or_nan(x):\n",
        "        return False\n",
        "\n",
        "    # `overflow` is boolean indicating whether we overflowed in gradient\n",
        "    def update_scale(self, overflow):\n",
        "        pass\n",
        "\n",
        "    @property\n",
        "    def loss_scale(self):\n",
        "        return self.cur_scale\n",
        "\n",
        "    def scale_gradient(self, module, grad_in, grad_out):\n",
        "        return tuple(self.loss_scale * g for g in grad_in)\n",
        "\n",
        "    def backward(self, loss):\n",
        "        scaled_loss = loss*self.loss_scale\n",
        "        scaled_loss.backward()\n",
        "\n",
        "class DynamicLossScaler:\n",
        "\n",
        "    def __init__(self,\n",
        "                 init_scale=2**32,\n",
        "                 scale_factor=2.,\n",
        "                 scale_window=1000):\n",
        "        self.cur_scale = init_scale\n",
        "        self.cur_iter = 0\n",
        "        self.last_overflow_iter = -1\n",
        "        self.scale_factor = scale_factor\n",
        "        self.scale_window = scale_window\n",
        "\n",
        "    # `params` is a list / generator of torch.Variable\n",
        "    def has_overflow(self, params):\n",
        "#        return False\n",
        "        for p in params:\n",
        "            if p.grad is not None and DynamicLossScaler._has_inf_or_nan(p.grad.data):\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    # `x` is a torch.Tensor\n",
        "    def _has_inf_or_nan(x):\n",
        "        cpu_sum = float(x.float().sum())\n",
        "        if cpu_sum == float('inf') or cpu_sum == -float('inf') or cpu_sum != cpu_sum:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    # `overflow` is boolean indicating whether we overflowed in gradient\n",
        "    def update_scale(self, overflow):\n",
        "        if overflow:\n",
        "            #self.cur_scale /= self.scale_factor\n",
        "            self.cur_scale = max(self.cur_scale/self.scale_factor, 1)\n",
        "            self.last_overflow_iter = self.cur_iter\n",
        "        else:\n",
        "            if (self.cur_iter - self.last_overflow_iter) % self.scale_window == 0:\n",
        "                self.cur_scale *= self.scale_factor\n",
        "#        self.cur_scale = 1\n",
        "        self.cur_iter += 1\n",
        "\n",
        "    @property\n",
        "    def loss_scale(self):\n",
        "        return self.cur_scale\n",
        "\n",
        "    def scale_gradient(self, module, grad_in, grad_out):\n",
        "        return tuple(self.loss_scale * g for g in grad_in)\n",
        "\n",
        "    def backward(self, loss):\n",
        "        scaled_loss = loss*self.loss_scale\n",
        "        scaled_loss.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_eMEbm9Bgau"
      },
      "source": [
        "##### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "OyRa7iFGkJ_a"
      },
      "outputs": [],
      "source": [
        "model = AuxiliaryPhonemesModel(hparams).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "LrNM-0beisTz"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_loader = DataLoader(train_dataset, num_workers=2, shuffle=True,\n",
        "                          batch_size=hparams.batch_size, pin_memory=False,\n",
        "                          drop_last=True, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "sEU13qCSZw8w"
      },
      "outputs": [],
      "source": [
        "spec_to_emb_linear = model.linear\n",
        "encoder = model.encoder\n",
        "decoder = model.decoder\n",
        "postnet = model.postnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "6hwHpATuhlN4"
      },
      "outputs": [],
      "source": [
        "def get_go_frame(memory):\n",
        "    \"\"\" Gets all zeros frames to use as first decoder input\n",
        "    PARAMS\n",
        "    ------\n",
        "    memory: decoder outputs\n",
        "\n",
        "    RETURNS\n",
        "    -------\n",
        "    decoder_input: all zeros frames\n",
        "    \"\"\"\n",
        "    B = memory.size(0)\n",
        "    decoder_input = Variable(memory.data.new(\n",
        "        B, hparams.vocab_size).zero_())\n",
        "    return decoder_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ggdULYOGpHIr"
      },
      "outputs": [],
      "source": [
        "def parse_decoder_inputs(self, decoder_inputs):\n",
        "    \"\"\" Prepares decoder inputs, i.e. mel outputs\n",
        "    PARAMS\n",
        "    ------\n",
        "    decoder_inputs: inputs used for teacher-forced training, i.e. mel-specs\n",
        "\n",
        "    RETURNS\n",
        "    -------\n",
        "    inputs: processed decoder inputs\n",
        "\n",
        "    \"\"\"\n",
        "    # (B, n_mel_channels, T_out) -> (B, T_out, n_mel_channels)\n",
        "    decoder_inputs = decoder_inputs.transpose(1, 2)\n",
        "    decoder_inputs = decoder_inputs.view(\n",
        "        decoder_inputs.size(0),\n",
        "        int(decoder_inputs.size(1)/self.n_frames_per_step), -1)\n",
        "    # (B, T_out, n_mel_channels) -> (T_out, B, n_mel_channels)\n",
        "    decoder_inputs = decoder_inputs.transpose(0, 1)\n",
        "    return decoder_inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "iOKaSpkvpcob"
      },
      "outputs": [],
      "source": [
        "prenet = Prenet(\n",
        "    hparams.vocab_size,\n",
        "    [hparams.prenet_dim, hparams.prenet_dim])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "KitQPlEf1IVg"
      },
      "outputs": [],
      "source": [
        "def parse_decoder_inputs(decoder_inputs):\n",
        "    \"\"\" Prepares decoder inputs, i.e. mel outputs\n",
        "    PARAMS\n",
        "    ------\n",
        "    decoder_inputs: inputs used for teacher-forced training, i.e. mel-specs\n",
        "\n",
        "    RETURNS\n",
        "    -------\n",
        "    inputs: processed decoder inputs\n",
        "\n",
        "    \"\"\"\n",
        "    # (B, n_mel_channels, T_out) -> (B, T_out, n_mel_channels)\n",
        "    decoder_inputs = decoder_inputs.transpose(1, 2)\n",
        "    decoder_inputs = decoder_inputs.view(\n",
        "        decoder_inputs.size(0),\n",
        "        int(decoder_inputs.size(1)/hparams.n_frames_per_step), -1)\n",
        "    # (B, T_out, n_mel_channels) -> (T_out, B, n_mel_channels)\n",
        "    decoder_inputs = decoder_inputs.transpose(0, 1)\n",
        "    return decoder_inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "k9DgSAa_ALQf"
      },
      "outputs": [],
      "source": [
        "criterion = Tacotron2Loss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrurCqazZClC",
        "outputId": "18bff0a4-1cac-427f-b6a1-6611ee14bafc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs Shape:  torch.Size([32, 80, 1007])\n",
            "Encoder Outputs Shape:  torch.Size([32, 1007, 128])\n",
            "Decoder Input Array:  tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0') torch.Size([1, 32, 74])\n",
            "Output Shape before parsing:  torch.Size([32, 74, 193])\n",
            "Output Shape after parsing:  torch.Size([194, 32, 74])\n",
            "6.428123950958252\n"
          ]
        }
      ],
      "source": [
        "for i, batch in enumerate(train_loader):\n",
        "  inputs, input_lengths, target, gate_padded, output_lengths = batch\n",
        "\n",
        "  print(\"Inputs Shape: \", inputs.shape)\n",
        "  inputs = inputs.cuda()\n",
        "  target = target.cuda()\n",
        "\n",
        "  input_lengths, output_lengths = input_lengths.data, output_lengths.data\n",
        "\n",
        "  embedded_inputs = spec_to_emb_linear(inputs.transpose(1,2)).transpose(1, 2)\n",
        "\n",
        "  encoder_outputs = encoder(embedded_inputs.cuda(), input_lengths)\n",
        "\n",
        "  print(\"Encoder Outputs Shape: \", encoder_outputs.shape)\n",
        "  ## get_go_frame() -- get the zeros values of encoder in\n",
        "  decoder_input = get_go_frame(encoder_outputs).unsqueeze(0)\n",
        "\n",
        "  print(\"Decoder Input Array: \", decoder_input, decoder_input.shape)\n",
        "\n",
        "  print(\"Output Shape before parsing: \", target.shape)\n",
        "\n",
        "  decoder_inputs = parse_decoder_inputs(target)\n",
        "  decoder_inputs = torch.cat((decoder_input.cuda(), decoder_inputs.cuda()), dim=0)\n",
        "\n",
        "  print(\"Output Shape after parsing: \", decoder_inputs.shape)\n",
        "\n",
        "  mel_outputs, gate_outputs, alignments = decoder(\n",
        "      encoder_outputs, target, input_lengths.cuda(),output_lengths.cuda())\n",
        "\n",
        "  mel_outputs_postnet = postnet(mel_outputs)\n",
        "\n",
        "\n",
        "  loss = criterion((mel_outputs, mel_outputs_postnet, gate_outputs, alignments), (target, gate_padded.cuda()))\n",
        "\n",
        "  print(loss.item())\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73rnqapD-h5Z",
        "outputId": "1bd1c6fc-3369-4ee1-a7fd-bcf28af6f1ef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 73])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "target_indices = torch.randint(0, 102, (1, 73))\n",
        "target_indices.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTTclH3H911j",
        "outputId": "6e783723-8f80-4efd-c7a8-218c020a9ad7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 74, 193]), torch.Size([32, 74, 193]))"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "mel_outputs.shape, target.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJDmAI3C-LW1",
        "outputId": "e5e3ed23-4c3d-4060-b17a-5581fca5fabb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.5222299098968506"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "loss = nn.CrossEntropyLoss()(mel_outputs, target)\n",
        "loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUjbGl9P4ll0",
        "outputId": "130adf58-0ee4-4578-8b87-8c74872c418c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 74, 193])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
        "mel_outputs_postnet.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mp23JBK84OAk",
        "outputId": "1be9e3e6-869b-40e2-8ea1-31b1ac637225"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 74, 193]),\n",
              " tensor([[-0.2074, -0.2070, -0.2115,  ..., -0.2887, -0.2358, -0.2514],\n",
              "         [-0.2025, -0.1904, -0.1847,  ..., -0.2187, -0.2090, -0.1847],\n",
              "         [-0.0672, -0.0562, -0.0922,  ..., -0.0850, -0.0973, -0.0838],\n",
              "         ...,\n",
              "         [-0.0374, -0.0395, -0.0453,  ..., -0.0460, -0.0523, -0.0576],\n",
              "         [-0.0320, -0.0271, -0.0257,  ..., -0.0235, -0.0549, -0.0681],\n",
              "         [-0.0116, -0.0058, -0.0360,  ..., -0.0197, -0.0101, -0.0180]],\n",
              "        device='cuda:0', grad_fn=<CloneBackward0>),\n",
              " torch.Size([32, 193, 1007]),\n",
              " torch.Size([32, 74, 193]))"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "mel_outputs.shape, gate_outputs, alignments.shape, mel_outputs_postnet.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w99-N1TdBmHh"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "VrJLkhsqCEDB"
      },
      "outputs": [],
      "source": [
        "model = AuxiliaryPhonemesModel(hparams).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "OKkG6psOB2fP"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, num_workers=2, shuffle=True,\n",
        "                          batch_size=hparams.batch_size, pin_memory=False,\n",
        "                          drop_last=True, collate_fn=collate_fn)\n",
        "\n",
        "valid_loader = DataLoader(val_dataset, num_workers=2, shuffle=True,\n",
        "                          batch_size=hparams.batch_size, pin_memory=False,\n",
        "                          drop_last=True, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "qvXoVBFKCHGz"
      },
      "outputs": [],
      "source": [
        "criterion = Tacotron2Loss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "JiORJ1D1CvJ5"
      },
      "outputs": [],
      "source": [
        "learning_rate = hparams.learning_rate\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
        "                              weight_decay=hparams.weight_decay)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Blz2WNZUC1KX",
        "outputId": "354e20c0-5d5f-453e-efd2-f13c1bbd8083"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AuxiliaryPhonemesModel(\n",
              "  (linear): Linear(in_features=80, out_features=128, bias=True)\n",
              "  (encoder): Encoder(\n",
              "    (lstm): LSTM(128, 64, batch_first=True, bidirectional=True)\n",
              "  )\n",
              "  (decoder): PhonemesDecoder(\n",
              "    (prenet): Prenet(\n",
              "      (layers): ModuleList(\n",
              "        (0): LinearNorm(\n",
              "          (linear_layer): Linear(in_features=74, out_features=32, bias=False)\n",
              "        )\n",
              "        (1): LinearNorm(\n",
              "          (linear_layer): Linear(in_features=32, out_features=32, bias=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (attention_rnn): LSTMCell(160, 256)\n",
              "    (attention_layer): Attention(\n",
              "      (query_layer): LinearNorm(\n",
              "        (linear_layer): Linear(in_features=256, out_features=128, bias=False)\n",
              "      )\n",
              "      (memory_layer): LinearNorm(\n",
              "        (linear_layer): Linear(in_features=128, out_features=128, bias=False)\n",
              "      )\n",
              "      (attention): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (location_layer): LocationLayer(\n",
              "        (location_conv): ConvNorm(\n",
              "          (conv): Conv1d(2, 32, kernel_size=(31,), stride=(1,), padding=(15,), bias=False)\n",
              "        )\n",
              "        (location_dense): LinearNorm(\n",
              "          (linear_layer): Linear(in_features=32, out_features=128, bias=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (decoder_rnn): LSTMCell(384, 256, bias=1)\n",
              "    (linear_projection): LinearNorm(\n",
              "      (linear_layer): Linear(in_features=384, out_features=74, bias=True)\n",
              "    )\n",
              "    (gate_layer): LinearNorm(\n",
              "      (linear_layer): Linear(in_features=384, out_features=1, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (postnet): Postnet(\n",
              "    (convolutions): ModuleList(\n",
              "      (0): Sequential(\n",
              "        (0): ConvNorm(\n",
              "          (conv): Conv1d(74, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "        )\n",
              "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): Sequential(\n",
              "        (0): ConvNorm(\n",
              "          (conv): Conv1d(128, 74, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "        )\n",
              "        (1): BatchNorm1d(74, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "model.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSzwUbYE4coJ"
      },
      "outputs": [],
      "source": [
        "# #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\n",
        "# !huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "JkwBTTnl4ebY"
      },
      "outputs": [],
      "source": [
        "def train(model, train_dataloader, valid_dataloader, save_path='/content/drive/MyDrive/Dubbing Project/models'):\n",
        "\n",
        "  criterion = Tacotron2Loss()\n",
        "\n",
        "  learning_rate = hparams.learning_rate\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
        "                                weight_decay=hparams.weight_decay)\n",
        "\n",
        "  best_valid_loss = float('inf')\n",
        "\n",
        "  wandb.init(\n",
        "      project=\"S2ST\",\n",
        "      config={\n",
        "        \"learning_rate\": hparams.learning_rate,\n",
        "        \"architecture\": \"Translatotron\",\n",
        "        \"dataset\": \"Librspeech 100 Hours of data\",\n",
        "        \"epochs\": hparams.epochs,\n",
        "        'batch_size': hparams.batch_size,\n",
        "        \"description\": f\"translatoron first run\",\n",
        "      }\n",
        "  )\n",
        "\n",
        "  for epoch in range(hparams.epochs):\n",
        "      # Training\n",
        "      model.train()\n",
        "      total_loss = 0.0\n",
        "\n",
        "      with tqdm(train_dataloader, desc=f'Training Epoch {epoch + 1}/{hparams.epochs}', unit='batch') as t:\n",
        "        for i, batch in enumerate(t):\n",
        "            inputs, input_lengths, target, gate_padded, output_lengths = batch\n",
        "\n",
        "            batch = inputs.cuda(), input_lengths.cuda(), target.cuda(), gate_padded.cuda(), output_lengths.cuda()\n",
        "\n",
        "            mel_outputs, mel_outputs_postnet, gate_outputs, alignments = model(batch)\n",
        "\n",
        "            loss = criterion((mel_outputs, mel_outputs_postnet, gate_outputs, alignments), (target.cuda(), gate_padded.cuda()))\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            t.set_postfix(loss=loss.item(), total_loss=total_loss / (i + 1))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            wandb.log({\"train_loss\": total_loss / (i + 1)})\n",
        "\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "      # Validation\n",
        "      model.eval()\n",
        "      total_valid_loss = 0.0\n",
        "\n",
        "      gc.collect()\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "      with torch.no_grad():\n",
        "          with tqdm(valid_dataloader, desc=f'Validation Epoch {epoch + 1}/{hparams.epochs}', unit='batch') as v:\n",
        "            for i, batch in enumerate(v):\n",
        "                inputs, input_lengths, target, gate_padded, output_lengths = batch\n",
        "\n",
        "                batch = inputs.cuda(), input_lengths.cuda(), target.cuda(), gate_padded.cuda(), output_lengths.cuda()\n",
        "\n",
        "                mel_outputs, mel_outputs_postnet, gate_outputs, alignments = model(batch)\n",
        "\n",
        "                loss = criterion((mel_outputs, mel_outputs_postnet, gate_outputs, alignments), (target.cuda(), gate_padded.cuda()))\n",
        "                v.set_postfix(loss=loss.item(), total_loss=total_valid_loss / (i + 1))\n",
        "\n",
        "                total_valid_loss += loss.item()\n",
        "                wandb.log({\"valid_loss\": total_valid_loss / (i + 1)})\n",
        "\n",
        "                gc.collect()\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "      # Save the model\n",
        "      save_filename = os.path.join(save_path, f'phoneme_predictor_{epoch + 1}.pt')\n",
        "      torch.save(model.state_dict(), save_filename)\n",
        "\n",
        "      gc.collect()\n",
        "      torch.cuda.empty_cache()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "8jcr-slf_Wav"
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNUGc8TE6vr9"
      },
      "outputs": [],
      "source": [
        "# save_path = '/content/drive/MyDrive/Dubbing Project/models'\n",
        "\n",
        "# ## f42e9dfb9ecc6347595dd9aa95ce1ce04e08004d\n",
        "# train(model, train_loader, valid_loader, save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6pi8d7533OG"
      },
      "outputs": [],
      "source": [
        "save_filename = os.path.join(save_path, f'final_phoneme_predictor.pt')\n",
        "torch.save(model.state_dict(), save_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzNfYryKPjj2"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6KajgpqrED_"
      },
      "outputs": [],
      "source": [
        "mel_outputs_postnet.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmWM5USJqWCC"
      },
      "outputs": [],
      "source": [
        "mel_out_postnet_indices = torch.argmax(mel_outputs_postnet, dim=1)\n",
        "\n",
        "phoneme_target_indices = torch.argmax(target, dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gc_zJ34qjS3"
      },
      "outputs": [],
      "source": [
        "mel_out_postnet_indices, phoneme_target_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQb0scZ-1go_"
      },
      "outputs": [],
      "source": [
        "# for i, batch in enumerate(train_loader):\n",
        "#   inputs, input_lengths, target, gate_padded, output_lengths = batch\n",
        "\n",
        "#   batch = inputs.cuda(), input_lengths.cuda(), target.cuda(), gate_padded.cuda(), output_lengths.cuda()\n",
        "\n",
        "#   mel_outputs, mel_outputs_postnet, gate_outputs, alignments = model(batch)\n",
        "\n",
        "#   loss = criterion((mel_outputs, mel_outputs_postnet, gate_outputs, alignments), (target.cuda(), gate_padded.cuda()))\n",
        "\n",
        "#   if i % 10 == 0:\n",
        "#     print(\"Loss is: \", loss.item())\n",
        "\n",
        "#   loss.backward()\n",
        "\n",
        "#   optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "NNjvkQ7227j3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path ='/content/drive/MyDrive/Dubbing Project/models/model_start_end/phoneme_predictor_9.pt'"
      ],
      "metadata": {
        "id": "rOr1iriJ29XV"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "fSDUjezjC0l4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3449a8c6-756e-4964-c237-c7fd9ba76a60"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(model_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "k0GmvODy1g98"
      },
      "outputs": [],
      "source": [
        "for i, batch in enumerate(valid_dataloader):\n",
        "    inputs, input_lengths, target, gate_padded, output_lengths = batch\n",
        "\n",
        "    batch = inputs.cuda(), input_lengths.cuda(), target.cuda(), gate_padded.cuda(), output_lengths.cuda()\n",
        "\n",
        "    mel_outputs, mel_outputs_postnet, gate_outputs, alignments = model(batch)\n",
        "\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_lengths"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XSUJLOrTQCK",
        "outputId": "56db183f-1df0-4699-8b6d-4024845728a6"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([74, 74, 74])"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mel_out_postnet_indices = torch.argmax(mel_outputs_postnet, dim=1)\n",
        "\n",
        "phoneme_target_indices = torch.argmax(target, dim=1)\n",
        "\n",
        "binary_tensor = torch.where(gate_outputs > 0, torch.tensor(1), torch.tensor(0))"
      ],
      "metadata": {
        "id": "eEA6wsFB2XFJ"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_phoneme"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZwByG9kUnJx",
        "outputId": "3c449524-9bd9-4ca4-ca49-0edcfc57a08e"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'0': 'AA2',\n",
              " '1': 'CH',\n",
              " '2': 'ZH',\n",
              " '3': 'DH',\n",
              " '4': 'UW0',\n",
              " '5': 'W',\n",
              " '6': 'IY0',\n",
              " '7': 'OY0',\n",
              " '8': 'UH2',\n",
              " '9': 'D',\n",
              " '10': 'Y',\n",
              " '11': 'Z',\n",
              " '12': 'OY2',\n",
              " '13': 'HH',\n",
              " '14': 'AY2',\n",
              " '15': 'IH0',\n",
              " '16': 'EH1',\n",
              " '17': 'UW2',\n",
              " '18': 'UH0',\n",
              " '19': 'JH',\n",
              " '20': 'ER2',\n",
              " '21': 'V',\n",
              " '22': 'IY2',\n",
              " '23': 'G',\n",
              " '24': 'M',\n",
              " '25': 'AH1',\n",
              " '26': 'irish',\n",
              " '27': 'AA0',\n",
              " '28': 'AY0',\n",
              " '29': 'OW1',\n",
              " '30': 'ER1',\n",
              " '31': 'AO0',\n",
              " '32': 'EY1',\n",
              " '33': 'L',\n",
              " '34': 'AA1',\n",
              " '35': 'AY1',\n",
              " '36': 'SH',\n",
              " '37': 'ER0',\n",
              " '38': 'AH0',\n",
              " '39': 'UH1',\n",
              " '40': 'K',\n",
              " '41': 'S',\n",
              " '42': 'AH2',\n",
              " '43': 'AO2',\n",
              " '44': 'IY1',\n",
              " '45': 'OW0',\n",
              " '46': 'T',\n",
              " '47': 'IH1',\n",
              " '48': 'AW2',\n",
              " '49': 'EH0',\n",
              " '50': 'org,',\n",
              " '51': 'AO1',\n",
              " '52': 'NG',\n",
              " '53': 'EH2',\n",
              " '54': 'R',\n",
              " '55': '#',\n",
              " '56': 'EY0',\n",
              " '57': 'AW0',\n",
              " '58': 'AE2',\n",
              " '59': 'AE1',\n",
              " '60': 'OW2',\n",
              " '61': 'N',\n",
              " '62': 'B',\n",
              " '63': 'AE0',\n",
              " '64': 'F',\n",
              " '65': 'AW1',\n",
              " '66': 'P',\n",
              " '67': 'TH',\n",
              " '68': 'EY2',\n",
              " '69': 'IH2',\n",
              " '70': 'UW1',\n",
              " '71': 'OY1',\n",
              " 72: '<end/>',\n",
              " 73: '<start/>'}"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_reverse_phoneme(ph_list):\n",
        "  return [reverse_phoneme[str(i)] for i in ph_list]"
      ],
      "metadata": {
        "id": "zgaYQIuNSo2m"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 0\n",
        "\n",
        "predicted_index = (binary_tensor[index] == 1).nonzero().max()\n",
        "target_index = (gate_padded[index] == 1).nonzero().max()\n",
        "\n",
        "\n",
        "target_index, predicted_index"
      ],
      "metadata": {
        "id": "m6d1HxA95CnW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b05d6b2d-616c-4587-ae1c-2cda1dbb8202"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(149), tensor(73, device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mel_out_postnet_indices[index]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZ68lw723EI7",
        "outputId": "05ba3ab2-54c7-4408-8bed-95b00b2d7bce"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([73, 59, 61,  9, 35,  5, 38, 33, 46, 44, 24, 38, 46, 41, 16, 54, 15, 11,\n",
              "        61, 34, 46, 39, 33, 65, 52, 38, 62, 65, 46,  3, 25, 54, 71, 11, 37, 11,\n",
              "        62, 38, 46,  3, 25, 41, 30, 33, 11, 35, 54, 38, 72, 72, 61, 72, 24, 61,\n",
              "        24, 33, 33, 33, 33, 46, 41, 40, 33, 24, 33, 33, 11, 46, 72, 33, 40, 33,\n",
              "        24,  5, 61, 41, 72, 24, 41, 46, 46, 40, 41, 33, 24, 46, 33, 61],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_reverse_phoneme(mel_out_postnet_indices[index].detach().cpu().tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hW0t_b-Ubyy",
        "outputId": "35b62c0c-6fe3-41d6-b9f7-e23f5efb939d"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<start/>', 'AE1', 'N', 'D', 'AY1', 'W', 'AH0', 'L', 'T', 'IY1', 'M', 'AH0', 'T', 'S', 'EH1', 'R', 'IH0', 'Z', 'N', 'AA1', 'T', 'UH1', 'L', 'AW1', 'NG', 'AH0', 'B', 'AW1', 'T', 'DH', 'AH1', 'R', 'OY1', 'Z', 'ER0', 'Z', 'B', 'AH0', 'T', 'DH', 'AH1', 'S', 'ER1', 'L', 'Z', 'AY1', 'R', 'AH0', '<end/>', '<end/>', 'N', '<end/>', 'M', 'N', 'M', 'L', 'L', 'L', 'L', 'T', 'S', 'K', 'L', 'M', 'L', 'L', 'Z', 'T', '<end/>', 'L', 'K', 'L', 'M', 'W', 'N', 'S', '<end/>', 'M', 'S', 'T', 'T', 'K', 'S', 'L', 'M', 'T', 'L', 'N']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "phoneme_target_indices[index][:target_index]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoiESdRd3aD0",
        "outputId": "0ad1c06a-41c8-43d3-c0b7-ca162bc673c6"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([73, 59, 61,  9, 35,  5, 38, 33, 41, 44,  3, 38, 46,  3, 16, 54, 15, 11,\n",
              "        61, 29, 36, 30, 40, 15, 52, 38, 62, 65, 46,  3, 25, 62, 71, 11, 37, 38,\n",
              "        62, 65, 46,  3, 25, 23, 30, 33, 11, 35,  3, 37, 72,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0])"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_reverse_phoneme(phoneme_target_indices[index].detach().cpu().tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKn9J0v1U_w7",
        "outputId": "a0d64d07-ecaf-4f7c-d6dd-5ac02125cad8"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<start/>', 'AE1', 'N', 'D', 'AY1', 'W', 'AH0', 'L', 'S', 'IY1', 'DH', 'AH0', 'T', 'DH', 'EH1', 'R', 'IH0', 'Z', 'N', 'OW1', 'SH', 'ER1', 'K', 'IH0', 'NG', 'AH0', 'B', 'AW1', 'T', 'DH', 'AH1', 'B', 'OY1', 'Z', 'ER0', 'AH0', 'B', 'AW1', 'T', 'DH', 'AH1', 'G', 'ER1', 'L', 'Z', 'AY1', 'DH', 'ER0', '<end/>', 'AA2', 'AA2', 'AA2', 'AA2', 'AA2', 'AA2', 'AA2', 'AA2', 'AA2', 'AA2', 'AA2', 'AA2', 'AA2', 'AA2', 'AA2', 'AA2', 'AA2', 'AA2', 'AA2', 'AA2', 'AA2', 'AA2', 'AA2', 'AA2', 'AA2', 'AA2', 'AA2', 'AA2', 'AA2', 'AA2', 'AA2', 'AA2', 'AA2', 'AA2', 'AA2', 'AA2', 'AA2', 'AA2', 'AA2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "binary_tensor[index]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66YdsTmh5dav",
        "outputId": "11ca2073-0b11-4f5b-db1b-66e963cfa8f0"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gate_padded[index]"
      ],
      "metadata": {
        "id": "8kIlkmMf5nhO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba13cd56-e8ab-4797-dd0b-c6bcdc8ea392"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11UoZMQyR02S",
        "outputId": "86607d45-3f44-4671-d806-74944e7196b6"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'0': 'AA2',\n",
              " '1': 'CH',\n",
              " '2': 'ZH',\n",
              " '3': 'DH',\n",
              " '4': 'UW0',\n",
              " '5': 'W',\n",
              " '6': 'IY0',\n",
              " '7': 'OY0',\n",
              " '8': 'UH2',\n",
              " '9': 'D',\n",
              " '10': 'Y',\n",
              " '11': 'Z',\n",
              " '12': 'OY2',\n",
              " '13': 'HH',\n",
              " '14': 'AY2',\n",
              " '15': 'IH0',\n",
              " '16': 'EH1',\n",
              " '17': 'UW2',\n",
              " '18': 'UH0',\n",
              " '19': 'JH',\n",
              " '20': 'ER2',\n",
              " '21': 'V',\n",
              " '22': 'IY2',\n",
              " '23': 'G',\n",
              " '24': 'M',\n",
              " '25': 'AH1',\n",
              " '26': 'irish',\n",
              " '27': 'AA0',\n",
              " '28': 'AY0',\n",
              " '29': 'OW1',\n",
              " '30': 'ER1',\n",
              " '31': 'AO0',\n",
              " '32': 'EY1',\n",
              " '33': 'L',\n",
              " '34': 'AA1',\n",
              " '35': 'AY1',\n",
              " '36': 'SH',\n",
              " '37': 'ER0',\n",
              " '38': 'AH0',\n",
              " '39': 'UH1',\n",
              " '40': 'K',\n",
              " '41': 'S',\n",
              " '42': 'AH2',\n",
              " '43': 'AO2',\n",
              " '44': 'IY1',\n",
              " '45': 'OW0',\n",
              " '46': 'T',\n",
              " '47': 'IH1',\n",
              " '48': 'AW2',\n",
              " '49': 'EH0',\n",
              " '50': 'org,',\n",
              " '51': 'AO1',\n",
              " '52': 'NG',\n",
              " '53': 'EH2',\n",
              " '54': 'R',\n",
              " '55': '#',\n",
              " '56': 'EY0',\n",
              " '57': 'AW0',\n",
              " '58': 'AE2',\n",
              " '59': 'AE1',\n",
              " '60': 'OW2',\n",
              " '61': 'N',\n",
              " '62': 'B',\n",
              " '63': 'AE0',\n",
              " '64': 'F',\n",
              " '65': 'AW1',\n",
              " '66': 'P',\n",
              " '67': 'TH',\n",
              " '68': 'EY2',\n",
              " '69': 'IH2',\n",
              " '70': 'UW1',\n",
              " '71': 'OY1',\n",
              " 72: '<end/>',\n",
              " 73: '<start/>'}"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jPFuMXHCSAD2"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M77sbI3LSFar",
        "outputId": "363b204a-287f-41c1-d8ea-307fed525ce1"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(73), tensor(73, device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xNvJFGgKSH1V"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1oSlNe1TfykdLvTCYZLujZDOOM83cBvsw",
      "authorship_tag": "ABX9TyM0nweHeMnTt74Ef8kYYQbI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}