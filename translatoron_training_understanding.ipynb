{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1zdJp8kdTapaQyfvoCVxUJmS6EQ-9AT0E",
      "authorship_tag": "ABX9TyOui3ayLiHq4iOppfd6Bw5I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wasimmadha/dubbing-project-research/blob/main/translatoron_training_understanding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mee2VWGMDdRd"
      },
      "outputs": [],
      "source": [
        "## Imports\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from scipy.signal import get_window\n",
        "import librosa.util as librosa_util\n",
        "import random\n",
        "import torch.utils.data\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from math import sqrt\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import read\n",
        "import torch\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "from torch.nn.modules import Module\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from scipy.signal import get_window\n",
        "from librosa.util import pad_center, tiny\n",
        "\n",
        "import torch\n",
        "from librosa.filters import mel as librosa_mel_fn\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### hparams.py\n",
        "\n",
        "class mapDict(dict):\n",
        "  __getattr__ = dict.get\n",
        "  __setattr__ = dict.__setitem__\n",
        "  __delattr__ = dict.__delitem__\n",
        "\n",
        "\n",
        "\n",
        "def create_hparams(hparams_string=None,verbose=False):\n",
        "  hparams = {\n",
        "    ################################\n",
        "    # Experiment Parameters        #\n",
        "    ################################\n",
        "    \"epochs\":2,\n",
        "    \"iters_per_checkpoint\":10,\n",
        "    \"seed\":1234,\n",
        "    \"dynamic_loss_scaling\":True,\n",
        "    \"fp16_run\":False,\n",
        "    \"distributed_run\":False,\n",
        "    \"dist_backend\":\"nccl\",\n",
        "    \"dist_url\":\"tcp://localhost:54321\",\n",
        "    \"cudnn_enabled\":True,\n",
        "    \"cudnn_benchmark\":False,\n",
        "    \"ignore_layers\":['embedding.weight'],\n",
        "\n",
        "    ################################\n",
        "    # Data Parameters             #\n",
        "    ################################\n",
        "    \"load_mel_from_disk\":False,\n",
        "    \"training_files\":'data/train',\n",
        "    \"validation_files\":'data/val',\n",
        "    \"text_cleaners\":['english_cleaners'],\n",
        "\n",
        "    ################################\n",
        "    # Audio Parameters             #\n",
        "    ################################\n",
        "    \"max_wav_value\":32768.0,\n",
        "    \"sampling_rate\":22050,\n",
        "    \"filter_length\":1024,\n",
        "    \"hop_length\":256,\n",
        "    \"win_length\":1024,\n",
        "    \"n_mel_channels\":80,\n",
        "    \"mel_fmin\":0.0,\n",
        "    \"mel_fmax\":8000.0,\n",
        "\n",
        "    #Data parameters\n",
        "    \"input_data_root\": r'C:\\Users\\Wasim\\DubbingProject\\Speech2Speech\\google_research\\translatotron\\data\\prepared_data\\source\\train',\n",
        "    \"output_data_root\": r'C:\\Users\\Wasim\\DubbingProject\\Speech2Speech\\google_research\\translatotron\\data\\prepared_data\\target\\train',\n",
        "    \"train_size\": 0.75,\n",
        "    #Output Audio Parameters\n",
        "    \"out_channels\":1025,\n",
        "    ################################\n",
        "    # Model Parameters             #\n",
        "    ################################\n",
        "    \"symbols_embedding_dim\":512,\n",
        "\n",
        "    # Encoder parameters\n",
        "    \"encoder_kernel_size\":5,\n",
        "    \"encoder_n_convolutions\":3,\n",
        "    \"encoder_embedding_dim\":128,\n",
        "\n",
        "    # Decoder parameters\n",
        "    \"n_frames_per_step\":1,  # currently only 1 is supported\n",
        "    \"decoder_rnn_dim\":256,\n",
        "    \"prenet_dim\":32,\n",
        "    \"max_decoder_steps\":1000,\n",
        "    \"gate_threshold\":0.5,\n",
        "    \"p_attention_dropout\":0.1,\n",
        "    \"p_decoder_dropout\":0.1,\n",
        "\n",
        "    # Attention parameters\n",
        "    \"attention_rnn_dim\":256,\n",
        "    \"attention_dim\":128,\n",
        "    \"attention_heads\": 4,\n",
        "\n",
        "    # Location Layer parameters\n",
        "    \"attention_location_n_filters\":32,\n",
        "    \"attention_location_kernel_size\":31,\n",
        "\n",
        "    # Mel-post processing network parameters\n",
        "    \"postnet_embedding_dim\":128,\n",
        "    \"postnet_kernel_size\":5,\n",
        "    \"postnet_n_convolutions\":2,\n",
        "\n",
        "    ################################\n",
        "    # Optimization Hyperparameters #\n",
        "    ################################\n",
        "    \"use_saved_learning_rate\":False,\n",
        "    \"learning_rate\":1e-3,\n",
        "    \"weight_decay\":1e-6,\n",
        "    \"grad_clip_thresh\":1.0,\n",
        "    \"batch_size\":1,\n",
        "    \"mask_padding\":True\n",
        "    # set model's padded outputs to padded values\n",
        "  }\n",
        "\n",
        "  hparams = mapDict(hparams)\n",
        "\n",
        "  return hparams\n"
      ],
      "metadata": {
        "id": "fpnt1AVREWkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## utils.py\n",
        "\n",
        "def get_mask_from_lengths(lengths):\n",
        "    max_len = torch.max(lengths).item()\n",
        "    ids = torch.arange(0, max_len, out=torch.cuda.LongTensor(max_len))\n",
        "    mask = (ids < lengths.unsqueeze(1)).bool()\n",
        "    return mask\n",
        "\n",
        "\n",
        "def load_wav_to_torch(full_path):\n",
        "    # print(full_path)\n",
        "    sampling_rate, data = read(full_path)\n",
        "    return torch.FloatTensor(data.astype(np.float32)), sampling_rate\n",
        "\n",
        "\n",
        "def load_filepaths_and_text(filename, split=\"|\"):\n",
        "    with open(filename, encoding='utf-8') as f:\n",
        "        filepaths_and_text = [line.strip().split(split) for line in f]\n",
        "    return filepaths_and_text\n",
        "\n",
        "\n",
        "def to_gpu(x):\n",
        "    x = x.contiguous()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        x = x.cuda(non_blocking=True)\n",
        "    return torch.autograd.Variable(x)\n"
      ],
      "metadata": {
        "id": "qZXZ2Z2XP2DA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## plotting_utils.py\n",
        "def save_figure_to_numpy(fig):\n",
        "    # save it to a numpy array.\n",
        "    data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
        "    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "    return data\n",
        "\n",
        "\n",
        "def plot_alignment_to_numpy(alignment, info=None):\n",
        "    fig, ax = plt.subplots(figsize=(6, 4))\n",
        "    im = ax.imshow(alignment, aspect='auto', origin='lower',\n",
        "                   interpolation='none')\n",
        "    fig.colorbar(im, ax=ax)\n",
        "    xlabel = 'Decoder timestep'\n",
        "    if info is not None:\n",
        "        xlabel += '\\n\\n' + info\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel('Encoder timestep')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    fig.canvas.draw()\n",
        "    data = save_figure_to_numpy(fig)\n",
        "    plt.close()\n",
        "    return data\n",
        "\n",
        "\n",
        "def plot_spectrogram_to_numpy(spectrogram):\n",
        "    fig, ax = plt.subplots(figsize=(12, 3))\n",
        "    im = ax.imshow(spectrogram, aspect=\"auto\", origin=\"lower\",\n",
        "                   interpolation='none')\n",
        "    plt.colorbar(im, ax=ax)\n",
        "    plt.xlabel(\"Frames\")\n",
        "    plt.ylabel(\"Channels\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "    fig.canvas.draw()\n",
        "    data = save_figure_to_numpy(fig)\n",
        "    plt.close()\n",
        "    return data\n",
        "\n",
        "\n",
        "def plot_gate_outputs_to_numpy(gate_targets, gate_outputs):\n",
        "    fig, ax = plt.subplots(figsize=(12, 3))\n",
        "    ax.scatter(range(len(gate_targets)), gate_targets, alpha=0.5,\n",
        "               color='green', marker='+', s=1, label='target')\n",
        "    ax.scatter(range(len(gate_outputs)), gate_outputs, alpha=0.5,\n",
        "               color='red', marker='.', s=1, label='predicted')\n",
        "\n",
        "    plt.xlabel(\"Frames (Green target, Red predicted)\")\n",
        "    plt.ylabel(\"Gate State\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "    fig.canvas.draw()\n",
        "    data = save_figure_to_numpy(fig)\n",
        "    plt.close()\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "-m1UkccePx4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tacotron2Logger(SummaryWriter):\n",
        "    def __init__(self, logdir):\n",
        "        super(Tacotron2Logger, self).__init__(logdir)\n",
        "\n",
        "    def log_training(self, reduced_loss, grad_norm, learning_rate, duration,\n",
        "                     iteration):\n",
        "            self.add_scalar(\"training.loss\", reduced_loss, iteration)\n",
        "            self.add_scalar(\"grad.norm\", grad_norm, iteration)\n",
        "            self.add_scalar(\"learning.rate\", learning_rate, iteration)\n",
        "            self.add_scalar(\"duration\", duration, iteration)\n",
        "\n",
        "    def log_validation(self, reduced_loss, model, y, y_pred, iteration):\n",
        "        self.add_scalar(\"validation.loss\", reduced_loss, iteration)\n",
        "        _, mel_outputs, gate_outputs, alignments = y_pred\n",
        "        mel_targets, gate_targets = y\n",
        "\n",
        "        # plot distribution of parameters\n",
        "        for tag, value in model.named_parameters():\n",
        "            tag = tag.replace('.', '/')\n",
        "            self.add_histogram(tag, value.data.cpu().numpy(), iteration)\n",
        "\n",
        "        # plot alignment, mel target and predicted, gate target and predicted\n",
        "        idx = random.randint(0, alignments.size(0) - 1)\n",
        "        self.add_image(\n",
        "            \"alignment\",\n",
        "            plot_alignment_to_numpy(alignments[idx].data.cpu().numpy().T),\n",
        "            iteration, dataformats='HWC')\n",
        "        self.add_image(\n",
        "            \"mel_target\",\n",
        "            plot_spectrogram_to_numpy(mel_targets[idx].data.cpu().numpy()),\n",
        "            iteration, dataformats='HWC')\n",
        "        self.add_image(\n",
        "            \"mel_predicted\",\n",
        "            plot_spectrogram_to_numpy(mel_outputs[idx].data.cpu().numpy()),\n",
        "            iteration, dataformats='HWC')\n",
        "        self.add_image(\n",
        "            \"gate\",\n",
        "            plot_gate_outputs_to_numpy(\n",
        "                gate_targets[idx].data.cpu().numpy(),\n",
        "                torch.sigmoid(gate_outputs[idx]).data.cpu().numpy()),\n",
        "            iteration, dataformats='HWC')"
      ],
      "metadata": {
        "id": "NmTX-fHOPuC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### distributed.py\n",
        "def _flatten_dense_tensors(tensors):\n",
        "    \"\"\"Flatten dense tensors into a contiguous 1D buffer. Assume tensors are of\n",
        "    same dense type.\n",
        "    Since inputs are dense, the resulting tensor will be a concatenated 1D\n",
        "    buffer. Element-wise operation on this buffer will be equivalent to\n",
        "    operating individually.\n",
        "    Arguments:\n",
        "        tensors (Iterable[Tensor]): dense tensors to flatten.\n",
        "    Returns:\n",
        "        A contiguous 1D buffer containing input tensors.\n",
        "    \"\"\"\n",
        "    if len(tensors) == 1:\n",
        "        return tensors[0].contiguous().view(-1)\n",
        "    flat = torch.cat([t.contiguous().view(-1) for t in tensors], dim=0)\n",
        "    return flat\n",
        "\n",
        "def _unflatten_dense_tensors(flat, tensors):\n",
        "    \"\"\"View a flat buffer using the sizes of tensors. Assume that tensors are of\n",
        "    same dense type, and that flat is given by _flatten_dense_tensors.\n",
        "    Arguments:\n",
        "        flat (Tensor): flattened dense tensors to unflatten.\n",
        "        tensors (Iterable[Tensor]): dense tensors whose sizes will be used to\n",
        "          unflatten flat.\n",
        "    Returns:\n",
        "        Unflattened dense tensors with sizes same as tensors and values from\n",
        "        flat.\n",
        "    \"\"\"\n",
        "    outputs = []\n",
        "    offset = 0\n",
        "    for tensor in tensors:\n",
        "        numel = tensor.numel()\n",
        "        outputs.append(flat.narrow(0, offset, numel).view_as(tensor))\n",
        "        offset += numel\n",
        "    return tuple(outputs)\n",
        "\n",
        "\n",
        "'''\n",
        "This version of DistributedDataParallel is designed to be used in conjunction with the multiproc.py\n",
        "launcher included with this example. It assumes that your run is using multiprocess with 1\n",
        "GPU/process, that the model is on the correct device, and that torch.set_device has been\n",
        "used to set the device.\n",
        "\n",
        "Parameters are broadcasted to the other processes on initialization of DistributedDataParallel,\n",
        "and will be allreduced at the finish of the backward pass.\n",
        "'''\n",
        "class DistributedDataParallel(Module):\n",
        "\n",
        "    def __init__(self, module):\n",
        "        super(DistributedDataParallel, self).__init__()\n",
        "        #fallback for PyTorch 0.3\n",
        "        if not hasattr(dist, '_backend'):\n",
        "            self.warn_on_half = True\n",
        "        else:\n",
        "            self.warn_on_half = True if dist._backend == dist.dist_backend.GLOO else False\n",
        "\n",
        "        self.module = module\n",
        "\n",
        "        for p in self.module.state_dict().values():\n",
        "            if not torch.is_tensor(p):\n",
        "                continue\n",
        "            dist.broadcast(p, 0)\n",
        "\n",
        "        def allreduce_params():\n",
        "            if(self.needs_reduction):\n",
        "                self.needs_reduction = False\n",
        "                buckets = {}\n",
        "                for param in self.module.parameters():\n",
        "                    if param.requires_grad and param.grad is not None:\n",
        "                        tp = type(param.data)\n",
        "                        if tp not in buckets:\n",
        "                            buckets[tp] = []\n",
        "                        buckets[tp].append(param)\n",
        "                if self.warn_on_half:\n",
        "                    if torch.cuda.HalfTensor in buckets:\n",
        "                        print(\"WARNING: gloo dist backend for half parameters may be extremely slow.\" +\n",
        "                              \" It is recommended to use the NCCL backend in this case. This currently requires\" +\n",
        "                              \"PyTorch built from top of tree master.\")\n",
        "                        self.warn_on_half = False\n",
        "\n",
        "                for tp in buckets:\n",
        "                    bucket = buckets[tp]\n",
        "                    grads = [param.grad.data for param in bucket]\n",
        "                    coalesced = _flatten_dense_tensors(grads)\n",
        "                    dist.all_reduce(coalesced)\n",
        "                    coalesced /= dist.get_world_size()\n",
        "                    for buf, synced in zip(grads, _unflatten_dense_tensors(coalesced, grads)):\n",
        "                        buf.copy_(synced)\n",
        "\n",
        "        for param in list(self.module.parameters()):\n",
        "            def allreduce_hook(*unused):\n",
        "                param._execution_engine.queue_callback(allreduce_params)\n",
        "            if param.requires_grad:\n",
        "                param.register_hook(allreduce_hook)\n",
        "\n",
        "    def forward(self, *inputs, **kwargs):\n",
        "        self.needs_reduction = True\n",
        "        return self.module(*inputs, **kwargs)\n",
        "\n",
        "    '''\n",
        "    def _sync_buffers(self):\n",
        "        buffers = list(self.module._all_buffers())\n",
        "        if len(buffers) > 0:\n",
        "            # cross-node buffer sync\n",
        "            flat_buffers = _flatten_dense_tensors(buffers)\n",
        "            dist.broadcast(flat_buffers, 0)\n",
        "            for buf, synced in zip(buffers, _unflatten_dense_tensors(flat_buffers, buffers)):\n",
        "                buf.copy_(synced)\n",
        "     def train(self, mode=True):\n",
        "        # Clear NCCL communicator and CUDA event cache of the default group ID,\n",
        "        # These cache will be recreated at the later call. This is currently a\n",
        "        # work-around for a potential NCCL deadlock.\n",
        "        if dist._backend == dist.dist_backend.NCCL:\n",
        "            dist._clear_group_cache()\n",
        "        super(DistributedDataParallel, self).train(mode)\n",
        "        self.module.train(mode)\n",
        "    '''\n",
        "'''\n",
        "Modifies existing model to do gradient allreduce, but doesn't change class\n",
        "so you don't need \"module\"\n",
        "'''\n",
        "def apply_gradient_allreduce(module):\n",
        "        if not hasattr(dist, '_backend'):\n",
        "            module.warn_on_half = True\n",
        "        else:\n",
        "            module.warn_on_half = True if dist._backend == dist.dist_backend.GLOO else False\n",
        "\n",
        "        for p in module.state_dict().values():\n",
        "            if not torch.is_tensor(p):\n",
        "                continue\n",
        "            dist.broadcast(p, 0)\n",
        "\n",
        "        def allreduce_params():\n",
        "            if(module.needs_reduction):\n",
        "                module.needs_reduction = False\n",
        "                buckets = {}\n",
        "                for param in module.parameters():\n",
        "                    if param.requires_grad and param.grad is not None:\n",
        "                        tp = param.data.dtype\n",
        "                        if tp not in buckets:\n",
        "                            buckets[tp] = []\n",
        "                        buckets[tp].append(param)\n",
        "                if module.warn_on_half:\n",
        "                    if torch.cuda.HalfTensor in buckets:\n",
        "                        print(\"WARNING: gloo dist backend for half parameters may be extremely slow.\" +\n",
        "                              \" It is recommended to use the NCCL backend in this case. This currently requires\" +\n",
        "                              \"PyTorch built from top of tree master.\")\n",
        "                        module.warn_on_half = False\n",
        "\n",
        "                for tp in buckets:\n",
        "                    bucket = buckets[tp]\n",
        "                    grads = [param.grad.data for param in bucket]\n",
        "                    coalesced = _flatten_dense_tensors(grads)\n",
        "                    dist.all_reduce(coalesced)\n",
        "                    coalesced /= dist.get_world_size()\n",
        "                    for buf, synced in zip(grads, _unflatten_dense_tensors(coalesced, grads)):\n",
        "                        buf.copy_(synced)\n",
        "\n",
        "        for param in list(module.parameters()):\n",
        "            def allreduce_hook(*unused):\n",
        "                Variable._execution_engine.queue_callback(allreduce_params)\n",
        "            if param.requires_grad:\n",
        "                param.register_hook(allreduce_hook)\n",
        "\n",
        "        def set_needs_reduction(self, input, output):\n",
        "            self.needs_reduction = True\n",
        "\n",
        "        module.register_forward_hook(set_needs_reduction)\n",
        "        return module"
      ],
      "metadata": {
        "id": "gwsYn7Xu8FZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### audio_preprocessing.py\n",
        "\n",
        "def window_sumsquare(window, n_frames, hop_length=200, win_length=800,\n",
        "                     n_fft=800, dtype=np.float32, norm=None):\n",
        "    \"\"\"\n",
        "    # from librosa 0.6\n",
        "    Compute the sum-square envelope of a window function at a given hop length.\n",
        "\n",
        "    This is used to estimate modulation effects induced by windowing\n",
        "    observations in short-time fourier transforms.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    window : string, tuple, number, callable, or list-like\n",
        "        Window specification, as in `get_window`\n",
        "\n",
        "    n_frames : int > 0\n",
        "        The number of analysis frames\n",
        "\n",
        "    hop_length : int > 0\n",
        "        The number of samples to advance between frames\n",
        "\n",
        "    win_length : [optional]\n",
        "        The length of the window function.  By default, this matches `n_fft`.\n",
        "\n",
        "    n_fft : int > 0\n",
        "        The length of each analysis frame.\n",
        "\n",
        "    dtype : np.dtype\n",
        "        The data type of the output\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    wss : np.ndarray, shape=`(n_fft + hop_length * (n_frames - 1))`\n",
        "        The sum-squared envelope of the window function\n",
        "    \"\"\"\n",
        "    if win_length is None:\n",
        "        win_length = n_fft\n",
        "\n",
        "    n = n_fft + hop_length * (n_frames - 1)\n",
        "    x = np.zeros(n, dtype=dtype)\n",
        "\n",
        "    # Compute the squared window at the desired length\n",
        "    win_sq = get_window(window, win_length, fftbins=True)\n",
        "    win_sq = librosa_util.normalize(win_sq, norm=norm)**2\n",
        "    win_sq = librosa_util.pad_center(win_sq, n_fft)\n",
        "\n",
        "    # Fill the envelope\n",
        "    for i in range(n_frames):\n",
        "        sample = i * hop_length\n",
        "        x[sample:min(n, sample + n_fft)] += win_sq[:max(0, min(n_fft, n - sample))]\n",
        "    return x\n",
        "\n",
        "\n",
        "def griffin_lim(magnitudes, stft_fn, n_iters=30):\n",
        "    \"\"\"\n",
        "    PARAMS\n",
        "    ------\n",
        "    magnitudes: spectrogram magnitudes\n",
        "    stft_fn: STFT class with transform (STFT) and inverse (ISTFT) methods\n",
        "    \"\"\"\n",
        "\n",
        "    angles = np.angle(np.exp(2j * np.pi * np.random.rand(*magnitudes.size())))\n",
        "    angles = angles.astype(np.float32)\n",
        "    angles = torch.autograd.Variable(torch.from_numpy(angles))\n",
        "    signal = stft_fn.inverse(magnitudes, angles).squeeze(1)\n",
        "\n",
        "    for i in range(n_iters):\n",
        "        _, angles = stft_fn.transform(signal)\n",
        "        signal = stft_fn.inverse(magnitudes, angles).squeeze(1)\n",
        "    return signal\n",
        "\n",
        "\n",
        "def dynamic_range_compression(x, C=1, clip_val=1e-5):\n",
        "    \"\"\"\n",
        "    PARAMS\n",
        "    ------\n",
        "    C: compression factor\n",
        "    \"\"\"\n",
        "    return torch.log(torch.clamp(x, min=clip_val) * C)\n",
        "\n",
        "\n",
        "def dynamic_range_decompression(x, C=1):\n",
        "    \"\"\"\n",
        "    PARAMS\n",
        "    ------\n",
        "    C: compression factor used to compress\n",
        "    \"\"\"\n",
        "    return torch.exp(x) / C\n"
      ],
      "metadata": {
        "id": "lKLWh2UmDlx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### data_utils.py\n",
        "class TextMelLoader(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "        1) loads audio,text pairs\n",
        "        2) normalizes text and converts them to sequences of one-hot vectors\n",
        "        3) computes mel-spectrograms from audio files.\n",
        "    \"\"\"\n",
        "    def __init__(self, audiopaths, hparams):\n",
        "        self.inputs = audiopaths[0]\n",
        "        self.outputs = audiopaths[1]\n",
        "        self.text_cleaners = hparams.text_cleaners\n",
        "        self.max_wav_value = hparams.max_wav_value\n",
        "        self.sampling_rate = hparams.sampling_rate\n",
        "        self.load_mel_from_disk = hparams.load_mel_from_disk\n",
        "        self.stft = TacotronSTFT(\n",
        "            hparams.filter_length, hparams.hop_length, hparams.win_length,\n",
        "            hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,\n",
        "            hparams.mel_fmax)\n",
        "        # random.seed(hparams.seed)\n",
        "        # random.shuffle(self.audiopaths_and_text)\n",
        "\n",
        "        for i, input_file in enumerate(self.inputs):\n",
        "            melspec = self.get_mel(input_file)\n",
        "            os.makedirs('melspec_input', exist_ok=True)\n",
        "\n",
        "            # # Plot and save the mel spectrogram as an image\n",
        "            # plt.figure(figsize=(8, 6))\n",
        "            # plt.imshow(melspec, cmap='viridis', origin='lower', aspect='auto')\n",
        "            # plt.title('Mel Spectrogram')\n",
        "            # plt.xlabel('Time')\n",
        "            # plt.ylabel('Frequency')\n",
        "            # plt.colorbar(format='%+2.0f dB')\n",
        "            # plt.savefig(os.path.join('melspec_input', f'mel_spectrogram_{i}.png'))\n",
        "\n",
        "        for i, input_file in enumerate(self.outputs):\n",
        "            melspec = self.get_mel(input_file)\n",
        "            os.makedirs('melspec_output', exist_ok=True)\n",
        "\n",
        "            # # Plot and save the mel spectrogram as an image\n",
        "            # plt.figure(figsize=(8, 6))\n",
        "            # plt.imshow(melspec, cmap='viridis', origin='lower', aspect='auto')\n",
        "            # plt.title('Mel Spectrogram')\n",
        "            # plt.xlabel('Time')\n",
        "            # plt.ylabel('Frequency')\n",
        "            # plt.colorbar(format='%+2.0f dB')\n",
        "            # plt.savefig(os.path.join('melspec_output', f'mel_spectrogram_{i}.png'))\n",
        "\n",
        "    def get_mel_spec_pair(self, index):\n",
        "        # separate filename and text\n",
        "        # lin = self.get_spec(self.outputs[index])\n",
        "        # mel = self.get_mel(self.inputs[index])\n",
        "        inputs = self.get_mel(self.inputs[index])\n",
        "        outputs = self.get_mel(self.outputs[index])\n",
        "\n",
        "        return (inputs,outputs)\n",
        "\n",
        "    def get_mel(self, filename):\n",
        "        if not self.load_mel_from_disk:\n",
        "            audio, sampling_rate = load_wav_to_torch(filename)\n",
        "\n",
        "            audio = audio[:, 0]\n",
        "            # if sampling_rate != self.stft.sampling_rate:\n",
        "            #     raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
        "            #         sampling_rate, self.stft.sampling_rate))\n",
        "            audio_norm = audio / self.max_wav_value\n",
        "            audio_norm = audio_norm.unsqueeze(0)\n",
        "            audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
        "            melspec = self.stft.mel_spectrogram(audio_norm)\n",
        "            melspec = torch.squeeze(melspec, 0)\n",
        "            melspec_arr = melspec[0].numpy()\n",
        "\n",
        "            # # Plot and save the mel spectrogram as an image\n",
        "            # plt.figure(figsize=(8, 6))\n",
        "            # plt.imshow(melspec, cmap='viridis', origin='lower', aspect='auto')\n",
        "            # plt.title('Mel Spectrogram')\n",
        "            # plt.xlabel('Time')\n",
        "            # plt.ylabel('Frequency')\n",
        "            # plt.colorbar(format='%+2.0f dB')\n",
        "            # plt.savefig('mel_spectrogram.png')\n",
        "\n",
        "        else:\n",
        "            melspec = torch.from_numpy(np.load(filename))\n",
        "            assert melspec.size(0) == self.stft.n_mel_channels, (\n",
        "                'Mel dimension mismatch: given {}, expected {}'.format(\n",
        "                    melspec.size(0), self.stft.n_mel_channels))\n",
        "\n",
        "        return melspec\n",
        "\n",
        "    def get_spec(self, filename):\n",
        "        if not self.load_mel_from_disk:\n",
        "            audio, sampling_rate = load_wav_to_torch(filename)\n",
        "            if sampling_rate != self.stft.sampling_rate:\n",
        "                raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
        "                    sampling_rate, self.stft.sampling_rate))\n",
        "            audio_norm = audio / self.max_wav_value\n",
        "            audio_norm = audio_norm.unsqueeze(0)\n",
        "            audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
        "            spec = self.stft.spectrogram(audio_norm)\n",
        "            spec = torch.squeeze(spec, 0)\n",
        "        else:\n",
        "            spec = torch.from_numpy(np.load(filename))\n",
        "            # assert melspec.size(0) == self.stft.n_mel_channels, (\n",
        "            #     'Mel dimension mismatch: given {}, expected {}'.format(\n",
        "            #         melspec.size(0), self.stft.n_mel_channels))\n",
        "\n",
        "        return spec\n",
        "\n",
        "    # def get_text(self, text):\n",
        "    #     text_norm = torch.IntTensor(text_to_sequence(text, self.text_cleaners))\n",
        "    #     return text_norm\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.get_mel_spec_pair(index)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "\n",
        "class TextMelCollate():\n",
        "    \"\"\" Zero-pads model inputs and targets based on number of frames per setep\n",
        "    \"\"\"\n",
        "    def __init__(self, n_frames_per_step):\n",
        "        self.n_frames_per_step = n_frames_per_step\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        \"\"\"Collate's training batch from normalized text and mel-spectrogram\n",
        "        PARAMS\n",
        "        ------\n",
        "        batch: [text_normalized, mel_normalized]\n",
        "        \"\"\"\n",
        "\n",
        "        # Right zero-pad mel-spec\n",
        "        # num_mels = batch[0][1].size(0)\n",
        "        # max_input_len = max([x[0].size(1) for x in batch])\n",
        "        # if max_input_len % self.n_frames_per_step != 0:\n",
        "        #     max_input_len += self.n_frames_per_step - max_input_len % self.n_frames_per_step\n",
        "        #     assert max_input_len % self.n_frames_per_step == 0\n",
        "\n",
        "        # # include mel padded and gate padded\n",
        "        # mel_padded = torch.FloatTensor(len(batch), num_mels, max_input_len)\n",
        "        # mel_padded.zero_()\n",
        "        # gate_padded = torch.FloatTensor(len(batch), max_input_len)\n",
        "        # gate_padded.zero_()\n",
        "        # input_lengths = torch.LongTensor(len(batch))\n",
        "        # for i in range(len(batch)):\n",
        "        #     mel = batch[i][0]\n",
        "        #     mel_padded[i, :, :mel.size(1)] = mel\n",
        "        #     gate_padded[i, mel.size(1)-1:] = 1\n",
        "        #     input_lengths[i] = mel.size(1)\n",
        "\n",
        "        # # input_lengths, ids_sorted_decreasing = torch.sort(\n",
        "        # #     torch.LongTensor([len(x[0]) for x in batch]),\n",
        "        # #     dim=0, descending=True)\n",
        "        # num_dims = batch[0][1].size(0)\n",
        "        # max_target_len = max([x[1].size(1) for x in batch])\n",
        "        # spec_padded = torch.FloatTensor(len(batch), num_dims, max_target_len)\n",
        "        # spec_padded.zero_()\n",
        "        # # gate_padded = torch.FloatTensor(len(batch), max_target_len)\n",
        "        # # gate_padded.zero_()\n",
        "        # output_lengths = torch.LongTensor(len(batch))\n",
        "        # for i in range(len(batch)):\n",
        "        #     spec = batch[i][1]\n",
        "        #     spec_padded[i, :, :spec.size(1)] = spec\n",
        "        #     # gate_padded[i, mel.size(1)-1:] = 1\n",
        "        #     output_lengths[i] = spec.size(1)\n",
        "\n",
        "        # return mel_padded, gate_padded,input_lengths, spec_padded, \\\n",
        "        #     output_lengths\n",
        "        num_mels = batch[0][0].size(0)\n",
        "        # max_input_len = max([x[0].size(1) for x in batch])\n",
        "        input_lengths, ids_sorted_decreasing = torch.sort(\n",
        "            torch.LongTensor([x[0].size(1) for x in batch]),\n",
        "            dim=0, descending=True)\n",
        "        max_input_len = input_lengths[0]\n",
        "        if max_input_len % self.n_frames_per_step != 0:\n",
        "            max_input_len += self.n_frames_per_step - max_input_len % self.n_frames_per_step\n",
        "            assert max_input_len % self.n_frames_per_step == 0\n",
        "\n",
        "        # include mel padded and gate padded\n",
        "        input_padded = torch.FloatTensor(len(batch), num_mels, max_input_len)\n",
        "        input_padded.zero_()\n",
        "        # gate_padded = torch.FloatTensor(len(batch), max_target_len)\n",
        "        # gate_padded.zero_()\n",
        "        for i in ids_sorted_decreasing:\n",
        "            mel = batch[i][0]\n",
        "            input_padded[i, :, :mel.size(1)] = mel\n",
        "            # gate_padded[i, mel.size(1)-1:] = 1\n",
        "\n",
        "\n",
        "        num_mels = batch[0][1].size(0)\n",
        "        max_target_len = max([x[1].size(1) for x in batch])\n",
        "        if max_target_len % self.n_frames_per_step != 0:\n",
        "            max_target_len += self.n_frames_per_step - max_target_len % self.n_frames_per_step\n",
        "            assert max_target_len % self.n_frames_per_step == 0\n",
        "\n",
        "        # include mel padded and gate padded\n",
        "        mel_padded = torch.FloatTensor(len(batch), num_mels, max_target_len)\n",
        "        mel_padded.zero_()\n",
        "        gate_padded = torch.FloatTensor(len(batch), max_target_len)\n",
        "        gate_padded.zero_()\n",
        "        output_lengths = torch.LongTensor(len(batch))\n",
        "        for i in ids_sorted_decreasing:\n",
        "            mel = batch[i][1]\n",
        "            mel_padded[i, :, :mel.size(1)] = mel\n",
        "            gate_padded[i, mel.size(1)-1:] = 1\n",
        "            output_lengths[i] = mel.size(1)\n",
        "\n",
        "        return input_padded, input_lengths, mel_padded, gate_padded, \\\n",
        "            output_lengths\n"
      ],
      "metadata": {
        "id": "zrVmvhg8Dswc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## stft.py\n",
        "\n",
        "\"\"\"\n",
        "BSD 3-Clause License\n",
        "\n",
        "Copyright (c) 2017, Prem Seetharaman\n",
        "All rights reserved.\n",
        "\n",
        "* Redistribution and use in source and binary forms, with or without\n",
        "  modification, are permitted provided that the following conditions are met:\n",
        "\n",
        "* Redistributions of source code must retain the above copyright notice,\n",
        "  this list of conditions and the following disclaimer.\n",
        "\n",
        "* Redistributions in binary form must reproduce the above copyright notice, this\n",
        "  list of conditions and the following disclaimer in the\n",
        "  documentation and/or other materials provided with the distribution.\n",
        "\n",
        "* Neither the name of the copyright holder nor the names of its\n",
        "  contributors may be used to endorse or promote products derived from this\n",
        "  software without specific prior written permission.\n",
        "\n",
        "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n",
        "ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n",
        "WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
        "DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR\n",
        "ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n",
        "(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n",
        "LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\n",
        "ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n",
        "(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n",
        "SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
        "\"\"\"\n",
        "\n",
        "class STFT(torch.nn.Module):\n",
        "    \"\"\"adapted from Prem Seetharaman's https://github.com/pseeth/pytorch-stft\"\"\"\n",
        "    def __init__(self, filter_length=800, hop_length=200, win_length=800,\n",
        "                 window='hann'):\n",
        "        super(STFT, self).__init__()\n",
        "        self.filter_length = filter_length\n",
        "        self.hop_length = hop_length\n",
        "        self.win_length = win_length\n",
        "        self.window = window\n",
        "        self.forward_transform = None\n",
        "        scale = self.filter_length / self.hop_length\n",
        "        fourier_basis = np.fft.fft(np.eye(self.filter_length))\n",
        "\n",
        "        cutoff = int((self.filter_length / 2 + 1))\n",
        "        fourier_basis = np.vstack([np.real(fourier_basis[:cutoff, :]),\n",
        "                                   np.imag(fourier_basis[:cutoff, :])])\n",
        "\n",
        "        forward_basis = torch.FloatTensor(fourier_basis[:, None, :])\n",
        "        inverse_basis = torch.FloatTensor(\n",
        "            np.linalg.pinv(scale * fourier_basis).T[:, None, :])\n",
        "\n",
        "        if window is not None:\n",
        "            assert(filter_length >= win_length)\n",
        "            # get window and zero center pad it to filter_length\n",
        "            fft_window = get_window(window, win_length, fftbins=True)\n",
        "            fft_window = pad_center(data=fft_window, size=filter_length)\n",
        "            fft_window = torch.from_numpy(fft_window).float()\n",
        "\n",
        "            # window the bases\n",
        "            forward_basis *= fft_window\n",
        "            inverse_basis *= fft_window\n",
        "\n",
        "        self.register_buffer('forward_basis', forward_basis.float())\n",
        "        self.register_buffer('inverse_basis', inverse_basis.float())\n",
        "\n",
        "    def transform(self, input_data):\n",
        "        num_batches = input_data.size(0)\n",
        "        num_samples = input_data.size(1)\n",
        "\n",
        "        self.num_samples = num_samples\n",
        "\n",
        "        # similar to librosa, reflect-pad the input\n",
        "        input_data = input_data.view(num_batches, 1, num_samples)\n",
        "        input_data = F.pad(\n",
        "            input_data.unsqueeze(1),\n",
        "            (int(self.filter_length / 2), int(self.filter_length / 2), 0, 0),\n",
        "            mode='reflect')\n",
        "        input_data = input_data.squeeze(1)\n",
        "\n",
        "        forward_transform = F.conv1d(\n",
        "            input_data,\n",
        "            Variable(self.forward_basis, requires_grad=False),\n",
        "            stride=self.hop_length,\n",
        "            padding=0)\n",
        "\n",
        "        cutoff = int((self.filter_length / 2) + 1)\n",
        "        real_part = forward_transform[:, :cutoff, :]\n",
        "        imag_part = forward_transform[:, cutoff:, :]\n",
        "\n",
        "        magnitude = torch.sqrt(real_part**2 + imag_part**2)\n",
        "        phase = torch.autograd.Variable(\n",
        "            torch.atan2(imag_part.data, real_part.data))\n",
        "\n",
        "        return magnitude, phase\n",
        "\n",
        "    def inverse(self, magnitude, phase):\n",
        "        recombine_magnitude_phase = torch.cat(\n",
        "            [magnitude*torch.cos(phase), magnitude*torch.sin(phase)], dim=1)\n",
        "\n",
        "        inverse_transform = F.conv_transpose1d(\n",
        "            recombine_magnitude_phase,\n",
        "            Variable(self.inverse_basis, requires_grad=False),\n",
        "            stride=self.hop_length,\n",
        "            padding=0)\n",
        "\n",
        "        if self.window is not None:\n",
        "            window_sum = window_sumsquare(\n",
        "                self.window, magnitude.size(-1), hop_length=self.hop_length,\n",
        "                win_length=self.win_length, n_fft=self.filter_length,\n",
        "                dtype=np.float32)\n",
        "            # remove modulation effects\n",
        "            approx_nonzero_indices = torch.from_numpy(\n",
        "                np.where(window_sum > tiny(window_sum))[0])\n",
        "            window_sum = torch.autograd.Variable(\n",
        "                torch.from_numpy(window_sum), requires_grad=False)\n",
        "            window_sum = window_sum.cuda() if magnitude.is_cuda else window_sum\n",
        "            inverse_transform[:, :, approx_nonzero_indices] /= window_sum[approx_nonzero_indices]\n",
        "\n",
        "            # scale by hop ratio\n",
        "            inverse_transform *= float(self.filter_length) / self.hop_length\n",
        "\n",
        "        inverse_transform = inverse_transform[:, :, int(self.filter_length/2):]\n",
        "        inverse_transform = inverse_transform[:, :, :-int(self.filter_length/2):]\n",
        "\n",
        "        return inverse_transform\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        self.magnitude, self.phase = self.transform(input_data)\n",
        "        reconstruction = self.inverse(self.magnitude, self.phase)\n",
        "        return reconstruction\n"
      ],
      "metadata": {
        "id": "lz9xn2VTPM8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## layers.py\n",
        "\n",
        "class LinearNorm(torch.nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, bias=True, w_init_gain='linear'):\n",
        "        super(LinearNorm, self).__init__()\n",
        "        self.linear_layer = torch.nn.Linear(in_dim, out_dim, bias=bias)\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(\n",
        "            self.linear_layer.weight,\n",
        "            gain=torch.nn.init.calculate_gain(w_init_gain))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear_layer(x)\n",
        "\n",
        "\n",
        "class ConvNorm(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,\n",
        "                 padding=None, dilation=1, bias=True, w_init_gain='linear'):\n",
        "        super(ConvNorm, self).__init__()\n",
        "        if padding is None:\n",
        "            assert(kernel_size % 2 == 1)\n",
        "            padding = int(dilation * (kernel_size - 1) / 2)\n",
        "\n",
        "        self.conv = torch.nn.Conv1d(in_channels, out_channels,\n",
        "                                    kernel_size=kernel_size, stride=stride,\n",
        "                                    padding=padding, dilation=dilation,\n",
        "                                    bias=bias)\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(\n",
        "            self.conv.weight, gain=torch.nn.init.calculate_gain(w_init_gain))\n",
        "\n",
        "    def forward(self, signal):\n",
        "        conv_signal = self.conv(signal)\n",
        "        return conv_signal\n",
        "\n",
        "\n",
        "class TacotronSTFT(torch.nn.Module):\n",
        "    def __init__(self, filter_length=1024, hop_length=256, win_length=1024,\n",
        "                 n_mel_channels=80, sampling_rate=22050, mel_fmin=0.0,\n",
        "                 mel_fmax=8000.0):\n",
        "        super(TacotronSTFT, self).__init__()\n",
        "        self.n_mel_channels = n_mel_channels\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.stft_fn = STFT(filter_length, hop_length, win_length)\n",
        "        mel_basis = librosa_mel_fn(\n",
        "            sr=sampling_rate, n_fft=filter_length, n_mels=n_mel_channels, fmin=mel_fmin, fmax=mel_fmax)\n",
        "        mel_basis = torch.from_numpy(mel_basis).float()\n",
        "        self.register_buffer('mel_basis', mel_basis)\n",
        "\n",
        "    def spectral_normalize(self, magnitudes):\n",
        "        output = dynamic_range_compression(magnitudes)\n",
        "        return output\n",
        "\n",
        "    def spectral_de_normalize(self, magnitudes):\n",
        "        output = dynamic_range_decompression(magnitudes)\n",
        "        return output\n",
        "\n",
        "    def spectrogram(self,y):\n",
        "        assert(torch.min(y.data) >= -1)\n",
        "        assert(torch.max(y.data) <= 1)\n",
        "\n",
        "        magnitudes, phases = self.stft_fn.transform(y)\n",
        "        magnitudes = magnitudes.data\n",
        "\n",
        "        return magnitudes\n",
        "\n",
        "    def mel_spectrogram(self, y):\n",
        "        \"\"\"Computes mel-spectrograms from a batch of waves\n",
        "        PARAMS\n",
        "        ------\n",
        "        y: Variable(torch.FloatTensor) with shape (B, T) in range [-1, 1]\n",
        "\n",
        "        RETURNS\n",
        "        -------\n",
        "        mel_output: torch.FloatTensor of shape (B, n_mel_channels, T)\n",
        "        \"\"\"\n",
        "        # assert(torch.min(y.data) >= -1)\n",
        "        # assert(torch.max(y.data) <= 1)\n",
        "        magnitudes, phases = self.stft_fn.transform(y)\n",
        "        magnitudes = magnitudes.data\n",
        "        mel_output = torch.matmul(self.mel_basis, magnitudes)\n",
        "        mel_output = self.spectral_normalize(mel_output)\n",
        "        return mel_output\n"
      ],
      "metadata": {
        "id": "wBvL4Wl5EO4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## model.py\n",
        "\n",
        "class LocationLayer(nn.Module):\n",
        "    def __init__(self, attention_n_filters, attention_kernel_size,\n",
        "                 attention_dim):\n",
        "        super(LocationLayer, self).__init__()\n",
        "        padding = int((attention_kernel_size - 1) / 2)\n",
        "        self.location_conv = ConvNorm(2, attention_n_filters,\n",
        "                                      kernel_size=attention_kernel_size,\n",
        "                                      padding=padding, bias=False, stride=1,\n",
        "                                      dilation=1)\n",
        "        self.location_dense = LinearNorm(attention_n_filters, attention_dim,\n",
        "                                         bias=False, w_init_gain='tanh')\n",
        "\n",
        "    def forward(self, attention_weights_cat):\n",
        "        processed_attention = self.location_conv(attention_weights_cat)\n",
        "        processed_attention = processed_attention.transpose(1, 2)\n",
        "        processed_attention = self.location_dense(processed_attention)\n",
        "        return processed_attention\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, attention_rnn_dim, embedding_dim, attention_dim,\n",
        "                 attention_location_n_filters, attention_location_kernel_size,num_heads=4):\n",
        "        super(Attention, self).__init__()\n",
        "        self.query_layer = LinearNorm(attention_rnn_dim, attention_dim,\n",
        "                                      bias=False, w_init_gain='tanh')\n",
        "        self.memory_layer = LinearNorm(embedding_dim, attention_dim, bias=False,\n",
        "                                       w_init_gain='tanh')\n",
        "        # self.v = LinearNorm(attention_dim, 1, bias=False)\n",
        "        self.attention = nn.MultiheadAttention(attention_dim,num_heads,batch_first=True)\n",
        "        self.location_layer = LocationLayer(attention_location_n_filters,\n",
        "                                            attention_location_kernel_size,\n",
        "                                            attention_dim)\n",
        "        self.score_mask_value = -float(\"inf\")\n",
        "\n",
        "    def get_alignment_energies(self, query, processed_memory,\n",
        "                               attention_weights_cat):\n",
        "        \"\"\"\n",
        "        PARAMS\n",
        "        ------\n",
        "        query: decoder output (batch, n_mel_channels * n_frames_per_step)\n",
        "        processed_memory: processed encoder outputs (B, T_in, attention_dim)\n",
        "        attention_weights_cat: cumulative and prev. att weights (B, 2, max_time)\n",
        "\n",
        "        RETURNS\n",
        "        -------\n",
        "        alignment (batch, max_time)\n",
        "        \"\"\"\n",
        "\n",
        "        processed_query = self.query_layer(query.unsqueeze(1))\n",
        "        processed_attention_weights = self.location_layer(attention_weights_cat)\n",
        "        # energies = self.v(torch.tanh(\n",
        "        #     processed_query + processed_attention_weights + processed_memory))\n",
        "        # energies = energies.squeeze(-1)\n",
        "        return processed_query,processed_attention_weights\n",
        "\n",
        "    def forward(self, attention_hidden_state, memory, processed_memory,\n",
        "                attention_weights_cat, mask):\n",
        "        \"\"\"\n",
        "        PARAMS\n",
        "        ------\n",
        "        attention_hidden_state: attention rnn last output\n",
        "        memory: encoder outputs\n",
        "        processed_memory: processed encoder outputs\n",
        "        attention_weights_cat: previous and cummulative attention weights\n",
        "        mask: binary mask for padded data\n",
        "        \"\"\"\n",
        "        # alignment = self.get_alignment_energies(\n",
        "            # attention_hidden_state, processed_memory, attention_weights_cat)\n",
        "\n",
        "        processed_query,processed_attention_weights = self.get_alignment_energies(\n",
        "        attention_hidden_state, processed_memory, attention_weights_cat)\n",
        "\n",
        "        # if mask is not None:\n",
        "        #     alignment.data.masked_fill_(mask, self.score_mask_value)\n",
        "\n",
        "        # attention_weights = F.softmax(alignment, dim=1)\n",
        "        # attention_context = torch.bmm(attention_weights.unsqueeze(1), memory)\n",
        "        # attention_context = attention_context.squeeze(1)\n",
        "\n",
        "        attention_context,attention_weights = self.attention(processed_query,processed_attention_weights,processed_memory)\n",
        "        return attention_context.squeeze(1), attention_weights.squeeze(1)\n",
        "\n",
        "\n",
        "# class Attention(nn.Module):\n",
        "#     def __init__(self, attention_rnn_dim, embedding_dim, attention_dim,\n",
        "#                  attention_location_n_filters, attention_location_kernel_size):\n",
        "#         super(Attention, self).__init__()\n",
        "#         self.query_layer = LinearNorm(attention_rnn_dim, attention_dim,\n",
        "#                                       bias=False, w_init_gain='tanh')\n",
        "#         self.memory_layer = LinearNorm(embedding_dim, attention_dim, bias=False,\n",
        "#                                        w_init_gain='tanh')\n",
        "#         self.v = LinearNorm(attention_dim, 1, bias=False)\n",
        "#         self.location_layer = LocationLayer(attention_location_n_filters,\n",
        "#                                             attention_location_kernel_size,\n",
        "#                                             attention_dim)\n",
        "#         self.score_mask_value = -float(\"inf\")\n",
        "\n",
        "#     def get_alignment_energies(self, query, processed_memory,\n",
        "#                                attention_weights_cat):\n",
        "#         \"\"\"\n",
        "#         PARAMS\n",
        "#         ------\n",
        "#         query: decoder output (batch, n_mel_channels * n_frames_per_step)\n",
        "#         processed_memory: processed encoder outputs (B, T_in, attention_dim)\n",
        "#         attention_weights_cat: cumulative and prev. att weights (B, 2, max_time)\n",
        "\n",
        "#         RETURNS\n",
        "#         -------\n",
        "#         alignment (batch, max_time)\n",
        "#         \"\"\"\n",
        "\n",
        "#         processed_query = self.query_layer(query.unsqueeze(1))\n",
        "#         processed_attention_weights = self.location_layer(attention_weights_cat)\n",
        "#         energies = self.v(torch.tanh(\n",
        "#             processed_query + processed_attention_weights + processed_memory))\n",
        "\n",
        "#         energies = energies.squeeze(-1)\n",
        "#         return energies\n",
        "\n",
        "#     def forward(self, attention_hidden_state, memory, processed_memory,\n",
        "#                 attention_weights_cat, mask):\n",
        "#         \"\"\"\n",
        "#         PARAMS\n",
        "#         ------\n",
        "#         attention_hidden_state: attention rnn last output\n",
        "#         memory: encoder outputs\n",
        "#         processed_memory: processed encoder outputs\n",
        "#         attention_weights_cat: previous and cummulative attention weights\n",
        "#         mask: binary mask for padded data\n",
        "#         \"\"\"\n",
        "#         alignment = self.get_alignment_energies(\n",
        "#             attention_hidden_state, processed_memory, attention_weights_cat)\n",
        "\n",
        "#         if mask is not None:\n",
        "#             alignment.data.masked_fill_(mask, self.score_mask_value)\n",
        "\n",
        "#         attention_weights = F.softmax(alignment, dim=1)\n",
        "#         attention_context = torch.bmm(attention_weights.unsqueeze(1), memory)\n",
        "#         attention_context = attention_context.squeeze(1)\n",
        "\n",
        "#         return attention_context, attention_weights\n",
        "\n",
        "\n",
        "class Prenet(nn.Module):\n",
        "    def __init__(self, in_dim, sizes):\n",
        "        super(Prenet, self).__init__()\n",
        "        in_sizes = [in_dim] + sizes[:-1]\n",
        "        self.layers = nn.ModuleList(\n",
        "            [LinearNorm(in_size, out_size, bias=False)\n",
        "             for (in_size, out_size) in zip(in_sizes, sizes)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for linear in self.layers:\n",
        "            x = F.dropout(F.relu(linear(x)), p=0.5, training=True)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Postnet(nn.Module):\n",
        "    \"\"\"Postnet\n",
        "        - Five 1-d convolution with 512 channels and kernel size 5\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hparams):\n",
        "        super(Postnet, self).__init__()\n",
        "        self.convolutions = nn.ModuleList()\n",
        "\n",
        "        self.convolutions.append(\n",
        "            nn.Sequential(\n",
        "                ConvNorm(hparams.n_mel_channels, hparams.postnet_embedding_dim,\n",
        "                         kernel_size=hparams.postnet_kernel_size, stride=1,\n",
        "                         padding=int((hparams.postnet_kernel_size - 1) / 2),\n",
        "                         dilation=1, w_init_gain='tanh'),\n",
        "                nn.BatchNorm1d(hparams.postnet_embedding_dim))\n",
        "        )\n",
        "\n",
        "        for i in range(1, hparams.postnet_n_convolutions - 1):\n",
        "            self.convolutions.append(\n",
        "                nn.Sequential(\n",
        "                    ConvNorm(hparams.postnet_embedding_dim,\n",
        "                             hparams.postnet_embedding_dim,\n",
        "                             kernel_size=hparams.postnet_kernel_size, stride=1,\n",
        "                             padding=int((hparams.postnet_kernel_size - 1) / 2),\n",
        "                             dilation=1, w_init_gain='tanh'),\n",
        "                    nn.BatchNorm1d(hparams.postnet_embedding_dim))\n",
        "            )\n",
        "\n",
        "        self.convolutions.append(\n",
        "            nn.Sequential(\n",
        "                ConvNorm(hparams.postnet_embedding_dim, hparams.n_mel_channels,\n",
        "                         kernel_size=hparams.postnet_kernel_size, stride=1,\n",
        "                         padding=int((hparams.postnet_kernel_size - 1) / 2),\n",
        "                         dilation=1, w_init_gain='linear'),\n",
        "                nn.BatchNorm1d(hparams.n_mel_channels))\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(len(self.convolutions) - 1):\n",
        "            x = F.dropout(torch.tanh(self.convolutions[i](x)), 0.5, self.training)\n",
        "        x = F.dropout(self.convolutions[-1](x), 0.5, self.training)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"Encoder module:\n",
        "        - Three 1-d convolution banks\n",
        "        - Bidirectional LSTM\n",
        "    \"\"\"\n",
        "    def __init__(self, hparams):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        # convolutions = []\n",
        "        # for _ in range(hparams.encoder_n_convolutions):\n",
        "        #     conv_layer = nn.Sequential(\n",
        "        #         ConvNorm(hparams.encoder_embedding_dim,\n",
        "        #                  hparams.encoder_embedding_dim,\n",
        "        #                  kernel_size=hparams.encoder_kernel_size, stride=1,\n",
        "        #                  padding=int((hparams.encoder_kernel_size - 1) / 2),\n",
        "        #                  dilation=1, w_init_gain='relu'),\n",
        "        #         nn.BatchNorm1d(hparams.encoder_embedding_dim))\n",
        "        #     convolutions.append(conv_layer)\n",
        "        # self.convolutions = nn.ModuleList(convolutions)\n",
        "\n",
        "        self.lstm = nn.LSTM(hparams.encoder_embedding_dim,\n",
        "                            int(hparams.encoder_embedding_dim/2), num_layers= 1, # According to paper it should be 8\n",
        "                            batch_first=True, bidirectional=True)\n",
        "\n",
        "    def forward(self, x, input_lengths):\n",
        "        # for conv in self.convolutions:\n",
        "        #     x = F.dropout(F.relu(conv(x)), 0.5, self.training)\n",
        "\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        # pytorch tensor are not reversible, hence the conversion\n",
        "        input_lengths = input_lengths.cpu().numpy()\n",
        "        x = nn.utils.rnn.pack_padded_sequence(\n",
        "            x, input_lengths, batch_first=True)\n",
        "\n",
        "        self.lstm.flatten_parameters()\n",
        "        outputs, _ = self.lstm(x)\n",
        "\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(\n",
        "            outputs, batch_first=True)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def inference(self, x):\n",
        "        # for conv in self.convolutions:\n",
        "        #     x = F.dropout(F.relu(conv(x)), 0.5, self.training)\n",
        "\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        self.lstm.flatten_parameters()\n",
        "        outputs, _ = self.lstm(x)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hparams):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.n_mel_channels = hparams.n_mel_channels\n",
        "        self.out_channels = hparams.out_channels\n",
        "        self.n_frames_per_step = hparams.n_frames_per_step\n",
        "        self.encoder_embedding_dim = hparams.encoder_embedding_dim\n",
        "        self.attention_rnn_dim = hparams.attention_rnn_dim\n",
        "        self.decoder_rnn_dim = hparams.decoder_rnn_dim\n",
        "        self.prenet_dim = hparams.prenet_dim\n",
        "        self.max_decoder_steps = hparams.max_decoder_steps\n",
        "        self.gate_threshold = hparams.gate_threshold\n",
        "        self.p_attention_dropout = hparams.p_attention_dropout\n",
        "        self.p_decoder_dropout = hparams.p_decoder_dropout\n",
        "        self.attention_dim = hparams.attention_dim\n",
        "\n",
        "        self.prenet = Prenet(\n",
        "            hparams.n_mel_channels * hparams.n_frames_per_step,\n",
        "            [hparams.prenet_dim, hparams.prenet_dim])\n",
        "\n",
        "        self.attention_rnn = nn.LSTMCell(\n",
        "            hparams.prenet_dim + hparams.encoder_embedding_dim,\n",
        "            hparams.attention_rnn_dim)\n",
        "\n",
        "        self.attention_layer = Attention(\n",
        "            hparams.attention_rnn_dim, hparams.encoder_embedding_dim,\n",
        "            hparams.attention_dim, hparams.attention_location_n_filters,\n",
        "            hparams.attention_location_kernel_size)\n",
        "\n",
        "        self.decoder_rnn = nn.LSTMCell(\n",
        "            hparams.attention_rnn_dim + hparams.encoder_embedding_dim,\n",
        "            hparams.decoder_rnn_dim, 1)\n",
        "\n",
        "        self.linear_projection = LinearNorm(\n",
        "            hparams.decoder_rnn_dim + hparams.encoder_embedding_dim,\n",
        "            hparams.n_mel_channels * hparams.n_frames_per_step)\n",
        "\n",
        "        self.gate_layer = LinearNorm(\n",
        "            hparams.decoder_rnn_dim + hparams.encoder_embedding_dim, 1,\n",
        "            bias=True, w_init_gain='sigmoid')\n",
        "\n",
        "    def get_go_frame(self, memory):\n",
        "        \"\"\" Gets all zeros frames to use as first decoder input\n",
        "        PARAMS\n",
        "        ------\n",
        "        memory: decoder outputs\n",
        "\n",
        "        RETURNS\n",
        "        -------\n",
        "        decoder_input: all zeros frames\n",
        "        \"\"\"\n",
        "        B = memory.size(0)\n",
        "        decoder_input = Variable(memory.data.new(\n",
        "            B, self.n_mel_channels * self.n_frames_per_step).zero_())\n",
        "        return decoder_input\n",
        "\n",
        "    def initialize_decoder_states(self, memory, mask):\n",
        "        \"\"\" Initializes attention rnn states, decoder rnn states, attention\n",
        "        weights, attention cumulative weights, attention context, stores memory\n",
        "        and stores processed memory\n",
        "        PARAMS\n",
        "        ------\n",
        "        memory: Encoder outputs\n",
        "        mask: Mask for padded data if training, expects None for inference\n",
        "        \"\"\"\n",
        "        B = memory.size(0)\n",
        "        MAX_TIME = memory.size(1)\n",
        "\n",
        "        self.attention_hidden = Variable(memory.data.new(\n",
        "            B, self.attention_rnn_dim).zero_())\n",
        "        self.attention_cell = Variable(memory.data.new(\n",
        "            B, self.attention_rnn_dim).zero_())\n",
        "\n",
        "        self.decoder_hidden = Variable(memory.data.new(\n",
        "            B, self.decoder_rnn_dim).zero_())\n",
        "        self.decoder_cell = Variable(memory.data.new(\n",
        "            B, self.decoder_rnn_dim).zero_())\n",
        "\n",
        "        self.attention_weights = Variable(memory.data.new(\n",
        "            B, MAX_TIME).zero_())\n",
        "        self.attention_weights_cum = Variable(memory.data.new(\n",
        "            B, MAX_TIME).zero_())\n",
        "        self.attention_context = Variable(memory.data.new(\n",
        "            B, self.encoder_embedding_dim).zero_())\n",
        "\n",
        "        self.memory = memory\n",
        "        self.processed_memory = self.attention_layer.memory_layer(memory)\n",
        "        self.mask = mask\n",
        "\n",
        "    def parse_decoder_inputs(self, decoder_inputs):\n",
        "        \"\"\" Prepares decoder inputs, i.e. mel outputs\n",
        "        PARAMS\n",
        "        ------\n",
        "        decoder_inputs: inputs used for teacher-forced training, i.e. mel-specs\n",
        "\n",
        "        RETURNS\n",
        "        -------\n",
        "        inputs: processed decoder inputs\n",
        "\n",
        "        \"\"\"\n",
        "        # (B, n_mel_channels, T_out) -> (B, T_out, n_mel_channels)\n",
        "        decoder_inputs = decoder_inputs.transpose(1, 2)\n",
        "        decoder_inputs = decoder_inputs.view(\n",
        "            decoder_inputs.size(0),\n",
        "            int(decoder_inputs.size(1)/self.n_frames_per_step), -1)\n",
        "        # (B, T_out, n_mel_channels) -> (T_out, B, n_mel_channels)\n",
        "        decoder_inputs = decoder_inputs.transpose(0, 1)\n",
        "        return decoder_inputs\n",
        "\n",
        "    def parse_decoder_outputs(self, mel_outputs, gate_outputs, alignments):\n",
        "        \"\"\" Prepares decoder outputs for output\n",
        "        PARAMS\n",
        "        ------\n",
        "        mel_outputs:\n",
        "        gate_outputs: gate output energies\n",
        "        alignments:\n",
        "\n",
        "        RETURNS\n",
        "        -------\n",
        "        mel_outputs:\n",
        "        gate_outpust: gate output energies\n",
        "        alignments:\n",
        "        \"\"\"\n",
        "        # (T_out, B) -> (B, T_out)\n",
        "        alignments = torch.stack(alignments).transpose(0, 1)\n",
        "        # (T_out, B) -> (B, T_out)\n",
        "        gate_outputs = torch.stack(gate_outputs).transpose(0, 1)\n",
        "        gate_outputs = gate_outputs.contiguous()\n",
        "        # (T_out, B, n_mel_channels) -> (B, T_out, n_mel_channels)\n",
        "        mel_outputs = torch.stack(mel_outputs).transpose(0, 1).contiguous()\n",
        "        # decouple frames per step\n",
        "        mel_outputs = mel_outputs.view(\n",
        "            mel_outputs.size(0), -1, self.n_mel_channels)\n",
        "        # (B, T_out, n_mel_channels) -> (B, n_mel_channels, T_out)\n",
        "        mel_outputs = mel_outputs.transpose(1, 2)\n",
        "\n",
        "        return mel_outputs, gate_outputs, alignments\n",
        "\n",
        "    def decode(self, decoder_input):\n",
        "        \"\"\" Decoder step using stored states, attention and memory\n",
        "        PARAMS\n",
        "        ------\n",
        "        decoder_input: previous mel output\n",
        "\n",
        "        RETURNS\n",
        "        -------\n",
        "        mel_output:\n",
        "        gate_output: gate output energies\n",
        "        attention_weights:\n",
        "        \"\"\"\n",
        "        cell_input = torch.cat((decoder_input, self.attention_context), -1)\n",
        "        self.attention_hidden, self.attention_cell = self.attention_rnn(\n",
        "            cell_input, (self.attention_hidden, self.attention_cell))\n",
        "        self.attention_hidden = F.dropout(\n",
        "            self.attention_hidden, self.p_attention_dropout, self.training)\n",
        "\n",
        "        attention_weights_cat = torch.cat(\n",
        "            (self.attention_weights.unsqueeze(1),\n",
        "              self.attention_weights_cum.unsqueeze(1)), dim=1)\n",
        "        self.attention_context, self.attention_weights = self.attention_layer(\n",
        "            self.attention_hidden, self.memory, self.processed_memory,\n",
        "            attention_weights_cat, self.mask)\n",
        "\n",
        "        # print(self.attention_context.shape,self.attention_weights.shape)\n",
        "        self.attention_weights_cum += self.attention_weights\n",
        "        decoder_input = torch.cat(\n",
        "            (self.attention_hidden, self.attention_context), -1)\n",
        "        self.decoder_hidden, self.decoder_cell = self.decoder_rnn(\n",
        "            decoder_input, (self.decoder_hidden, self.decoder_cell))\n",
        "        self.decoder_hidden = F.dropout(\n",
        "            self.decoder_hidden, self.p_decoder_dropout, self.training)\n",
        "\n",
        "        decoder_hidden_attention_context = torch.cat(\n",
        "            (self.decoder_hidden, self.attention_context), dim=1)\n",
        "        decoder_output = self.linear_projection(\n",
        "            decoder_hidden_attention_context)\n",
        "\n",
        "        gate_prediction = self.gate_layer(decoder_hidden_attention_context)\n",
        "        return decoder_output, gate_prediction, self.attention_weights\n",
        "\n",
        "    def forward(self, memory,decoder_inputs,memory_lengths,output_lengths):\n",
        "        \"\"\" Decoder forward pass for training\n",
        "        PARAMS\n",
        "        ------\n",
        "        memory: Encoder outputs\n",
        "        decoder_inputs: Decoder inputs for teacher forcing. i.e. mel-specs\n",
        "        memory_lengths: Encoder output lengths for attention masking.\n",
        "\n",
        "        RETURNS\n",
        "        -------\n",
        "        mel_outputs: mel outputs from the decoder\n",
        "        gate_outputs: gate outputs from the decoder\n",
        "        alignments: sequence of attention weights from the decoder\n",
        "        \"\"\"\n",
        "        # print(memory.shape)\n",
        "        decoder_input = self.get_go_frame(memory).unsqueeze(0)\n",
        "        decoder_inputs = self.parse_decoder_inputs(decoder_inputs)\n",
        "        decoder_inputs = torch.cat((decoder_input, decoder_inputs), dim=0)\n",
        "        decoder_inputs = self.prenet(decoder_inputs)\n",
        "        self.initialize_decoder_states(\n",
        "            memory, mask=~get_mask_from_lengths(memory_lengths))\n",
        "        # print(decoder_inputs.shape)\n",
        "        mel_outputs, gate_outputs, alignments = [], [], []\n",
        "        while len(mel_outputs) < decoder_inputs.size(0) - 1:\n",
        "            decoder_input = decoder_inputs[len(mel_outputs)]\n",
        "            mel_output, gate_output, attention_weights = self.decode(\n",
        "                decoder_input)\n",
        "            mel_outputs += [mel_output.squeeze(1)]\n",
        "            gate_outputs += [gate_output.squeeze(1)]\n",
        "            alignments += [attention_weights]\n",
        "\n",
        "        mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
        "            mel_outputs, gate_outputs, alignments)\n",
        "\n",
        "        # print(mel_outputs.shape)\n",
        "        return mel_outputs, gate_outputs, alignments\n",
        "\n",
        "    def inference(self, memory):\n",
        "        \"\"\" Decoder inference\n",
        "        PARAMS\n",
        "        ------\n",
        "        memory: Encoder outputs\n",
        "\n",
        "        RETURNS\n",
        "        -------\n",
        "        mel_outputs: mel outputs from the decoder\n",
        "        gate_outputs: gate outputs from the decoder\n",
        "        alignments: sequence of attention weights from the decoder\n",
        "        \"\"\"\n",
        "        decoder_input = self.get_go_frame(memory)\n",
        "\n",
        "        self.initialize_decoder_states(memory, mask=None)\n",
        "\n",
        "        mel_outputs, gate_outputs, alignments = [], [], []\n",
        "        while True:\n",
        "            decoder_input = self.prenet(decoder_input)\n",
        "            mel_output, gate_output, alignment = self.decode(decoder_input)\n",
        "\n",
        "            mel_outputs += [mel_output.squeeze(1)]\n",
        "            gate_outputs += [gate_output]\n",
        "            alignments += [alignment]\n",
        "\n",
        "            if torch.sigmoid(gate_output.data) > self.gate_threshold:\n",
        "                break\n",
        "            elif len(mel_outputs) == self.max_decoder_steps:\n",
        "                print(\"Warning! Reached max decoder steps\")\n",
        "                break\n",
        "\n",
        "            decoder_input = mel_output\n",
        "\n",
        "        mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
        "            mel_outputs, gate_outputs, alignments)\n",
        "\n",
        "        return mel_outputs, gate_outputs, alignments\n",
        "\n",
        "\n",
        "class Tacotron2(nn.Module):\n",
        "    def __init__(self, hparams):\n",
        "        super(Tacotron2, self).__init__()\n",
        "        self.mask_padding = hparams.mask_padding\n",
        "        self.fp16_run = hparams.fp16_run\n",
        "        self.n_mel_channels = hparams.n_mel_channels\n",
        "        self.n_frames_per_step = hparams.n_frames_per_step\n",
        "        # self.embedding = nn.Embedding(\n",
        "        #     hparams.n_symbols, hparams.symbols_embedding_dim)\n",
        "        self.linear = nn.Linear(hparams.n_mel_channels,hparams.encoder_embedding_dim)\n",
        "        # std = sqrt(2.0 / (hparams.n_symbols + hparams.symbols_embedding_dim))\n",
        "        # val = sqrt(3.0) * std  # uniform bounds for std\n",
        "        # self.embedding.weight.data.uniform_(-val, val)\n",
        "        self.encoder = Encoder(hparams)\n",
        "        self.decoder = Decoder(hparams)\n",
        "        self.postnet = Postnet(hparams)\n",
        "\n",
        "    def parse_batch(self, batch):\n",
        "        # # text_padded, input_lengths, mel_padded, gate_padded, \\\n",
        "        # #     output_lengths = batch\n",
        "        # mels, gate,input_lengths, spec, output_lengths = batch\n",
        "        # mels = to_gpu(mels).float()\n",
        "        # gate = to_gpu(gate).float()\n",
        "        # input_lengths = to_gpu(input_lengths).long()\n",
        "        # inp_len = torch.max(input_lengths.data).item()\n",
        "        # spec = to_gpu(spec).float()\n",
        "        # out_lengths = to_gpu(out_lenghts).long()\n",
        "        # out_len = torch.max(out_lengths).item()\n",
        "        # # gate_padded = to_gpu(gate_padded).float()\n",
        "        # # output_lengths = to_gpu(output_lengths).long()\n",
        "\n",
        "        # return (\n",
        "        #     (mels,gate,input_lengths,max_len,spec,output_lengths,out_len),\n",
        "        #     (spec,output_lengths,out_len))\n",
        "        input_padded, input_lengths, mel_padded, gate_padded, \\\n",
        "            output_lengths = batch\n",
        "        input_padded = to_gpu(input_padded).float()\n",
        "        input_lengths = to_gpu(input_lengths).long()\n",
        "        max_len = torch.max(input_lengths.data).item()\n",
        "        mel_padded = to_gpu(mel_padded).float()\n",
        "        gate_padded = to_gpu(gate_padded).float()\n",
        "        output_lengths = to_gpu(output_lengths).long()\n",
        "\n",
        "        return (\n",
        "            (input_padded, input_lengths, mel_padded, max_len, output_lengths),\n",
        "            (mel_padded, gate_padded))\n",
        "\n",
        "    def parse_output(self, outputs, output_lengths=None):\n",
        "        if self.mask_padding and output_lengths is not None:\n",
        "            mask = ~get_mask_from_lengths(output_lengths)\n",
        "            mask = mask.expand(self.n_mel_channels, mask.size(0), mask.size(1))\n",
        "            mask = mask.permute(1, 0, 2)\n",
        "\n",
        "            outputs[0].data.masked_fill_(mask, 0.0)\n",
        "            outputs[1].data.masked_fill_(mask, 0.0)\n",
        "            outputs[2].data.masked_fill_(mask[:, 0, :], 1e3)  # gate energies\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # mels,gate,input_lengths,max_len,spec,output_lengths,out_len = inputs\n",
        "        # mels, input_lengths = mels.data, input_lengths.data\n",
        "\n",
        "        # # embedded_inputs = self.embedding(text_inputs).transpose(1, 2)\n",
        "\n",
        "        # encoder_outputs = self.encoder(mels, input_lengths)\n",
        "\n",
        "        # mel_outputs, gate_outputs, alignments = self.decoder(\n",
        "        #     encoder_outputs, spec,input_lengths,output_lengths)\n",
        "\n",
        "        # mel_outputs_postnet = self.postnet(mel_outputs)\n",
        "        # mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
        "\n",
        "        # return self.parse_output(\n",
        "        #     [mel_outputs, mel_outputs_postnet, gate_outputs, alignments],\n",
        "        #     output_lengths)\n",
        "        inputs, input_lengths, mels, max_len, output_lengths = inputs\n",
        "        input_lengths, output_lengths = input_lengths.data, output_lengths.data\n",
        "\n",
        "        embedded_inputs = self.linear(inputs.transpose(1,2)).transpose(1, 2)\n",
        "\n",
        "        encoder_outputs = self.encoder(embedded_inputs, input_lengths)\n",
        "        # print(encoder_outputs.shape)\n",
        "        mel_outputs, gate_outputs, alignments = self.decoder(\n",
        "            encoder_outputs, mels, input_lengths,output_lengths)\n",
        "\n",
        "        mel_outputs_postnet = self.postnet(mel_outputs)\n",
        "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
        "\n",
        "        return self.parse_output(\n",
        "            [mel_outputs, mel_outputs_postnet, gate_outputs, alignments],\n",
        "            output_lengths)\n",
        "\n",
        "    def inference(self, inputs):\n",
        "        # embedded_inputs = self.embedding(inputs).transpose(1, 2)\n",
        "        encoder_outputs = self.encoder.inference(inputs)\n",
        "        mel_outputs, gate_outputs, alignments = self.decoder.inference(\n",
        "            encoder_outputs)\n",
        "\n",
        "        mel_outputs_postnet = self.postnet(mel_outputs)\n",
        "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
        "\n",
        "        outputs = self.parse_output(\n",
        "            [mel_outputs, mel_outputs_postnet, gate_outputs, alignments])\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "MUa1TRUwQDhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## loss_function.py\n",
        "class Tacotron2Loss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Tacotron2Loss, self).__init__()\n",
        "\n",
        "    def forward(self, model_output, targets):\n",
        "        mel_target, gate_target = targets[0], targets[1]\n",
        "        mel_target.requires_grad = False\n",
        "        gate_target.requires_grad = False\n",
        "        gate_target = gate_target.view(-1, 1)\n",
        "\n",
        "        mel_out, mel_out_postnet, gate_out, _ = model_output\n",
        "        gate_out = gate_out.view(-1, 1)\n",
        "        mel_loss = nn.MSELoss()(mel_out, mel_target) + \\\n",
        "            nn.MSELoss()(mel_out_postnet, mel_target)\n",
        "        gate_loss = nn.BCEWithLogitsLoss()(gate_out, gate_target)\n",
        "        return mel_loss + gate_loss"
      ],
      "metadata": {
        "id": "1PBatUCaSBNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## loss_scaler.py\n",
        "\n",
        "import torch\n",
        "\n",
        "class LossScaler:\n",
        "\n",
        "    def __init__(self, scale=1):\n",
        "        self.cur_scale = scale\n",
        "\n",
        "    # `params` is a list / generator of torch.Variable\n",
        "    def has_overflow(self, params):\n",
        "        return False\n",
        "\n",
        "    # `x` is a torch.Tensor\n",
        "    def _has_inf_or_nan(x):\n",
        "        return False\n",
        "\n",
        "    # `overflow` is boolean indicating whether we overflowed in gradient\n",
        "    def update_scale(self, overflow):\n",
        "        pass\n",
        "\n",
        "    @property\n",
        "    def loss_scale(self):\n",
        "        return self.cur_scale\n",
        "\n",
        "    def scale_gradient(self, module, grad_in, grad_out):\n",
        "        return tuple(self.loss_scale * g for g in grad_in)\n",
        "\n",
        "    def backward(self, loss):\n",
        "        scaled_loss = loss*self.loss_scale\n",
        "        scaled_loss.backward()\n",
        "\n",
        "class DynamicLossScaler:\n",
        "\n",
        "    def __init__(self,\n",
        "                 init_scale=2**32,\n",
        "                 scale_factor=2.,\n",
        "                 scale_window=1000):\n",
        "        self.cur_scale = init_scale\n",
        "        self.cur_iter = 0\n",
        "        self.last_overflow_iter = -1\n",
        "        self.scale_factor = scale_factor\n",
        "        self.scale_window = scale_window\n",
        "\n",
        "    # `params` is a list / generator of torch.Variable\n",
        "    def has_overflow(self, params):\n",
        "#        return False\n",
        "        for p in params:\n",
        "            if p.grad is not None and DynamicLossScaler._has_inf_or_nan(p.grad.data):\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    # `x` is a torch.Tensor\n",
        "    def _has_inf_or_nan(x):\n",
        "        cpu_sum = float(x.float().sum())\n",
        "        if cpu_sum == float('inf') or cpu_sum == -float('inf') or cpu_sum != cpu_sum:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    # `overflow` is boolean indicating whether we overflowed in gradient\n",
        "    def update_scale(self, overflow):\n",
        "        if overflow:\n",
        "            #self.cur_scale /= self.scale_factor\n",
        "            self.cur_scale = max(self.cur_scale/self.scale_factor, 1)\n",
        "            self.last_overflow_iter = self.cur_iter\n",
        "        else:\n",
        "            if (self.cur_iter - self.last_overflow_iter) % self.scale_window == 0:\n",
        "                self.cur_scale *= self.scale_factor\n",
        "#        self.cur_scale = 1\n",
        "        self.cur_iter += 1\n",
        "\n",
        "    @property\n",
        "    def loss_scale(self):\n",
        "        return self.cur_scale\n",
        "\n",
        "    def scale_gradient(self, module, grad_in, grad_out):\n",
        "        return tuple(self.loss_scale * g for g in grad_in)\n",
        "\n",
        "    def backward(self, loss):\n",
        "        scaled_loss = loss*self.loss_scale\n",
        "        scaled_loss.backward()"
      ],
      "metadata": {
        "id": "uJcbsw0USHQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hparams = create_hparams()\n",
        "\n",
        "torch.manual_seed(hparams.seed)\n",
        "torch.cuda.manual_seed(hparams.seed)"
      ],
      "metadata": {
        "id": "FYMEpEy1QpDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = '/content/drive/MyDrive/Dubbing Project/speech2speechData /never_have/wav_files/hindi'\n",
        "output_path = '/content/drive/MyDrive/Dubbing Project/speech2speechData /never_have/wav_files/english'\n",
        "\n",
        "t_input_files = [os.path.join(input_path, file_name) for file_name in os.listdir(input_path)[:10]]\n",
        "t_output_files = [os.path.join(output_path, file_name) for file_name in os.listdir(output_path)[:10]]\n",
        "\n",
        "v_input_files = [os.path.join(input_path, file_name) for file_name in os.listdir(input_path)[15:20]]\n",
        "v_output_files = [os.path.join(output_path, file_name) for file_name in os.listdir(output_path)[15:20]]"
      ],
      "metadata": {
        "id": "28urjRNYReA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = TextMelLoader((t_input_files, t_output_files), hparams)\n",
        "valset = TextMelLoader((v_input_files, v_output_files), hparams)\n",
        "\n",
        "collate_fn = TextMelCollate(hparams.n_frames_per_step)\n",
        "\n",
        "train_loader = DataLoader(trainset, num_workers=2, shuffle=True,\n",
        "                          batch_size=hparams.batch_size, pin_memory=False,\n",
        "                          drop_last=True, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "f1lv6SEYS_22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxCadTlw8Uow",
        "outputId": "db30a849-a523-4932-ab0f-9550b29e86e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 2.8684e+00,  2.5318e+00,  1.7466e+00,  ...,  2.8774e+00,\n",
              "           2.8729e+00,  3.4255e+00],\n",
              "         [ 3.1703e+00,  3.2526e+00,  3.6137e+00,  ...,  3.7573e+00,\n",
              "           4.1325e+00,  3.9588e+00],\n",
              "         [ 2.8741e+00,  3.3548e+00,  3.5645e+00,  ...,  3.4109e+00,\n",
              "           4.5083e+00,  4.8201e+00],\n",
              "         ...,\n",
              "         [-1.0485e+00, -8.5859e-01, -4.9844e-01,  ..., -7.8928e-02,\n",
              "          -2.1793e-01, -5.3728e-01],\n",
              "         [-1.1455e+00, -8.2102e-01, -6.4068e-01,  ...,  4.9775e-02,\n",
              "          -1.6804e-04, -1.9076e-01],\n",
              "         [-1.1992e+00, -6.7772e-01, -4.6821e-01,  ..., -2.6285e-01,\n",
              "          -3.8602e-01, -5.0477e-01]]),\n",
              " tensor([[-3.0442, -3.7131, -6.1225,  ..., -7.6227, -4.9524, -4.2499],\n",
              "         [-2.9478, -3.7208, -4.4675,  ..., -6.4821, -4.6230, -3.9773],\n",
              "         [-2.9164, -2.9182, -2.7841,  ..., -5.6637, -4.5555, -3.9699],\n",
              "         ...,\n",
              "         [-8.6789, -8.5417, -8.0932,  ..., -9.0226, -8.8471, -8.7186],\n",
              "         [-8.9122, -8.7363, -8.4004,  ..., -9.1009, -8.9522, -8.5598],\n",
              "         [-8.4188, -8.4643, -8.2133,  ..., -9.1792, -9.2624, -9.1135]]))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Tacotron2(hparams).cuda()\n",
        "spec_to_emb_linear = model.linear\n",
        "encoder = model.encoder"
      ],
      "metadata": {
        "id": "8tWi9-UZ1f15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spec_to_emb_linear = nn.Linear(hparams.n_mel_channels,hparams.encoder_embedding_dim)\n",
        "encoder_lstm = nn.LSTM(hparams.encoder_embedding_dim,\n",
        "                            int(hparams.encoder_embedding_dim/2), num_layers= 8,\n",
        "                            batch_first=True, bidirectional=True)"
      ],
      "metadata": {
        "id": "Ka8xFbIe_xgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hparams.prenet_dim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNe2zXX1qQRI",
        "outputId": "d035d198-4e88-4fca-bb2d-30cd62b4c0ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prenet = Prenet(\n",
        "            hparams.n_mel_channels * hparams.n_frames_per_step,\n",
        "            [hparams.prenet_dim, hparams.prenet_dim]).cuda()\n",
        "\n",
        "attention_layer = Attention(\n",
        "            hparams.attention_rnn_dim, hparams.encoder_embedding_dim,\n",
        "            hparams.attention_dim, hparams.attention_location_n_filters,\n",
        "            hparams.attention_location_kernel_size).cuda()\n",
        "\n",
        "postnet = Postnet(hparams).cuda()"
      ],
      "metadata": {
        "id": "lxrSGsAGqIzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_go_frame(memory):\n",
        "    \"\"\" Gets all zeros frames to use as first decoder input\n",
        "    PARAMS\n",
        "    ------\n",
        "    memory: decoder outputs\n",
        "\n",
        "    RETURNS\n",
        "    -------\n",
        "    decoder_input: all zeros frames\n",
        "    \"\"\"\n",
        "    B = memory.size(0)\n",
        "    decoder_input = Variable(memory.data.new(\n",
        "        B, hparams.n_mel_channels * hparams.n_frames_per_step).zero_())\n",
        "    return decoder_input\n",
        "\n",
        "def parse_decoder_inputs(decoder_inputs):\n",
        "    \"\"\" Prepares decoder inputs, i.e. mel outputs\n",
        "    PARAMS\n",
        "    ------\n",
        "    decoder_inputs: inputs used for teacher-forced training, i.e. mel-specs\n",
        "\n",
        "    RETURNS\n",
        "    -------\n",
        "    inputs: processed decoder inputs\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # (B, n_mel_channels, T_out) -> (B, T_out, n_mel_channels)\n",
        "    decoder_inputs = decoder_inputs.transpose(1, 2)\n",
        "    decoder_inputs = decoder_inputs.view(\n",
        "        decoder_inputs.size(0),\n",
        "        int(decoder_inputs.size(1)/hparams.n_frames_per_step), -1)\n",
        "    # (B, T_out, n_mel_channels) -> (T_out, B, n_mel_channels)\n",
        "    decoder_inputs = decoder_inputs.transpose(0, 1)\n",
        "    return decoder_inputs"
      ],
      "metadata": {
        "id": "W2QHLBKpn0rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_rnn = nn.LSTMCell(\n",
        "    hparams.prenet_dim + hparams.encoder_embedding_dim,\n",
        "    hparams.attention_rnn_dim).cuda()\n",
        "\n",
        "attention_layer = Attention(\n",
        "    hparams.attention_rnn_dim, hparams.encoder_embedding_dim,\n",
        "    hparams.attention_dim, hparams.attention_location_n_filters,\n",
        "    hparams.attention_location_kernel_size).cuda()\n",
        "\n",
        "decoder_rnn = nn.LSTMCell(\n",
        "    hparams.attention_rnn_dim + hparams.encoder_embedding_dim,\n",
        "    hparams.decoder_rnn_dim, 1).cuda()\n",
        "\n",
        "linear_projection = LinearNorm(\n",
        "    hparams.decoder_rnn_dim + hparams.encoder_embedding_dim,\n",
        "    hparams.n_mel_channels * hparams.n_frames_per_step).cuda()\n",
        "\n",
        "gate_layer = LinearNorm(\n",
        "    hparams.decoder_rnn_dim + hparams.encoder_embedding_dim, 1,\n",
        "    bias=True, w_init_gain='sigmoid').cuda()\n"
      ],
      "metadata": {
        "id": "2xAu017PW3IH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_decoder_outputs(mel_outputs, gate_outputs, alignments):\n",
        "    \"\"\" Prepares decoder outputs for output\n",
        "    PARAMS\n",
        "    ------\n",
        "    mel_outputs:\n",
        "    gate_outputs: gate output energies\n",
        "    alignments:\n",
        "\n",
        "    RETURNS\n",
        "    -------\n",
        "    mel_outputs:\n",
        "    gate_outpust: gate output energies\n",
        "    alignments:\n",
        "    \"\"\"\n",
        "    # (T_out, B) -> (B, T_out)\n",
        "    alignments = torch.stack(alignments).transpose(0, 1)\n",
        "    # (T_out, B) -> (B, T_out)\n",
        "    gate_outputs = torch.stack(gate_outputs).transpose(0, 1)\n",
        "    gate_outputs = gate_outputs.contiguous()\n",
        "    # (T_out, B, n_mel_channels) -> (B, T_out, n_mel_channels)\n",
        "    mel_outputs = torch.stack(mel_outputs).transpose(0, 1).contiguous()\n",
        "    # decouple frames per step\n",
        "    mel_outputs = mel_outputs.view(\n",
        "        mel_outputs.size(0), -1, hparams.n_mel_channels)\n",
        "    # (B, T_out, n_mel_channels) -> (B, n_mel_channels, T_out)\n",
        "    mel_outputs = mel_outputs.transpose(1, 2)\n",
        "\n",
        "    return mel_outputs, gate_outputs, alignments"
      ],
      "metadata": {
        "id": "7oIupC0PZ-KO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_output(outputs, output_lengths=None):\n",
        "    if hparams.mask_padding and output_lengths is not None:\n",
        "        mask = ~get_mask_from_lengths(output_lengths)\n",
        "        mask = mask.expand(hparams.n_mel_channels, mask.size(0), mask.size(1))\n",
        "        mask = mask.permute(1, 0, 2)\n",
        "\n",
        "        outputs[0].data.masked_fill_(mask, 0.0)\n",
        "        outputs[1].data.masked_fill_(mask, 0.0)\n",
        "        outputs[2].data.masked_fill_(mask[:, 0, :], 1e3)  # gate energies\n",
        "\n",
        "    return outputs\n"
      ],
      "metadata": {
        "id": "Qaz8Ev31eUo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, batch in enumerate(train_loader):\n",
        "    inputs, input_lengths, mels, max_len, output_lengths = batch\n",
        "\n",
        "    input_lengths, output_lengths = input_lengths.data, output_lengths.data\n",
        "\n",
        "    # print(\"Shape of Input: \", inputs.shape, input_lengths, mels.shape, max_len.shape, output_lengths)\n",
        "\n",
        "    # print(\"Encoder Dimension: \", hparams.encoder_embedding_dim)\n",
        "\n",
        "    # print(inputs.transpose(1,2).shape)\n",
        "    # embedded_inputs = spec_to_emb_linear(inputs.transpose(1,2)).transpose(1, 2)\n",
        "\n",
        "    # print(\"Inputs after Linear Layer: \", embedded_inputs.shape)\n",
        "\n",
        "    # embedded_inputs = embedded_inputs.transpose(1, 2)\n",
        "\n",
        "    # # pytorch tensor are not reversible, hence the conversion\n",
        "    # input_lengths = input_lengths.cpu().numpy()\n",
        "    # embedded_inputs = nn.utils.rnn.pack_padded_sequence(\n",
        "    #     embedded_inputs, input_lengths, batch_first=True)\n",
        "\n",
        "    # print(\"Padded Sequence: \", embedded_inputs.data.shape)\n",
        "\n",
        "    # encoder_lstm.flatten_parameters()\n",
        "\n",
        "    # outputs, _ = encoder_lstm(embedded_inputs)\n",
        "\n",
        "    # ## Pad the sequences to max lenght\n",
        "    # outputs, _ = nn.utils.rnn.pad_packed_sequence(\n",
        "    #     outputs, batch_first=True)\n",
        "\n",
        "    # print(\"Padded Sequence after Outputs: \", outputs.data.shape)\n",
        "\n",
        "    input_lengths, output_lengths = input_lengths.data, output_lengths.data\n",
        "\n",
        "    embedded_inputs = spec_to_emb_linear(inputs.transpose(1,2)).transpose(1, 2)\n",
        "\n",
        "    encoder_outputs = encoder(embedded_inputs.cuda(), input_lengths)\n",
        "\n",
        "    print(\"Encoder Outputs Shape: \", encoder_outputs.shape)\n",
        "    ## get_go_frame() -- get the zeros values of encoder in\n",
        "    decoder_input = get_go_frame(encoder_outputs).unsqueeze(0)\n",
        "\n",
        "    print(\"Decoder Input Array: \", decoder_input, decoder_input.shape)\n",
        "\n",
        "    print(\"Output Shape before parsing: \", mels.shape)\n",
        "    decoder_inputs = parse_decoder_inputs(mels)\n",
        "    print(\"Output shape after parsing: \", decoder_inputs.shape)\n",
        "\n",
        "    decoder_inputs = torch.cat((decoder_input.cuda(), decoder_inputs.cuda()), dim=0)\n",
        "\n",
        "    print(\"Decoder Shape after Concating: \", decoder_inputs.shape)\n",
        "\n",
        "    ## Prenet\n",
        "    decoder_inputs = prenet(decoder_inputs)\n",
        "\n",
        "    print(\"Decoder Shape after PreNet: \", decoder_inputs.shape)\n",
        "\n",
        "    memory = encoder_outputs.cuda()\n",
        "    mask=~get_mask_from_lengths(input_lengths.cuda())\n",
        "\n",
        "    B = memory.size(0)\n",
        "    MAX_TIME = memory.size(1)\n",
        "\n",
        "    attention_hidden = Variable(memory.data.new(\n",
        "        B, hparams.attention_rnn_dim).zero_())\n",
        "    print(\"Shape of attention_hidden: \", attention_hidden.shape)\n",
        "\n",
        "    attention_cell = Variable(memory.data.new(\n",
        "        B, hparams.attention_rnn_dim).zero_())\n",
        "    print(\"Shape of attention_cell: \", attention_cell.shape)\n",
        "\n",
        "    decoder_hidden = Variable(memory.data.new(\n",
        "        B, hparams.decoder_rnn_dim).zero_())\n",
        "    print(\"Shape of decoder_hidden: \", decoder_hidden.shape)\n",
        "\n",
        "    decoder_cell = Variable(memory.data.new(\n",
        "        B, hparams.decoder_rnn_dim).zero_())\n",
        "    print(\"Shape of decoder_cell: \", decoder_cell.shape)\n",
        "\n",
        "    attention_weights = Variable(memory.data.new(\n",
        "        B, MAX_TIME).zero_())\n",
        "    print(\"Shape of attention_weights: \", attention_weights.shape)\n",
        "\n",
        "    attention_weights_cum = Variable(memory.data.new(\n",
        "        B, MAX_TIME).zero_())\n",
        "    print(\"Shape of attention_weights_cum: \", attention_weights_cum.shape)\n",
        "\n",
        "    attention_context = Variable(memory.data.new(\n",
        "        B, hparams.encoder_embedding_dim).zero_())\n",
        "    print(\"Shape of attention_context: \", attention_context.shape)\n",
        "\n",
        "    memory = memory\n",
        "    processed_memory = attention_layer.memory_layer(memory)\n",
        "\n",
        "    mel_outputs, gate_outputs, alignments = [], [], []\n",
        "\n",
        "    while len(mel_outputs) < decoder_inputs.size(0) - 1:\n",
        "        decoder_input = decoder_inputs[len(mel_outputs)]\n",
        "\n",
        "        cell_input = torch.cat((decoder_input, attention_context), -1)\n",
        "\n",
        "        attention_hidden, attention_cell = attention_rnn(\n",
        "            cell_input, (attention_hidden, attention_cell))\n",
        "        attention_hidden = F.dropout(\n",
        "            attention_hidden, hparams.p_attention_dropout, True)\n",
        "\n",
        "        attention_weights_cat = torch.cat(\n",
        "            (attention_weights.unsqueeze(1),\n",
        "              attention_weights_cum.unsqueeze(1)), dim=1)\n",
        "        attention_context, attention_weights = attention_layer(\n",
        "            attention_hidden, memory, processed_memory,\n",
        "            attention_weights_cat, mask)\n",
        "\n",
        "        # print(self.attention_context.shape,self.attention_weights.shape)\n",
        "        attention_weights_cum += attention_weights\n",
        "        decoder_input = torch.cat(\n",
        "            (attention_hidden, attention_context), -1)\n",
        "        decoder_hidden, decoder_cell = decoder_rnn(\n",
        "            decoder_input, (decoder_hidden, decoder_cell))\n",
        "        decoder_hidden = F.dropout(\n",
        "            decoder_hidden, hparams.p_decoder_dropout, True)\n",
        "\n",
        "        decoder_hidden_attention_context = torch.cat(\n",
        "            (decoder_hidden, attention_context), dim=1)\n",
        "        decoder_output = linear_projection(\n",
        "            decoder_hidden_attention_context)\n",
        "\n",
        "        gate_prediction = gate_layer(decoder_hidden_attention_context)\n",
        "        mel_outputs += [decoder_output.squeeze(1)]\n",
        "        gate_outputs += [gate_prediction.squeeze(1)]\n",
        "        alignments += [attention_weights]\n",
        "\n",
        "        decoder_input = decoder_output\n",
        "\n",
        "    print(\"Outputs before parsing: \")\n",
        "    print(\"Mel Outputs: \", len(mel_outputs), mel_outputs[0].shape)\n",
        "    print(\"Gate Outputs: \", len(gate_outputs), gate_outputs[0].shape)\n",
        "    print(\"Alignments: \", len(alignments), alignments[0].shape)\n",
        "\n",
        "    mel_outputs, gate_outputs, alignments = parse_decoder_outputs(\n",
        "            mel_outputs, gate_outputs, alignments)\n",
        "\n",
        "    print(\"Outputs after parsing: \")\n",
        "    print(\"Mel Outputs: \", len(mel_outputs), mel_outputs[0].shape)\n",
        "    print(\"Gate Outputs: \", len(gate_outputs), gate_outputs[0].shape)\n",
        "    print(\"Alignments: \", len(alignments), alignments[0].shape)\n",
        "\n",
        "    mel_outputs_postnet = postnet(mel_outputs)\n",
        "    mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
        "\n",
        "    [mel_outputs, mel_outputs_postnet, gate_outputs, alignments] = parse_output(\n",
        "                [mel_outputs, mel_outputs_postnet, gate_outputs, alignments],\n",
        "                output_lengths.cuda())\n",
        "    break"
      ],
      "metadata": {
        "id": "xnZmOQ3yV_GX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa54cad5-9e48-4753-ef6d-a5f4d6a16723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder Outputs Shape:  torch.Size([1, 1876, 128])\n",
            "Decoder Input Array:  tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]], device='cuda:0') torch.Size([1, 1, 80])\n",
            "Output Shape before parsing:  torch.Size([1, 80, 1876])\n",
            "Output shape after parsing:  torch.Size([1876, 1, 80])\n",
            "Decoder Shape after Concating:  torch.Size([1877, 1, 80])\n",
            "Decoder Shape after PreNet:  torch.Size([1877, 1, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-2980d42609fc>:5: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  ids = torch.arange(0, max_len, out=torch.cuda.LongTensor(max_len))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of attention_hidden:  torch.Size([1, 256])\n",
            "Shape of attention_cell:  torch.Size([1, 256])\n",
            "Shape of decoder_hidden:  torch.Size([1, 256])\n",
            "Shape of decoder_cell:  torch.Size([1, 256])\n",
            "Shape of attention_weights:  torch.Size([1, 1876])\n",
            "Shape of attention_weights_cum:  torch.Size([1, 1876])\n",
            "Shape of attention_context:  torch.Size([1, 128])\n",
            "Outputs before parsing: \n",
            "Mel Outputs:  1876 torch.Size([1, 80])\n",
            "Gate Outputs:  1876 torch.Size([1])\n",
            "Alignments:  1876 torch.Size([1, 1876])\n",
            "Outputs after parsing: \n",
            "Mel Outputs:  1 torch.Size([80, 1876])\n",
            "Gate Outputs:  1 torch.Size([1876])\n",
            "Alignments:  1 torch.Size([1876, 1876])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VPKvtLXRzhVD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}